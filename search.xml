<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>非线性假设、多分类和神经网络</title>
      <link href="/posts/articletemplate.html"/>
      <url>/posts/articletemplate.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言，技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言。</div><h3 id="1-非线性假设"><a href="#1-非线性假设" class="headerlink" title="1.非线性假设"></a>1.非线性假设</h3><p>当物体特征数量太多，我们就要考虑使用非线性算法来对物体进行分类，如果使用逻辑回归这种算法，会导致模型收敛变得缓慢，甚至出现过拟合情况，因为逻辑回归会为了更好拟合数据而使用高阶参数。</p><p><img src="https://s2.loli.net/2022/03/28/x4ONB3GJ8yVd7Wz.png" alt="image-20220219225909772" style="zoom:80%;"></p><h4 id="2-神经网络-NN"><a href="#2-神经网络-NN" class="headerlink" title="2.神经网络(NN)"></a>2.神经网络(NN)</h4><blockquote><p>为什么神经网络能做分类呢？</p></blockquote><p>神经网络将特征按不同比例(权重)进行组合，在全连接的过程中得到最合适/拟合的权重组合(也就是非线性假设函数$h_\theta$中的$\theta$)，然后经由激活函数(非线性组合一下得到输出)。</p><p>只有将对象属性/参数结合起来（如房子的大小和家具陈列、地理位置等），才能更好判断出房子的价格（需要预测的结果）。</p><p><img src="https://s2.loli.net/2022/03/28/mxFNM9rvdyq84WK.png" alt="image-20220219234338230" style="zoom:80%;"></p><p>全连接层(输入层映射到隐层)的每个参数计算后都需要被激活一次</p><p><img src="https://s2.loli.net/2022/03/28/H3sNRAL2v5XEKfW.png" alt="image-20220219235243638" style="zoom: 67%;"></p><h5 id="2-1-论文中的一些表示—AND、OR、NOT和XNOR异或"><a href="#2-1-论文中的一些表示—AND、OR、NOT和XNOR异或" class="headerlink" title="2.1 论文中的一些表示—AND、OR、NOT和XNOR异或"></a>2.1 论文中的一些表示—AND、OR、NOT和XNOR异或</h5><p>AND<img src="https://s2.loli.net/2022/03/28/ZlGNathq8dXmYCV.png" alt="image-20220220192829203" style="zoom:80%;"></p><p>OR<img src="https://s2.loli.net/2022/03/28/ef9FNB3A24PTksy.png" alt="image-20220220192932316" style="zoom:80%;"></p><p>NOT<img src="https://s2.loli.net/2022/03/28/7lnY6VgX2ryUqFK.png" alt="image-20220220172055080" style="zoom:80%;"></p><p>XNOR异或<img src="https://s2.loli.net/2022/03/28/fXwVFEiU57RgPyz.png" alt="image-20220220193427186" style="zoom:80%;"></p><h5 id="2-2-神经网络中的代价函数表示"><a href="#2-2-神经网络中的代价函数表示" class="headerlink" title="2.2 神经网络中的代价函数表示"></a>2.2 神经网络中的代价函数表示</h5><p><img src="https://s2.loli.net/2022/03/28/NSC25OPhgl8Rp6f.png" alt="image-20220220202947242" style="zoom:80%;"></p><p><strong>每连接一层神经元，就是将这些权重求和的一个过程</strong>。<strong>在计算代价函数时，偏置项(也就是$\theta_0$)是不参与正则化的</strong>，但是即使你将其正则化了，也不会影响结果</p><h5 id="2-3-神经网络中的优化算法-反向传播算法BP-Back-Propagation-prop"><a href="#2-3-神经网络中的优化算法-反向传播算法BP-Back-Propagation-prop" class="headerlink" title="2.3 神经网络中的优化算法(反向传播算法BP)Back Propagation/prop"></a>2.3 神经网络中的优化算法(反向传播算法BP)Back Propagation/prop</h5><p><img src="/posts/articletemplate.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\image-20220221142339918.png" alt="image-20220221142339918" style="zoom:80%;"></p><p>前向传播：<strong>加权</strong>然后<strong>加偏置</strong>最后<strong>激活</strong>，a是激活向量，z是加权值</p><p><img src="https://s2.loli.net/2022/03/28/BnYvZ7d3pPAxyeH.png" alt="image-20220220225158982" style="zoom:80%;"></p><p> BP算法遵循<strong>链式法则</strong>，需从后往前才能得到$\delta$误差（输出层激活值-实际值），前一层的计算方法是：<strong>当前层的权重*上一层的误差再点积当前层激活值的导数</strong>（如果激活函数是sigmoid的话，其导数结果为sigmoid(1-sigmoid)，故每层的激活向量的导数为<strong>a’(1-a’)</strong>）</p><p>注意：输入层不需要梯度下降，也就是没有$\delta_0$</p><p><img src="/posts/articletemplate.htm/Users/86172/Documents/WeChat Files/wxid_yp1bexdqvj8122/FileStorage/File/2022-03/每日学习/每日学习/每日学习/吴恩达机器学习/image-20220221154414270.png" alt="image-20220221154414270" style="zoom:80%;"></p><p>D(erivative)就是对每个参数的偏导（也就是梯度矩阵），然后就可以使用梯度下降或其他高级优化方法求最小值</p><p><img src="https://s2.loli.net/2022/03/28/DGYoPyuNeJ7mXgH.png" alt="image-20220221162306493" style="zoom:80%;"></p><ol><li>在计算反向传播(梯度下降的时候)，不需要将偏置项计算入内</li><li>某个节点的梯度计算方法是：<strong>该节点权重</strong>分别<strong>与下一层各节点的损失的梯度</strong>相乘求和</li></ol><h4 id="3-梯度检测-gt-判断梯度下降是否正常工作"><a href="#3-梯度检测-gt-判断梯度下降是否正常工作" class="headerlink" title="3. 梯度检测-&gt;判断梯度下降是否正常工作"></a>3. 梯度检测-&gt;判断梯度下降是否正常工作</h4><p>在BP过程中有太多的细节，当他与梯度下降或其他优化算法一起工作时(BP的代码实现)很容易出bug，而且代价函数也在迭代的过程中不断下降，其实得到的结果是高一个量级的。为了解决这一问题，我们可以使用<strong>梯度检测</strong>来检查梯度下降（反向传播）是否正确。</p><h5 id="3-1-双侧差分和单侧差分"><a href="#3-1-双侧差分和单侧差分" class="headerlink" title="3.1 双侧差分和单侧差分"></a>3.1 双侧差分和单侧差分</h5><p><img src="https://s2.loli.net/2022/03/28/rKwWHAckqsoFMVU.png" alt="image-20220221175341924" style="zoom:80%;"></p><ol><li><p><strong>取代价函数某点，计算该点左右两边($±\epsilon$)的差值，梯度近似值=差值/$2\epsilon$，也就是y/x</strong>。单侧差分就是只计算$J(\theta+\epsilon)-J(\theta)$的值，据推导后发现双边误差的精度O($n^2$)比单边误差O($n$)高。</p><p><img src="https://s2.loli.net/2022/03/28/Nt7nKskeQVBuFG1.png" alt="image-20220221200745112" style="zoom:80%;"></p></li><li><p>$\epsilon$的<strong>经验值</strong>为$\epsilon=10^{-4}$/$10^{-7}$，不能取太大也不能取太小，太大计算出的梯度可能不太可信(可能区间有其他的坡)，太小计算出的梯度几乎等于该梯度的值。$\epsilon$值也被当做阈值来判断梯度下降是否正常工作</p></li><li><p><strong>计算双侧差分得到梯度之后再与通过反向传播梯度下降的梯度计算误差，要是小于阈值(difference&lt;$\epsilon$)即视为正常工作</strong>，误差计算公式如下（L2范数）：</p><p><img src="https://s2.loli.net/2022/03/28/zry42eMUIwx7not.png" alt="image-20220221193407012" style="zoom:80%;"></p></li></ol><p>注意：检验完梯度下降没有问题后，记得关闭该功能，因为梯度检测速度非常慢；梯度检验<strong>对比较小的w和b比较敏感</strong>，因此可以随机初始化后先计算一次，后面最终计算得到结果后再计算一次</p><h4 id="4-权重参数的随机初始化"><a href="#4-权重参数的随机初始化" class="headerlink" title="4.权重参数的随机初始化"></a>4.权重参数的随机初始化</h4><p>如果权重矩阵不采用随机初始化，则会导致计算得到的<strong>梯度是一致的</strong>，从而在梯度下降后，<strong>前一层节点对应的后一层关于该参数的节点的值都是相等的(同步增减)</strong>，<strong>导致该节点经过隐层变换后得到的特征都是一样的</strong>，这是没有意义的。</p><p>所以将权重参数随机初始化很关键，如<code>np.random.rand()</code>和<code>np.random.randn()</code>返回标准正态分布的样本</p><h4 id="5-模型设置"><a href="#5-模型设置" class="headerlink" title="5.模型设置"></a>5.模型设置</h4><ul><li>输入参数的个数就是特征的维度</li><li>输出参数的个数就是分类的个数</li><li>中间隐层分解的特征个数保持一致，特征个数和层数视模型复杂度而定，特征个数和层数越多计算量越大，但是通常情况下结果是更准确的</li></ul><h4 id="6-神经网络训练步骤"><a href="#6-神经网络训练步骤" class="headerlink" title="6.神经网络训练步骤"></a>6.神经网络训练步骤</h4><ol><li>随机初始化权重参数</li><li>前向传播计算x，$h(x)$/y</li><li>计算出代价函数$j_\theta$</li><li>反向传播计算梯度derivatives</li><li>梯度检测(训练之前先进行梯度检测，在确定正常工作后再开始训练，关闭梯度检测)，不能与dropout一起使用（dropout中随机停用部分神经元后，前一次和后一次的$\delta$无法计算，使得$J_{\theta}$损失函数无法定义），可以先关掉dropout确定梯度下降算法正常工作后，关闭梯度检测，开启dropout</li><li>使用最优化算法得到梯度最小值</li></ol><h4 id="7-优化模型"><a href="#7-优化模型" class="headerlink" title="7.优化模型"></a>7.优化模型</h4><p>当测试假设函数/预测函数，但是和预期有很大误差时，可以选择以下一些方法来调优</p><ul><li><strong>更多训练数据</strong>，过拟合情况下，测试集错误率很小，而测试集错误率很大，增加数据有助于解决高方差问题</li><li><strong>筛选已有特征，适当删减</strong>，通常是当前特征的量级太高或者特征有部分重复，导致数据过拟合或无法拟合，解决高方差问题</li><li><strong>增加特征</strong>，通常情况下是预测函数太简单，无法很好拟合数据，可以解决高偏差(欠拟合)问题</li><li>增加多项式(polynomial)特征（如x²，y²，xy等）或者说增加特征量级，用于解决高偏差问题</li><li>减小/增加正则化惩罚项的系数，分别对应处理高偏差和高方差问题</li></ul><h4 id="8-评估-estimate-假设-gt-分析和优化模型"><a href="#8-评估-estimate-假设-gt-分析和优化模型" class="headerlink" title="8.评估(estimate)假设-&gt;分析和优化模型"></a>8.评估(estimate)假设-&gt;分析和优化模型</h4><p>评估算法得到的预测函数是否可行，以及诊断优化模型方法是否可行，把时间用在刀刃上。</p><h5 id="8-1-诊断-diagnosing-偏差-bias-和方差-variance"><a href="#8-1-诊断-diagnosing-偏差-bias-和方差-variance" class="headerlink" title="8.1 诊断(diagnosing)偏差(bias)和方差(variance)"></a>8.1 诊断(diagnosing)偏差(bias)和方差(variance)</h5><p>在模型过拟合或者欠拟合时，搞清楚是偏差(量级太小，预测函数过于简单，无法完全拟合/特征不够有代表性)问题还是方差(量级太大/数据太少)问题，或者两者都有关，这样能很快帮我们找到有效的途径和方法来改进算法</p><p><img src="https://s2.loli.net/2022/03/28/YeTt9WJqbxalFmL.png" alt="image-20220222185836079" style="zoom:80%;"></p><p>偏差过大(欠拟合)会导致在预测函数量级较小的情况下，训练集error和测试集error都很大；方差(量级)过大(过拟合)会导致在预测函数数量级较大的情况下，训练集error很小而测试集error却很大。（在权重参数归一化后，偏置初始化值一般设置为1）</p><h5 id="8-2-正则化和偏差、方差之间的关系"><a href="#8-2-正则化和偏差、方差之间的关系" class="headerlink" title="8.2 正则化和偏差、方差之间的关系"></a>8.2 正则化和偏差、方差之间的关系</h5><p><img src="https://s2.loli.net/2022/03/28/W9Ej3cuvYRimqPn.png" alt="image-20220222191150855" style="zoom:80%;"></p><p>当惩罚项系数$\lambda$很大时，权重项被惩罚(减)完了，预测值$h_{\theta}$≈bias；当惩罚项系数很小时，没有起到惩罚作用，参数该过拟合的还是要过拟合</p><p><img src="https://s2.loli.net/2022/03/28/dRvXGUy5bktioBp.png" alt="image-20220222192549317" style="zoom: 67%;"></p><h5 id="8-3-学习曲线"><a href="#8-3-学习曲线" class="headerlink" title="8.3 学习曲线"></a>8.3 学习曲线</h5><p><img src="https://s2.loli.net/2022/03/28/hrDlXRCVdKWwq6E.png" alt="image-20220222195938767" style="zoom: 80%;"></p><p>训练集错误率随着样本的增加而增加，测试集则随着训练样本的增加错误率越来越低</p><p><img src="https://s2.loli.net/2022/03/28/Bo8enfCaDFPYiuZ.png" alt="image-20220222200211054" style="zoom:80%;"></p><p>当偏差太大时，会导致错误率稳定在一个较高的位置，此时使用更大量级(方差variance)的预测函数进行训练，训练集和测试集的错误率也就下降了。（该状态下加入更多数据无意义）</p><p><img src="https://s2.loli.net/2022/03/28/kUmlX1u7qGFCyfA.png" alt="image-20220222200859819" style="zoom:80%;"></p><p>当方差(量级)太大时，会导致训练集误差开始很小，然后慢慢增大；而测试集开始误差很大，随着数据的增加慢慢减小，后面甚至会出现错误率交集的情况下，这种情况下增加数据是有效果的，但是最好是降低预测函数量级或者加正则化惩罚项来防止过拟合。</p><p><img src="https://s2.loli.net/2022/03/28/X1iREgqycYJkla4.png" alt="image-20220222212412522" style="zoom:80%;"></p><p>模型太小(层数和特征个数)会导致欠拟合，但是速度会很快；模型太大(有很多特征或有很多层)会导致过拟合。通常在大模型中会考虑使用更大的模型配合正则化来防止过拟合，而不是选用简单的模型。</p><h5 id="8-4-误差分析"><a href="#8-4-误差分析" class="headerlink" title="8.4 误差分析"></a>8.4 误差分析</h5><p>手动检查算法出错的数据(验证集中)，分析总结这部分数据的通病/类型，并观察是否有特征能解决这类问题。可以通过<strong>交叉验证</strong>来判断误差分析得出的结论是否有效。</p><h5 id="8-5-交叉验证-错误率"><a href="#8-5-交叉验证-错误率" class="headerlink" title="8.5 交叉验证(错误率)"></a>8.5 交叉验证(错误率)</h5><p>对于特定问题，单一规则的数值评估指标被称为交叉验证（<strong>也就是使用这条规则和不使用这条规则</strong>）。如是否用词干(stemming)，<strong>如果单一规则能产生明显的错误率降低，就证明该规则有效</strong>。</p><p><img src="https://s2.loli.net/2022/03/28/h7NTL9fHcoZxQdA.png" alt="image-20220222235339653" style="zoom: 80%;"></p><h4 id="9-模型选择和训练、验证、测试"><a href="#9-模型选择和训练、验证、测试" class="headerlink" title="9.模型选择和训练、验证、测试"></a>9.模型选择和训练、验证、测试</h4><h5 id="9-1-模型选择"><a href="#9-1-模型选择" class="headerlink" title="9.1 模型选择"></a>9.1 模型选择</h5><p>选择合适的预测模型，如<img src="/posts/articletemplate.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\image-20220222161110430.png" alt="image-20220222161110430" style="zoom:80%;"></p><p>计算不同维度下梯度下降得到的权重，然后计算出损失值，最后去验证集中验证，不过验证集中的数据也不能完全说明数据的泛化能力，但是维度较低但损失较小的尝尝泛化能力更好</p><h5 id="9-2-模型训练"><a href="#9-2-模型训练" class="headerlink" title="9.2 模型训练"></a>9.2 模型训练</h5><p>训练集 : (交叉)验证集cross validation : 测试集 = 6 : 2 : 2</p><p>使用训练集来得到最优权重，再用验证集计算损失值$J_\theta$选出最合适的模型，最后在测试集上检验效果；一般不会同时用测试集计算损失选模型，又在测试集上检验效果，这样得到的结果不如上述多个验证集的情况。</p><h5 id="9-3-如何快速训练一个有效模型-重点"><a href="#9-3-如何快速训练一个有效模型-重点" class="headerlink" title="*9.3 如何快速训练一个有效模型(重点)"></a>*9.3 如何快速训练一个有效模型(重点)</h5><p>首先拿到问题后，我们先进行<strong>简单分析(问题分析、模型选择和预测函数选择)</strong>，然后选择简单方式<strong>快速实现问题并画出学习曲线</strong>，<strong>诊断是偏差问题还是方差问题</strong>，然后<strong>再进行误差分析</strong>（检查验证集中出错的数据，分析总结问题），最后对症下药，进行优化。</p><h4 id="10-隐层层数与神经元个数的选择"><a href="#10-隐层层数与神经元个数的选择" class="headerlink" title="10.隐层层数与神经元个数的选择"></a>10.隐层层数与神经元个数的选择</h4><p>参考：<a href="https://zhuanlan.zhihu.com/p/100419971">https://zhuanlan.zhihu.com/p/100419971</a></p><p><strong>隐层的作用是允许模型计算更加复杂的特征</strong>，在CNN中，模型的前几层表示的是低级特征，如轮廓等，越到后面表示的特征越高级，如脸部、身体部位等，甚至更为复杂的，我们人类无法理解的特征。层数越多，拟合效果也就越好。</p><p><strong>神经元(特征)个数的作用类似于预测函数中的参数的量级</strong>。<img src="/posts/articletemplate.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\image-20220222224125601.png" alt="image-20220222224125601" style="zoom: 25%;">(如左图为两个神经元)</p><p><strong>隐层的神经元越多代表特征越多</strong>，相比更多的神经元，增加隐层层数往往能获得更好的效果。</p><p><img src="/posts/articletemplate.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\image-20220222223153220.png" alt="image-20220222223153220" style="zoom:80%;"></p><h4 id="11-不对称性分类的误差分析"><a href="#11-不对称性分类的误差分析" class="headerlink" title="11.不对称性分类的误差分析"></a>11.不对称性分类的误差分析</h4><p>当模型需要的精度不一样时（比如在癌症分类中，模型精度要非常高，因为在该模型中，患癌者的概率只有1%，错误率太高会导致结果不准确），此时我们就需要对模型进行精度调优，但是要分析方法是否合适，比如<strong>样本偏斜类问题（正负样本数量不对称）</strong>时，<strong>只看单个正/负结果精度和错误率是否提升不是一个有效方法</strong>。</p><h5 id="11-1-查准率-Precision-和召回率-Recall"><a href="#11-1-查准率-Precision-和召回率-Recall" class="headerlink" title="11.1 查准率(Precision)和召回率(Recall)"></a>11.1 查准率(Precision)和召回率(Recall)</h5><p>P表示正类，N表示负类</p><p>T表示正确判断，F表示错误判断</p><p>P只用来表示已取出样本，N只用来表示未取出样本</p><div class="table-container"><table><thead><tr><th style="text-align:left"></th><th>相关(Relevant)，正类</th><th>不相关(Non-Relevant)，负类</th></tr></thead><tbody><tr><td style="text-align:left">取出Retrieved</td><td>TP(true positives)正类判正，女判女</td><td>FP负类判正，男判女</td></tr><tr><td style="text-align:left">未取出Not Retrieved</td><td>FN(false negatives)正类判负，女判男</td><td>TN负类判负，男判男</td></tr></tbody></table></div><blockquote><p>例子</p></blockquote><p>已知条件：班级总人数100人，男生80人，女生20人</p><p>目标：找出所有女生</p><p>结果：在班级中选了50人，其中20人是女生，30人是男生</p><p>TP=20，FP=30，FN=0，TN=50</p><p><strong>TP+FN=正样本量，TP+FP=预测样本量</strong></p><hr><p><strong>Precision</strong>：误检，查准率，精度</p><script type="math/tex; mode=display">Precision=\frac{TP}{TP+FP}</script><p><strong>Recall</strong>：漏检，查全率，召回率</p><script type="math/tex; mode=display">Recall=\frac{TP}{TP+FN}</script><p>在二分类任务中，<strong>设置的置信度的阈值越大</strong>(如70%才置为1)，此时<strong>精度会变高，但是查全率会降低</strong>；<strong>反之精度会降低，查全率会变高</strong>（偏保守）</p><p><img src="/posts/articletemplate.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\非线性假设(多分类)和神经网络.assets\image-20220301131559362.png" alt="image-20220301131559362" style="zoom: 80%;"></p><h5 id="11-2-评估指标F1-score"><a href="#11-2-评估指标F1-score" class="headerlink" title="11.2 评估指标F1-score"></a>11.2 评估指标F1-score</h5><script type="math/tex; mode=display">F_1=2\frac{PR}{P+R}</script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>逻辑回归LR、正则化和高级优化</title>
      <link href="/posts/b.html"/>
      <url>/posts/b.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言，技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言。</div><p>线性回归预测函数：</p><script type="math/tex; mode=display">h_{\theta}(x)=\theta^{T}x</script><p>逻辑回归预测函数：</p><script type="math/tex; mode=display">h_{\theta}^{(x)}=\frac{1}{1+e^{-\theta^{T}x}}</script><p>线性回归损失函数：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h(\theta^{(i)}-y^i))^2+\frac{\lambda}{2m}\sum_{j=1}^m\theta_j^2</script><p>逻辑回归损失函数：</p><p>MSE直接应用到LR中会导致损失函数变成非凸函数，所以我们加入log让损失函数变成了凸函数</p><p>极大似然（二项分布中）：<img src="https://s2.loli.net/2022/03/28/Zg134Siu2ethEDf.png" alt="image-20220307115455459" style="zoom:80%;"></p><p>非二项分布：<img src="https://s2.loli.net/2022/03/28/2UN81YxLvpl35Iw.png" alt="image-20220308100421863" style="zoom:80%;">（特定采样结果出现的概率累乘）</p><p>由于小数连乘操作可能造成<strong>下溢</strong>，一般会采用<strong>极大对数似然</strong>进行计算</p><p>极大对数似然（二项分布中）：<img src="https://s2.loli.net/2022/03/28/f869VnQHBs4AtWO.png" alt="image-20220307115510199" style="zoom:80%;"></p><p>非二项分布：<img src="https://s2.loli.net/2022/03/28/zS9diUqkCawYLHm.png" alt="image-20220308102624958" style="zoom:80%;"></p><p>损失函数（经验损失+结构损失）：</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{2m}\sum_{i=1}^m(y^ilog{h_\theta(x^i)}+(1-y^i)log(1-h_\theta x^{(i)})^2+\frac{\lambda}{2m}\sum_{j=1}^m\theta_j^2</script><p>两者损失函数求导后，<strong>除了假设函数不一样，表示形式是一样的</strong>：</p><script type="math/tex; mode=display">\frac{\part J(\theta)}{\part{\theta_j}}=\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)})x_j^{(i)}</script><p><img src="https://s2.loli.net/2022/03/28/GpygQk6mlCI8bFt.png" alt="image-20220301171851453" style="zoom:80%;"></p><p>损失函数中参数倍数变化并不会影响最优值的最终结果</p><h4 id="1-1-逻辑回归LR-logistic-regression"><a href="#1-1-逻辑回归LR-logistic-regression" class="headerlink" title="1.1 逻辑回归LR(logistic regression)"></a>1.1 逻辑回归LR(logistic regression)</h4><h5 id="1-1-1-LR与sigmiod"><a href="#1-1-1-LR与sigmiod" class="headerlink" title="1.1.1 LR与sigmiod"></a>1.1.1 LR与sigmiod</h5><script type="math/tex; mode=display">h_{\theta}^{(x)}=\frac{1}{1+e^{-\theta^{T}x}}</script><p>其中$\theta$是收敛之后得到的结果</p><p><img src="https://s2.loli.net/2022/03/28/fDN6jpi1LTVytJ3.png" alt="image-20220218231835992" style="zoom:80%;"></p><p><img src="/posts/b.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\image-20220219101314250.png" alt="image-20220219101314250" style="zoom:80%;"></p><p>根据sigmoid曲线，$h_{\theta}≥0$时，置为1；否则置为0</p><h6 id="1-1-1-1-决策边界"><a href="#1-1-1-1-决策边界" class="headerlink" title="1.1.1.1 决策边界"></a>1.1.1.1 决策边界</h6><p><img src="/posts/b.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\image-20220219102924814.png" alt="image-20220219102924814" style="zoom:80%;"></p><p><img src="/posts/b.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\image-20220219103450449.png" alt="image-20220219103450449" style="zoom:80%;"></p><h5 id="1-1-2-代价函数"><a href="#1-1-2-代价函数" class="headerlink" title="1.1.2 代价函数"></a>1.1.2 代价函数</h5><p>当我们把线性回归的代价函数放到逻辑回归上使用时，会发现代价函数$J$由凸函数(convex)变成了有很多局部最大值的非凸函数，导致寻找最小值变得困难，所有我们选择了另一种能使LR变成凸函数的代价函数。</p><p><img src="/posts/b.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\image-20220219104254098.png" alt="image-20220219104254098" style="zoom:80%;"></p><p>而对数函数log的曲线，能让代价函数变为凸函数的方程吗？</p><p>分析</p><p><img src="/posts/b.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\image-20220219110405097.png" alt="image-20220219110405097" style="zoom:80%;"></p><p>化简</p><p><img src="/posts/b.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\image-20220219112119546.png" alt="image-20220219112119546" style="zoom:80%;"></p><p>得到如下结果，使用了==极大似然法==（能够在统计学中能为不同模型快速寻找参数），并且结果是凸函数</p><script type="math/tex; mode=display">\begin{align}J(\theta)&  = \frac{1}{m}\sum_{i=1}^mCost(h_{\theta}(x^{(i)},y^{(i)}))\\& = -\frac{1}{m}[\sum_{i=1}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]\end{align}</script><p>参数梯度下降：</p><p><img src="/posts/b.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\image-20220219113754588.png" alt="image-20220219113754588" style="zoom:80%;"></p><p>==可以发现，求导后线性回归和逻辑回归的公式是一样的<strong>，</strong>但是他们的假设函数h(θ)是不同的，所以两个函数梯度下降公式是不同的==</p><p>求导sigmiod得到$\partial_{sigmoid}=sigmoid[1-sigmoid]$</p><h4 id="1-2-高级优化"><a href="#1-2-高级优化" class="headerlink" title="1.2 高级优化"></a>1.2 高级优化</h4><ul><li>共轭梯度法Conjugate Gradient</li><li>拟牛顿法中的对称正定迭代矩阵BFGS</li><li>近似BFGS，L-BFGS相对BFGS能够减少空间的使用</li></ul><h5 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h5><ul><li>无需设定学习率，学习率自动动态调整</li><li>大部分情况下速度比梯度下降法快很多</li></ul><h5 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h5><p>实现比梯度下降法复杂很多，但是基本上都有封装好的库，如python中的<code>scipy.optimize.fmin_bfgs</code></p><h4 id="1-3-逻辑回归的多分类任务"><a href="#1-3-逻辑回归的多分类任务" class="headerlink" title="1.3 逻辑回归的多分类任务"></a>1.3 逻辑回归的多分类任务</h4><p>训练多个逻辑回归分类器，然后将输入放到各分类器中，将输入归类为得分值最大的类别即可</p><h4 id="1-4-过拟合和欠拟合解决"><a href="#1-4-过拟合和欠拟合解决" class="headerlink" title="1.4 过拟合和欠拟合解决"></a>1.4 过拟合和欠拟合解决</h4><h5 id="1-4-1-过拟合"><a href="#1-4-1-过拟合" class="headerlink" title="1.4.1 过拟合"></a>1.4.1 过拟合</h5><ul><li>适当减少多余的参数</li><li>使用正则化，适当减少参数维度(阶/次方)/大小</li><li>增加数据量</li><li>dropout</li><li>清晰数据</li><li>提取终止训练</li></ul><h5 id="1-4-2-欠拟合"><a href="#1-4-2-欠拟合" class="headerlink" title="1.4.2 欠拟合"></a>1.4.2 欠拟合</h5><ul><li>增加特征和数据</li><li>增加高阶多项式项</li><li>减少正则化参数</li></ul><h4 id="1-5-正则化惩罚项"><a href="#1-5-正则化惩罚项" class="headerlink" title="1.5 正则化惩罚项"></a>1.5 正则化惩罚项</h4><p>加入惩罚项后，会降低高维参数的值，让他们趋于0（也就是==简化假设模型，限制模型参数，保持参数尽量小==），这样能让假设h函数变得更加的平滑</p><p><img src="/posts/b.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\image-20220219164340840.png" alt="image-20220219164340840"></p><p>我们不知道哪些参数是高维的，该去降低哪些参数的维度，在代价函数中加入正则化惩罚项，对每个参数进行限制</p><h5 id="1-5-1-惩罚项公式以及作用"><a href="#1-5-1-惩罚项公式以及作用" class="headerlink" title="1.5.1 惩罚项公式以及作用"></a>1.5.1 惩罚项公式以及作用</h5><p>公式：<img src="/posts/b.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\image-20220219170024442.png" alt="image-20220219170024442"></p><p> <img src="/posts/b.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\image-20220219222810433.png" alt="image-20220219222810433"></p><p>==简化假设模型，限制模型参数，保持参数尽量小==，$\lambda$作用是控制两个不同目标之间的取舍，设置合适的$\lambda$参数防止模型欠拟合或者无明显作用</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>朴素贝叶斯和贝叶斯网络</title>
      <link href="/posts/c.html"/>
      <url>/posts/c.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言，技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言。</div><h4 id="1-基础知识"><a href="#1-基础知识" class="headerlink" title="1. 基础知识"></a>1. 基础知识</h4><pre><code>P(a,b,c)就是P(abc)A与B相互独立时：    P(AB) = P(A)P(B)    P(B) = P(AB)/P(A)    P(A|B) = P(A) = P(AB)/P(B) = P(A)P(B)/P(B)任意条件下：    P(AB) = P(A|B)P(B)    P(A|B) = P(AB)/P(B)    P(AUB) = P(A)+P(B)-P(AB)    P(B) = P(AUB)-P(A)+P(AB)</code></pre><ul><li>P(AB)表示A与B<strong>同时发生</strong>的概率</li><li>P(A|B)表示<strong>B已经发生的条件</strong>下B发生的概率</li><li><strong>边缘概率/先验概率</strong>：某件事发生的概率，如P(A)</li></ul><p>A与B<strong>任意条件下</strong>时，<strong>P(AB)=P(A|B)P(B)</strong>；A与B<strong>相互独立</strong>时，<strong>P(AB)=P(A)P(B)</strong></p><p>A与B<strong>任意条件下</strong>时，<strong>P(A|B)=P(AB)/P(B)</strong>，A与B<strong>相互独立</strong>时，<strong>P(A|B)=P(A)</strong></p><p>贝叶斯公式：</p><script type="math/tex; mode=display">P(A|B)=\frac{P(B|A)P(A)}{P(B)}</script><h5 id="1-1-联合概率的计算"><a href="#1-1-联合概率的计算" class="headerlink" title="1.1 联合概率的计算"></a>1.1 联合概率的计算</h5><p><strong>任意条件下，P(abc)=P(a)P(b|a)P(c|ab)</strong></p><p>先举个例子：<img src="https://s2.loli.net/2022/03/28/FhNUmBfRdKeP2IY.png" alt="image-20220320104823599" style="zoom: 50%;"></p><p>因为a导致b，a,b导致c，所以有P(a,b,c)=P(c|a,b)P(b|a)P(a)，也<strong>可以理解为每个节点是如何产生的</strong>。</p><blockquote><p>问题：那么在条件独立的情况下呢？或者说如何判断条件独立？</p></blockquote><h4 id="2-D-Separation"><a href="#2-D-Separation" class="headerlink" title="2. D-Separation"></a>2. D-Separation</h4><p><strong>D-Separation</strong>是一种用来<strong>判断变量是否条件独立</strong>的图形化方法</p><p>在贝叶斯网络中：（具体可以参考概率图模型中的贝叶斯网络结构）</p><ul><li><p>head-to-head结构中：a,b独立，a,b导致c<img src="https://s2.loli.net/2022/03/28/D7ZXTklfavutx3p.png" alt="image-20220320110606333" style="zoom:33%;"></p><p>c未知，即a,b独立：根据P(a,b,c)=P(a)P(b)P(c|a,b)，可得P(a,b)=P(a)P(b)</p></li><li><p>tail-to-tail结构中，c独立，c导致a,b<img src="https://s2.loli.net/2022/03/28/68XFjA7m2y4rQha.png" alt="image-20220320113906129" style="zoom: 33%;"></p><p>c给定时，a,b独立：根据P(a,b,c)=P(a|c)P(b|c)P(c)，则P(a,b|c)=P(a,b,c)/P(c)，可得P(a,b|c)=P(a|c)P(b|c)</p></li><li><p>head-to-tail结构中，a独立，a导致c，c导致b<img src="https://s2.loli.net/2022/03/28/sPiWXRekqD8SBI2.png" alt="image-20220320114735885" style="zoom:33%;"></p><p>c给定时，a，b独立：根据P(a,b,c)=P(a)P(c|a)P(b|c)和P(a,b|c)=P(a,b,c)/P(c)，可得P(a,b|c)=P(a|c)P(b|c)</p></li></ul><blockquote><p>练习</p></blockquote><p><img src="https://s2.loli.net/2022/03/28/5cSAxd9HGrLPlOI.png" alt="image-20220320105331298" style="zoom:50%;"></p><p>联合概率分布为：p(1234567)=p(1)p(2)p(3)p(4|1,2,3)p(5|1,3)p(6|4)p(7|4,5)，其中()是条件独立的。</p><h5 id="2-1-朴素贝叶斯和贝叶斯网络对比"><a href="#2-1-朴素贝叶斯和贝叶斯网络对比" class="headerlink" title="2.1 朴素贝叶斯和贝叶斯网络对比"></a>2.1 朴素贝叶斯和贝叶斯网络对比</h5><div class="table-container"><table><thead><tr><th></th><th>朴素贝叶斯</th><th>贝叶斯网络</th></tr></thead><tbody><tr><td>优点</td><td>简单，能够得到条件概率最大的类别</td><td>解决复杂问题的不确定性和关联性有很强的优势。对缺失数据不敏感；可以学习因果关系，加深对数据的理解；可以将先验知识融入模型；避免过拟合问题</td></tr><tr><td>缺点</td><td>如果特征不独立，使用贝叶斯会增加模型复杂度</td><td>由于假设概率一致，所以无法对变量进行筛选</td></tr><tr><td>应用场景</td><td>分类（如：判断是否是垃圾邮件，在文件中出现某些词则它是垃圾邮件的概率）</td><td>推断隐变量概率分布；文本分类中可以解决相邻词和近义词的关系</td></tr></tbody></table></div>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>线性回归、代价函数和梯度下降法</title>
      <link href="/posts/d.html"/>
      <url>/posts/d.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言，技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言。</div><p>线性回归预测函数：</p><script type="math/tex; mode=display">h_{\theta}(x)=\theta^{T}x</script><p>逻辑回归预测函数：</p><script type="math/tex; mode=display">h_{\theta}^{(x)}=\frac{1}{1+e^{-\theta^{T}x}}</script><p>线性回归损失函数：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h(\theta^{(i)}-y^i))^2+\frac{\lambda}{2m}\sum_{j=1}^m\theta_j^2</script><p>逻辑回归损失函数：</p><p>如果直接使用线性回归的<strong>MSE</strong>会让逻辑回归的代价函数变成<strong>非凸函数</strong>，这样就会导致有非常多的局部最优值，导致梯度下降法失效。所以引入了<strong>交叉熵损失函数</strong>来替代线性回归的MSE(均方误差)</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{2m}\sum_{i=1}^m(y^ilog{h_\theta(x^i)}+(1-y^i)log(1-h_\theta x^{(i)})^2+\frac{\lambda}{2m}\sum_{j=1}^m\theta_j^2</script><p>两者损失函数求导后，<strong>除了假设函数不一样，表示形式是一样的</strong>：</p><script type="math/tex; mode=display">\frac{\part J(\theta)}{\part{\theta_j}}=\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)})x_j^{(i)}</script><p><img src="https://s2.loli.net/2022/03/28/GpygQk6mlCI8bFt.png" alt="image-20220301171851453" style="zoom:80%;"></p><p>损失函数中参数倍数变化并不会影响最优值的最终结果</p><h3 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1.线性回归"></a>1.线性回归</h3><p>两个变量：$y=wx+b$</p><p>多个变量：<strong>假设函数</strong>(hypothesis)：$h_\theta(x)=\theta^Tx=\theta_0x_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n$</p><p><img src="https://s2.loli.net/2022/03/28/nfjgz4ZQEJXOLwo.png" alt="image-20220218112740442"></p><h4 id="1-1-代价函数-cost-function"><a href="#1-1-代价函数-cost-function" class="headerlink" title="1.1 代价函数(cost function)"></a>1.1 代价函数(cost function)</h4><ol><li>求(预测值-真实值)差的平方的和，也就是SSE的最小值$min\displaystyle\sum^{m}<em>{i=1}(\hat{y}_i-y</em>{i})^2$</li><li>均方差MSE：$J<em>{(w,b)}=\frac{1}{2m}\displaystyle\sum^{m}</em>{i=1}(\hat{y}<em>i-y</em>{i})^2$，为了方便计算，系数$\frac{1}{m}$换成$\frac{1}{2m}$</li></ol><p>根据x的不同系数w得损失曲线，<strong>根据最小的loss值得到对应系数w</strong></p><p><img src="https://s2.loli.net/2022/03/28/WAg5HqMniC6w42I.png" alt="image-20220215221913546" style="zoom:80%;"></p><p><img src="https://s2.loli.net/2022/03/28/whomk5BlH17nViy.png" alt="image-20220215230923601" style="zoom:80%;"></p><h4 id="1-2-梯度下降-迭代求最优值"><a href="#1-2-梯度下降-迭代求最优值" class="headerlink" title="1.2 梯度下降(迭代求最优值)"></a>1.2 梯度下降(迭代求最优值)</h4><p><strong>步长（学习率$\alpha$）</strong>决定了梯度下降的速度，梯度会下降到直至收敛convergence（也就是到局部最小值才停止），所以太大的步长会导致在坡底(<strong>局部最小值</strong>)震荡</p><p><strong>初始化起点</strong>也能影响梯度下降的速度和得到的局部最小值（<strong>局部最小值可能有很多个，初始化下降起点(也就是w和b)会影响局部最小值</strong>）。一般情况下，设置初始化<code>w, b = 0, 0</code></p><p><img src="https://s2.loli.net/2022/03/28/AaiZnC9kW5BgfRT.png" alt="image-20220215232421193" style="zoom:80%;"></p><p>梯度下降公式：<script type="math/tex">\theta_j=\theta_j-\alpha\frac{\partial{J(\theta_0,\theta_1)}}{\partial_{\theta_j}}\space\space\space(for j = 0 \space and \space j = 1)</script></p><h5 id="1-2-1-参数梯度下降实现步骤-方法"><a href="#1-2-1-参数梯度下降实现步骤-方法" class="headerlink" title="1.2.1 参数梯度下降实现步骤/方法"></a>1.2.1 参数梯度下降实现步骤/方法</h5><p><img src="https://s2.loli.net/2022/03/28/EFlKmzTdo4fD3iQ.png" alt="image-20220217215555223"></p><p>正确的梯度更新应该是<strong>多个参数同步更新（先获取下降梯度再更新参数）</strong>，否则会影响在其他参数的更新，最终影响结果</p><p>如果刚好初始化值为局部最小值，则代价函数$J_\theta$的值为0</p><p>梯度下降时，学习率$\alpha$不需要变更，因为在梯度下降的过程中，代价函数的梯度$\partial_{J}$会随着慢慢下降而减小，所以梯度下降的速度也会减缓</p><p><img src="https://s2.loli.net/2022/03/28/YvAKqT3VrGIzsEF.png" alt="image-20220217223410230" style="zoom: 80%;"></p><p>线性回归的代价函数求导后得到(二元梯度下降)：</p><p>其中$\theta_{0}$为常数<img src="https://s2.loli.net/2022/03/28/6IkqDcxJjYpL35w.png" alt="image-20220217225105167"></p><p>MSE梯度下降公式：<img src="https://s2.loli.net/2022/03/28/gBysLWF3rZGExVO.png" alt="image-20220218215115716" style="zoom:80%;"></p><p>多元梯度下降：</p><p><img src="https://s2.loli.net/2022/03/28/T3XF17zNe5KVkZM.png" alt="image-20220218154711231" style="zoom:80%;"></p><h5 id="1-2-2-凸函数-convex-function-与线性回归"><a href="#1-2-2-凸函数-convex-function-与线性回归" class="headerlink" title="1.2.2 凸函数(convex function)与线性回归"></a>1.2.2 凸函数(convex function)与线性回归</h5><p><img src="https://s2.loli.net/2022/03/28/hyjdtB8Xlq4uLNo.png" alt="image-20220217230356998" style="zoom:80%;"></p><p>凸函数没有局部最优，只有一个全局最优，像这种函数，只要使用<strong>线性回归</strong>总是能收敛到全局最优</p><h5 id="1-2-3-批梯度下降法-Batch-Gradient-Descent"><a href="#1-2-3-批梯度下降法-Batch-Gradient-Descent" class="headerlink" title="1.2.3 批梯度下降法(Batch Gradient Descent)"></a>1.2.3 批梯度下降法(Batch Gradient Descent)</h5><p>考虑全局的一种方法，在线性回归中使用的MSE即均方差即是考虑了所有数据的一种BGD</p><h5 id="1-2-4-特征缩放-归一化"><a href="#1-2-4-特征缩放-归一化" class="headerlink" title="1.2.4 特征缩放/归一化"></a>1.2.4 特征缩放/归一化</h5><p>==归一化可以加快梯度下降的速度，也就是更快地收敛==</p><p><img src="https://s2.loli.net/2022/03/28/nb7cEaxNlXvMyR3.png" alt="image-20220218155743004" style="zoom: 67%;"></p><h6 id="1-2-4-1-均值归一化Mean-Normalization"><a href="#1-2-4-1-均值归一化Mean-Normalization" class="headerlink" title="1.2.4.1 均值归一化Mean Normalization"></a>1.2.4.1 均值归一化Mean Normalization</h6><script type="math/tex; mode=display">x=\frac{x-\mu}{x_{max}-x_{min}}</script><h5 id="1-2-5-小技巧"><a href="#1-2-5-小技巧" class="headerlink" title="1.2.5 小技巧"></a>1.2.5 小技巧</h5><blockquote><p>如何能够快速判断梯度下降是否正在有效工作/收敛呢？</p></blockquote><p>正确的学习率：<img src="https://s2.loli.net/2022/03/28/nbvBFYIiG38VEZo.png" alt="image-20220218161645087"></p><p>错误的学习率：<img src="https://s2.loli.net/2022/03/28/ZbzGvoDtc14rey3.png" alt="image-20220218162815860"></p><p>方法1：(推荐)运行过程中，根据<strong>迭代次数</strong>和<strong>代价函数的值/导数</strong>(下降速度)来判断梯度是否有效下降/收敛，也就是上述绘制曲线，通过看曲线的方式</p><p>方法2：设定一个阈值，当代价函数变化值小于该阈值则停止训练。但是该方式的缺点是通常这个阈值不好选择</p><h6 id="1-2-5-1-总结"><a href="#1-2-5-1-总结" class="headerlink" title="1.2.5.1 总结"></a>1.2.5.1 总结</h6><ul><li>$\alpha$学习率太小会导致梯度下降速度很慢</li><li>$\alpha$太大会导致梯度反向增长，震荡，甚至是收敛速度慢等</li></ul><p>设置较小的学习率总能收敛，但是速度会偏慢，通过观察运行时的曲线选择合适的学习率</p><p><img src="https://s2.loli.net/2022/03/28/J9C8WspuhiAtyRc.png" alt="image-20220218163635970"></p><h4 id="1-3-多项式回归和线性回归"><a href="#1-3-多项式回归和线性回归" class="headerlink" title="1.3 多项式回归和线性回归"></a>1.3 多项式回归和线性回归</h4><p>在选择特征时，可能有多个角度：如在房价预测时，你可以通过房子的纵深和宽度来计算影响因子，也可以通过面积来直接计算；根据模型/数据实际的效果来选择最合适的即可。</p><p>多项式拟合：<img src="https://s2.loli.net/2022/03/28/Nlj7FZAKzOBXVS6.png" alt="image-20220218170807251" style="zoom:80%;"></p><p>有时候我们能使用线性拟合的方式来得到多项式拟合的效果，如</p><p><img src="https://s2.loli.net/2022/03/28/gy5XRmeC8ObMBpw.png" alt="image-20220218171023479" style="zoom:80%;"></p><h4 id="1-4-正规方程-直接求解最优值-Norm-Equation"><a href="#1-4-正规方程-直接求解最优值-Norm-Equation" class="headerlink" title="1.4 正规方程(直接求解最优值)Norm Equation"></a>1.4 正规方程(直接求解最优值)Norm Equation</h4><p><img src="https://s2.loli.net/2022/03/28/S6P8On1GuzvUc49.png" alt="image-20220218202947319" style="zoom:80%;"></p><p>$\theta=(X^{T}X)^{-1}X^{T}y$该公式计算结果可以直接求得代价函数最小化的$\theta$，也就是算得其中一个参数系数的最优解</p><p>在使用了Norm Equation正规方程后，数据可以不用归一化处理，直接计算即可</p><h5 id="1-4-1-正规方程在不可逆情况下的解决方法"><a href="#1-4-1-正规方程在不可逆情况下的解决方法" class="headerlink" title="1.4.1 正规方程在不可逆情况下的解决方法"></a>1.4.1 正规方程在不可逆情况下的解决方法</h5><p>在Octave/Matlab中使用pinv(伪逆)/inv可以计算得到矩阵的逆，矩阵在一定条件下是不可逆的(矩阵的值为0，也就是某些特征之间存在线性关系，说明部分特征是多余的；样本太少，特征太多，适当减少特征或者使用正则化)，但是使用pinv仍然可以得到该矩阵的逆</p><p><img src="https://s2.loli.net/2022/03/28/l3tRJxmQ8wc2djN.png" alt="image-20220218212054877" style="zoom:80%;"></p><h4 id="1-5-梯度下降法VS正规方程"><a href="#1-5-梯度下降法VS正规方程" class="headerlink" title="1.5 梯度下降法VS正规方程"></a>1.5 梯度下降法VS正规方程</h4><div class="table-container"><table><thead><tr><th>\</th><th>梯度下降法Gradient Desent</th><th>正规方程Norm Equation</th></tr></thead><tbody><tr><td>缺点</td><td>1.需要设置学习率$\alpha$ 2.需要迭代升级</td><td>当参数比较大时，计算$\theta=(X^{T}X)^{-1}X^{T}y$的时间成本为$O(n^{3})$，但参数个数较小(10k以下?)时，速度会较梯度下降法快；对于一些复杂的学习算法，我们不得不使用梯度下降法来替代正规方程</td></tr><tr><td>优点</td><td>当参数非常大时依然能非常好地工作；在一些复杂算法中仍然适用，而正规方程只使用于特定的一些算法中，如线性回归等</td><td>不需要设置学习率$\alpha$；不需要迭代</td></tr></tbody></table></div><p><strong>自适应优化算法（如momentum-SGD、NAG、Adagrad、Adadelta、RMSprop）训练出来的结果通常都不如SGD，尽管这些自适应优化算法在训练时表现的看起来更好。 使用者应当慎重使用自适应优化算法。</strong></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>关于itsNeko开源博客食用方法</title>
      <link href="/posts/itsneko-opensource-blog.html"/>
      <url>/posts/itsneko-opensource-blog.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言：本文是关于itsNeko开源博客食用方法详解，感谢使用本开源博客。时间过的好快，还好我都记录下来了。</div><h3 id="itsNeko开源介绍"><a href="#itsNeko开源介绍" class="headerlink" title="itsNeko开源介绍"></a>itsNeko开源介绍</h3><ul><li>基于<br>itsNeko开源博客是基于Hexo博客Matery主题魔改，感谢各位对本博客的喜爱与支持。</li><li>作者<br>Hello，itsNeko，我是本博客的作者，itsNeko博主博客: <a href="https://nekodeng.gitee.io/" target="_blank">itsNeko博客~</a></li><li>赞赏<br>如果你觉得本开源博客还可以，欢迎大家的赞赏，赞赏二维码见页面：<a href="https://nekodeng.gitee.io/donate/" target="_blank">赞赏itsNeko开源博主~</a></li><li>提示<br>本博客为纯静态，无数据库，文章使用markdown格式，图片存在json里面，整体打包上传至服务器即可。</li><li>建议<br>最好懂一点编程知识，建议使用VsCode，善于使用<code>“ ctrl+F ”</code>快捷键定位然后修改自定义内容。</li></ul><h3 id="源码下载及命令"><a href="#源码下载及命令" class="headerlink" title="源码下载及命令"></a>源码下载及命令</h3><ul><li>本开源博客源码已公开在在博主的GitHub仓库，国内也可使用Gitee，希望得到各位的<code>小星星，Star</code>。</li><li>GitHub仓库：<a href target="_blank">itsNeko开源博客源码GitHub地址</a></li><li><p>Gitee仓库：<a href target="_blank">itsNeko开源博客源码Gitee地址</a></p></li><li><p>首先，新建一个文件夹名为“ <code>nekoblog</code> ”，在该文件夹下打开git bash，执行命令行</p></li></ul><pre><code>git clone 仓库地址</code></pre><ul><li>将下载的整个文件夹在VsCode中打开，在终端中依次执行以下命令行</li></ul><pre><code>npm installhexo inithexo ghexo s</code></pre><ul><li>然后在浏览器中，打开“ <a href="http://localhost:4000/">http://localhost:4000/</a> ”，即可实时预览网站，再依次修改网站内容。</li></ul><p>注意：依次执行完上述4条命令后，若遇到报错极大概率是npm install的问题，也可能是网络问题，建议删除整个“ node_modules ”文件夹，然后重新执行上述命令行。</p><h3 id="各页面配置详解"><a href="#各页面配置详解" class="headerlink" title="各页面配置详解"></a>各页面配置详解</h3><h4 id="首页"><a href="#首页" class="headerlink" title="首页"></a>首页</h4><h5 id="1、网站标题描述等"><a href="#1、网站标题描述等" class="headerlink" title="1、网站标题描述等"></a>1、网站标题描述等</h5><p>在<code>/nekoblog/_config.yml</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ Site ”</code>，然后自行修改内容。</p><h5 id="2、logo图片与logo字体"><a href="#2、logo图片与logo字体" class="headerlink" title="2、logo图片与logo字体"></a>2、logo图片与logo字体</h5><ul><li>logo图片:<br>在<code>/themes/_config.yml</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ 配置网站favicon和网站LOGO ”</code>，然后自行修改图片并注意图片格式（此处建议图片大小为180*116最佳）。</li><li>logo字体:<br>在<code>/themes/layout/_partial/header.ejs</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ itsNeko-openSource-Blog ”</code>，然后自行修改大屏幕和小屏幕下logo字体（此处建议小屏下字体数目不宜过多）。 </li></ul><h5 id="3、banner图上打字效果字体"><a href="#3、banner图上打字效果字体" class="headerlink" title="3、banner图上打字效果字体"></a>3、banner图上打字效果字体</h5><p>在<code>/themes/_config.yml</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ sub1 ”</code>，然后自行修改。</p><h5 id="4、《质数的孤独》内容部分"><a href="#4、《质数的孤独》内容部分" class="headerlink" title="4、《质数的孤独》内容部分"></a>4、《质数的孤独》内容部分</h5><p>在<code>/themes/layout/_widget/dream.ejs</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ 《质数的孤独》 ”</code>，然后自行修改标题和内容。</p><h5 id="5、公告栏作者等内容部分"><a href="#5、公告栏作者等内容部分" class="headerlink" title="5、公告栏作者等内容部分"></a>5、公告栏作者等内容部分</h5><p>在<code>/themes/layout/index.ejs</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ 公告文字栏开始 ”</code>，然后自行修改内容。</p><h5 id="6、footer内容部分"><a href="#6、footer内容部分" class="headerlink" title="6、footer内容部分"></a>6、footer内容部分</h5><p>在<code>/themes/layout/_partial/footer.ejs</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ 版权信息 ”</code>，然后自行修改各类内容；此处还要在<code>/themes/_config.yml</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ socialLink ”</code>，然后自行修改。</p><h4 id="关于"><a href="#关于" class="headerlink" title="关于"></a>关于</h4><h5 id="1、配置个人信息"><a href="#1、配置个人信息" class="headerlink" title="1、配置个人信息"></a>1、配置个人信息</h5><p>在<code>/themes/_config.yml</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ profile ”</code>，然后自行修改头像、职业和个人介绍。</p><h5 id="2、个人介绍内容"><a href="#2、个人介绍内容" class="headerlink" title="2、个人介绍内容"></a>2、个人介绍内容</h5><p>在<code>/themes/layout/about.ejs</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ 关于我哦 ”</code>，然后自行个人介绍介绍。</p><h4 id="相册"><a href="#相册" class="headerlink" title="相册"></a>相册</h4><h5 id="1、页面介绍"><a href="#1、页面介绍" class="headerlink" title="1、页面介绍"></a>1、页面介绍</h5><p>在<code>/themes/layout/galley.ejs</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ itsNeko开源博客相册 ”</code>，然后自行修改此内容。</p><h5 id="2、相册数据"><a href="#2、相册数据" class="headerlink" title="2、相册数据"></a>2、相册数据</h5><p>相册是存在json里面的，在<code>/nekoblog/source/_data/galley.json</code>文件中。一个相册就是在一个{}对象内的json数据。</p><h5 id="3、新建一个相册流程"><a href="#3、新建一个相册流程" class="headerlink" title="3、新建一个相册流程"></a>3、新建一个相册流程</h5><ul><li>首先，找到<code>/nekoblog/source/galley</code>文件夹。</li><li>复制已存在的“ itsNeko博主的绘画作品 ”文件夹，并取名。</li><li>打开新取名文件夹下的index.md文件，然后修改title字段必须与新取名文件夹名称相同，否则运行报错。</li><li>然后，在<code>/nekoblog/source/_data/galley.json</code>文件中，复制已存在的{}对象内的json数据，与第一段并以逗号区隔，然后修改新相册的各类内容。并注意：<code>name，url_name，album内的title这三个字段内容必须与新取名文件夹名称相同，否则运行报错。</code></li></ul><h4 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h4><h5 id="增加友情链接"><a href="#增加友情链接" class="headerlink" title="增加友情链接"></a>增加友情链接</h5><ul><li>友链信息是存储在<code>/nekoblog/source/_data/friends.json</code>文件中，一个友链就是在一个{}对象内的json数据，自行增添。</li></ul><h5 id="增加网址收藏"><a href="#增加网址收藏" class="headerlink" title="增加网址收藏"></a>增加网址收藏</h5><ul><li>网址收藏信息是存储在<code>/nekoblog/source/_data/collection.json</code>文件中，一个网址信息就是在一个{}对象内的json数据，自行增添。</li></ul><h4 id="其余"><a href="#其余" class="headerlink" title="其余"></a>其余</h4><p>其余页面自定义修改的内容不多，或者没必要修改，这里给出其余页面的对应文件，可自行根据需要定位到对应页面文件中进行修改。</p><ul><li><code>书单 -&gt; /themes/layout/books.ejs</code></li><li><code>留言板 -&gt; /themes/layout/contact.ejs</code></li><li><code>实战项目 -&gt; /themes/layout/project.ejs</code></li><li><code>博客打赏记录 -&gt; /themes/layout/donate.ejs</code></li><li><code>ticktack -&gt; /themes/layout/ticktack.ejs</code></li><li><code>实战项目 -&gt; /themes/layout/project.ejs</code></li><li><code>vlog -&gt; /themes/layout/videos.ejs</code><br>使用“ ctrl+F ”快捷键定位到<code>“ src=”//player.bilibili ”</code>，然后自行修改两个视频的src，建议使用B站的外链。</li><li><code>music -&gt; /themes/layout/musics.ejs</code><br>使用“ ctrl+F ”快捷键定位到<code>“ id=” ”</code>，然后自行修改两个歌单的id，建议使用网易云和QQ音乐的外链。</li><li><code>urls.txt -&gt; /nekoblog/urls.txt</code>将内容替换成自己网站的url地址即可。</li><li><code>网站2个_config.yml文件 -&gt; 主要是网站的总体配置，自行打开两个文件然后只修改里面的包含个人信息的部分，其余不动。</code></li></ul><h3 id="撰写第一篇文章"><a href="#撰写第一篇文章" class="headerlink" title="撰写第一篇文章"></a>撰写第一篇文章</h3><ul><li>修改完个人信息后，你便可以撰写属于你的第一篇文章啦。</li><li>文章采用markdown语法，所有文章存储在<code>/nekoblog/source/_posts</code>文件夹中。</li><li>这里我已给出常用的两类文章模板，可直接复制粘贴然后（修改文章名，链接后缀建议用英文，标签建议只用一个，分类，时间，以及文章banner图片）形成新的文章。</li></ul><h3 id="怎样开通评论"><a href="#怎样开通评论" class="headerlink" title="怎样开通评论"></a>怎样开通评论</h3><ul><li>本开源博客使用valine评论，环境以及搭好，只需填写valine配置数据即可。</li><li>自行根据网上教程得到配置数据，关键词 “ Hexo 博客添加 Valine 评论系统 ”，教程之一：<a href="https://www.zhyong.cn/posts/95cb/" target="_blank">Hexo 博客添加 Valine 评论系统</a></li><li>注意：本开源博客中已经搭好了环境，<code>比如valine.min.js，valine.ejs等文件已经存在，不用再根据教程添加这些文件</code>，只需根据教程步骤配置其余，然后获取到 APP ID 和 APP KEY。</li><li>然后，将获取到的 APP ID 和 APP KEY，在<code>/themes/_config.yml</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ valine ”</code>，然后自行修改appId，appKey值即可。</li><li>多说一句，若最后评论功能总是报错，不知道怎么弄，那么建议一切根据外链教程（这个不行，换个完整版教程）操作，可替换本博客已经存在的环境文件，<code>学会自己独立使用浏览器解决问题</code>。</li></ul><h3 id="网站怎样上线"><a href="#网站怎样上线" class="headerlink" title="网站怎样上线"></a>网站怎样上线</h3><ul><li><p><code>免费版 使用gitee免费托管</code><br>关键词，“ Hexo部署到Gitee ”，随机教程：<a href="https://blog.csdn.net/qq_38157825/article/details/112783631" target="_blank">Hexo 部署到 Gitee</a></p></li><li><p><code>收费版 自行购买域名服务器</code><br>关键词，“ Hexo部署到自己服务器 ”</p></li></ul><h3 id="七零八碎补充"><a href="#七零八碎补充" class="headerlink" title="七零八碎补充"></a>七零八碎补充</h3><p>可自行根据需要定位到对应文件中进行修改。</p><ul><li><p><code>赞赏二维码图片 -&gt; /themes/source/medias/reward</code></p></li><li><p><code>鼠标左键点击文字 -&gt; /themes/source/js/click_show_text.js</code></p></li><li><p><code>网站标题栏和footer栏背景颜色 -&gt; /themes/source/css/matery.css</code><br>在该文件中使用“ ctrl+F ”快捷键定位到<code>“ 网站标题栏和footer栏背景颜色 ”</code>，然后自行修改颜色。</p></li><li><p><code>各大页面banner图 -&gt; 若你不想使用默认图片，将以下代码复制到对应页面ejs文件中并修改图片链接即可。</code></p></li></ul><pre><code>&lt;style&gt;  /* banner背景图 */  .bg-cover &#123;      background-image: url(&quot;图片链接地址&quot;)!important;  &#125;&lt;/style&gt;</code></pre><h3 id="怎样联系itsNeko"><a href="#怎样联系itsNeko" class="headerlink" title="怎样联系itsNeko"></a>怎样联系itsNeko</h3><ul><li>留言：itsNeko博客地址:<a href="https://nekodeng.gitee.io/" target="_blank">itsNeko博客~</a></li><li>邮箱：nekodeng@qq.com</li><li>QQ: 2018854221</li></ul>]]></content>
      
      
      <categories>
          
          <category> itsNeko博客食用方法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 关于itsNeko开源博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>拉格朗日乘子法和KKT条件</title>
      <link href="/posts/a.html"/>
      <url>/posts/a.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言，技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言。</div><p>如果两个模型的概率分布是不一样的，所以在衡量模型的差异的时候，不能直接定性衡量两个模型之间的差异，而是需要<strong>定量</strong>的衡量两个模型的差异（<strong>比如极大似然估计、最小二乘法和交叉熵</strong>）</p><h4 id="1-信息量"><a href="#1-信息量" class="headerlink" title="1.信息量"></a>1.信息量</h4><p><strong>信息量</strong>可以理解成一个事件从不确定变成确定的<strong>难度程度(概率)</strong>（或者说事件对某个人带来的价值大小）。难度越大，信息量越大。比如，阿根廷进入8强到赢得决赛的难度为$\frac{1}{2^3}$，则信息量为3比特，再比如中国队从8强赢得决赛的难度为$\frac{1}{2^{10}}$，则信息量为10比特。</p><p>公式为：$I(i)=-log_2p_i$，log以2为底因为最后要以<strong>比特</strong>表示信息量</p><p><img src="https://s2.loli.net/2022/03/28/5BcbiGfMIKqoxDr.png" alt="image-20220306204926864" style="zoom:80%;"></p><h4 id="2-熵"><a href="#2-熵" class="headerlink" title="2.熵"></a>2.熵</h4><p><strong>熵</strong>衡量一个系统中的所有事件从不确定到确定的难度大小。对整体的概率模型进行一个衡量，衡量结果能反映出这个概率模型的不确定程度/混乱程度，熵是信息量的<strong>期望</strong>。</p><p>事件的不确定性越高，则信息量越高，即信息量函数$f$与概率$P$成<strong>负相关</strong>，即$f(P)=log\frac{1}{P}=-log(P)$</p><p>两个<strong>独立事件</strong>所产生的信息量等于各自信息量之和，即$f(P_1,P_2)=f(P_1)+f(P_2)$</p><p><strong>信息熵表示的是信息量的期望</strong>，即$H(P)=E[-log(P)]=\displaystyle\sum_XP(X)f(X)=-\displaystyle\sum_XP(X)logP(X)$</p><p>可以证明：$0 \le H(P) \le log\abs{X}$，所以当且仅当$X$的分布为均匀分布时有$H(P)=log\abs{X}$，即$P(X)=\frac{1}{\abs{X}}$时熵最大</p><h5 id="2-1-最大熵原理"><a href="#2-1-最大熵原理" class="headerlink" title="2.1 最大熵原理"></a>2.1 最大熵原理</h5><ol><li>最大熵<code>Max Entropy</code>原理：学习概率模型时，在所有可能的概率分布(模型)中，熵最大的分布(模型)是最好的模型（概率最均匀）<ul><li>通常会有<strong>其他已知条件</strong>来确定概率模型的<strong>集合</strong>，因此最大熵的原理是：在满足已知条件的情况下，选取熵最大的模型</li><li>在满足已知条件的前提下，如果没有其他信息，则不确定部分都是<strong>等可能</strong>的（均匀分布时，熵最大）。而这种等可能性就是由熵最大化得到的</li></ul></li><li>最大熵原理选取熵最大的模型，而决策树的目的是得到熵最小的划分。原因在于：<ul><li>最大熵原理认为在满足已知条件之后，选择不确定性最大(即不确定部分都是等可能的)的模型。即不再施加已知条件之外的约束。这是<strong>求最大不确定性的过程</strong></li><li>决策树的划分目标是为了通过不断划分从而降低实例所属的类的不确定性，最终给实例一个合适的分类。这是一个<strong>不确定性不断减小的过程</strong></li></ul></li></ol><h5 id="2-2-在期望约束下的最大熵模型"><a href="#2-2-在期望约束下的最大熵模型" class="headerlink" title="2.2 在期望约束下的最大熵模型"></a>2.2 在期望约束下的最大熵模型</h5><p>期望的约束表示为：$E[f(X)]=\displaystyle\sum_XP(X)f(X)=\tau$，其中$f(X)$是约束条件，$E(f(X))$是一个常数。</p><p>假设有k个约束条件：<img src="https://s2.loli.net/2022/03/28/Fsebz26OAiVLTfd.png" alt="image-20220316163251704" style="zoom: 67%;"></p><p>即求解约束最优化问题：<img src="https://s2.loli.net/2022/03/28/f1UTQicGBeI2E9W.png" alt="image-20220316163435147" style="zoom:67%;"></p><p>利用拉格朗日乘子法求解：<img src="https://s2.loli.net/2022/03/28/vZbxPV13CwQaprE.png" alt="image-20220316163546810" style="zoom:67%;"></p><p>解得：<img src="https://s2.loli.net/2022/03/28/fmTLuQrX5YFG1z3.png" alt="image-20220316163627242" style="zoom:67%;"></p><h5 id="2-3-香农熵Shannon-entropy"><a href="#2-3-香农熵Shannon-entropy" class="headerlink" title="2.3 香农熵Shannon entropy"></a>2.3 香农熵Shannon entropy</h5><script type="math/tex; mode=display">H(p)=-Ep[logp]=\left\{\begin{matrix} H(p)=-\int_xP(x)logP(x)dx \\ H(p)=-\displaystyle\sum_xP(x)logP(x)\end{matrix}\right.</script><p>香农熵就是在连续分布和离散分布中，对信息量的期望</p><p>公式为：熵=事件的概率x事件信息量的和，即各事件对系统贡献的信息量$H(x)=-\displaystyle \sum_{i=1}^nP(x_i)log{P(x_i)}$</p><p>比如，对于二分类(或者是二项分布)的任务，$H(x)=-P(x)logP(x)-(1-P(x))log(P(x))$</p><p><img src="https://s2.loli.net/2022/03/28/pCPa7mrutBW2qKh.png" alt="image-20220306205005670" style="zoom: 80%;"></p><h4 id="3-相对熵-KL散度"><a href="#3-相对熵-KL散度" class="headerlink" title="3.相对熵(KL散度)"></a>3.相对熵(KL散度)</h4><blockquote><p>通信/编码角度的理解</p></blockquote><ul><li>$H(P)$为服从$P(X)$分布的信源$X$的<strong>信息熵</strong>，<strong>也表示对信源$X$编码所需的平均比特数</strong></li><li>$H(P,Q)$称为交叉熵，<strong>表示对服从$P(X)$分布的信源$X$，按照分布$Q(X)$来进行编码所需的平均比特数</strong>，也表示<strong>利用分布$Q(X)$来表示服从$P(X)$分布的$X$的困难程度</strong></li><li>$D(P||Q)$为相对熵，表示用$Q(X)$来对信源$X$编码平均所需要的<strong>额外比特数</strong></li></ul><p>对于同一随机变量x，有两个独立的概率分布，我们可以用KL散度来衡量两个分布的差异。机器学习中，当我们不知道一个模型时，没办法直接求熵，需要依靠相对熵来计算两个模型的差异，也就是loss值（P往往表示样本的真实分布，Q表示预测模型的分布）</p><p><img src="https://s2.loli.net/2022/03/28/FDetwXlmhR1QUoY.png" alt="image-20220308160033264" style="zoom:80%;"></p><p>令<img src="https://s2.loli.net/2022/03/28/lLaBIZx153H2V8C.png" alt="image-20220308160141012" style="zoom:80%;"></p><p>则相对熵可以写成以下形式：</p><p><img src="https://s2.loli.net/2022/03/28/CwjZrgfx7LeHY8u.png" alt="image-20220308160220771" style="zoom:80%;"></p><p>根据吉布斯不等式，$D_{KL}$中$H(P,Q)-H(P)≥0$</p><p>仅当两个模型完全相等时$D(P||Q)=0$，<strong>有差异时$D(P||Q)&gt;0$</strong></p><p>交叉熵越小，表示两个模型越相近，或者说转码不会有过多冗余比特</p><p><strong>因为P的熵是确定的，所以求KL散度(相对熵)变成了求交叉熵的问</strong></p><p>上述讨论的相对熵是在事件数(样本量)一样的情况下，当模型事件数量(样本量)不一样的时候取事件数多的那个</p><p><img src="https://s2.loli.net/2022/03/28/HKgVjZbqDkrI5G1.png" alt="image-20220308162326928" style="zoom:80%;"></p><h4 id="4-交叉熵"><a href="#4-交叉熵" class="headerlink" title="4.交叉熵"></a>4.交叉熵</h4><p>在分类问题中，<strong>通常使用交叉熵损失度量标签的真实分布和由分类器预测的分布之间的差异</strong>。要找两个模型最优值，就是要找交叉熵最小的情况。</p><p><img src="https://s2.loli.net/2022/03/28/hEP3ec76xTy2ORV.png" alt="image-20220308161727614" style="zoom:80%;"></p><p>一般来说<strong>以P为样本数据分布，Q为待优化的预测分布</strong></p><p>在机器学习当中，我们对模型的训练实际上就是一个参数估计的过程。我们<strong>对模型的参数进行调整的过程就是调整模型$Q(X)$来逼近真实数据$P(X)$的优化过程</strong></p><h5 id="4-1-交叉熵与极大似然估计"><a href="#4-1-交叉熵与极大似然估计" class="headerlink" title="4.1 交叉熵与极大似然估计"></a>4.1 交叉熵与极大似然估计</h5><p>极大似然估计<img src="https://s2.loli.net/2022/03/28/Q6AtH79PxvwmZsC.png" alt="image-20220307111247148" style="zoom:80%;">等价于最小化负对数似然<img src="/posts/a.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\熵、信息量、KL散度、交叉熵.assets\image-20220307111346317.png" alt="image-20220307111346317" style="zoom:80%;"></p><p>这与逻辑回归中，用极大似然估计推出的损失函数在形式上是一样的，但是实际意义上是不一样的</p><ul><li><p><strong>极大似然估计中的log是为了将连乘计算量简化为连加</strong></p><p>极大似然估计：<img src="https://s2.loli.net/2022/03/28/VPG2s1N4DKty5Fa.png" alt="image-20220307115957278" style="zoom:80%;"></p><p>极大对数似然估计：<img src="https://s2.loli.net/2022/03/28/qpYzWBU3kjuVMQv.png" alt="image-20220307120100501" style="zoom:80%;"></p><p>$log(xyz)=log(x)+log(y)+log(z)$；熵中则是为了计算概率对应的信息量引入-log</p></li><li><p>而且<strong>一个是有量纲，一个是没有量纲的</strong>（交叉熵中的信息量是有量纲(比特)的，但是极大似然估计中是没有的）</p></li><li><p>而且极大似然估计中求的是极大值，LR中强行加了一个负号，使其变成了最小值；熵中是为了计算困难程度对应的概率引入-log（如夺冠的概率为$\frac{1}{8}$，最后夺冠了，则信息量为$-log_2(\frac{1}{8})=3比特$）</p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>这是技术文章模板</title>
      <link href="/posts/articletemplate.html"/>
      <url>/posts/articletemplate.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言，技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言。</div><h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><ul><li>项目1</li><li>项目2</li><li>项目3</li></ul><p>正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文</p><p><img src="/posts/articletemplate.htm/图片url" alt="图片介绍"></p><p><code>正文一些需要高亮色的文字</code></p><p><a href="链接地址" class="LinkCard" target="_blank">引入链接</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这是技术文章模板</title>
      <link href="/posts/articletemplate6.html"/>
      <url>/posts/articletemplate6.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言，技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言。</div><h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><ul><li>项目1</li><li>项目2</li><li>项目3</li></ul><p>正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文</p><p><img src="/posts/articletemplate6.htm/图片url" alt="图片介绍"></p><p><code>正文一些需要高亮色的文字</code></p><p><a href="链接地址" class="LinkCard" target="_blank">引入链接</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这是技术文章模板</title>
      <link href="/posts/articletemplate5.html"/>
      <url>/posts/articletemplate5.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言，技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言。</div><h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><ul><li>项目1</li><li>项目2</li><li>项目3</li></ul><p>正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文</p><p><img src="/posts/articletemplate5.htm/图片url" alt="图片介绍"></p><p><code>正文一些需要高亮色的文字</code></p><p><a href="链接地址" class="LinkCard" target="_blank">引入链接</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这是技术文章模板</title>
      <link href="/posts/articletemplate4.html"/>
      <url>/posts/articletemplate4.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言，技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言。</div><p>线性回归预测函数：</p><script type="math/tex; mode=display">h_{\theta}(x)=\theta^{T}x</script><p>逻辑回归预测函数：</p><script type="math/tex; mode=display">h_{\theta}^{(x)}=\frac{1}{1+e^{-\theta^{T}x}}</script><p>线性回归损失函数：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h(\theta^{(i)}-y^i))^2+\frac{\lambda}{2m}\sum_{j=1}^m\theta_j^2</script><p>逻辑回归损失函数：</p><p>MSE直接应用到LR中会导致损失函数变成非凸函数，所以我们加入log让损失函数变成了凸函数</p><p>极大似然（二项分布中）：<img src="https://s2.loli.net/2022/03/28/JVhKAlC8GqdmHuB.png" alt="image-20220307115455459" style="zoom:80%;"></p><p>非二项分布：<img src="https://s2.loli.net/2022/03/28/xtyceCuAWTp3Hfz.png" alt="image-20220308100421863" style="zoom:80%;">（特定采样结果出现的概率累乘）</p><p>由于小数连乘操作可能造成<strong>下溢</strong>，一般会采用<strong>极大对数似然</strong>进行计算</p><p>极大对数似然（二项分布中）：<img src="https://s2.loli.net/2022/03/28/RVpeWD29NGnHkd4.png" alt="image-20220307115510199" style="zoom:80%;"></p><p>非二项分布：<img src="https://s2.loli.net/2022/03/28/Pt3ulvn1SE6fYzW.png" alt="image-20220308102624958" style="zoom:80%;"></p><p>损失函数（经验损失+结构损失）：</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{2m}\sum_{i=1}^m(y^ilog{h_\theta(x^i)}+(1-y^i)log(1-h_\theta x^{(i)})^2+\frac{\lambda}{2m}\sum_{j=1}^m\theta_j^2</script><p>两者损失函数求导后，<strong>除了假设函数不一样，表示形式是一样的</strong>：</p><script type="math/tex; mode=display">\frac{\part J(\theta)}{\part{\theta_j}}=\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)})x_j^{(i)}</script><p><img src="https://s2.loli.net/2022/03/28/qBdCjLkpsAW7gcu.png" alt="image-20220301171851453" style="zoom:80%;"></p><p>损失函数中参数倍数变化并不会影响最优值的最终结果</p><h4 id="1-1-逻辑回归LR-logistic-regression"><a href="#1-1-逻辑回归LR-logistic-regression" class="headerlink" title="1.1 逻辑回归LR(logistic regression)"></a>1.1 逻辑回归LR(logistic regression)</h4><h5 id="1-1-1-LR与sigmiod"><a href="#1-1-1-LR与sigmiod" class="headerlink" title="1.1.1 LR与sigmiod"></a>1.1.1 LR与sigmiod</h5><script type="math/tex; mode=display">h_{\theta}^{(x)}=\frac{1}{1+e^{-\theta^{T}x}}</script><p>其中$\theta$是收敛之后得到的结果</p><p><img src="https://s2.loli.net/2022/03/28/qDIhoYGim7lnbKk.png" alt="image-20220218231835992" style="zoom:80%;"></p><p><img src="https://s2.loli.net/2022/03/28/t1lJq9RNegYmLcH.png" alt="image-20220219101314250" style="zoom:80%;"></p><p>根据sigmoid曲线，$h_{\theta}≥0$时，置为1；否则置为0</p><h6 id="1-1-1-1-决策边界"><a href="#1-1-1-1-决策边界" class="headerlink" title="1.1.1.1 决策边界"></a>1.1.1.1 决策边界</h6><p><img src="https://s2.loli.net/2022/03/28/2BNgWfo6KwAqGzE.png" alt="image-20220219102924814" style="zoom:80%;"></p><p><img src="https://s2.loli.net/2022/03/28/5pSIeCALls2z8nr.png" alt="image-20220219103450449" style="zoom:80%;"></p><h5 id="1-1-2-代价函数"><a href="#1-1-2-代价函数" class="headerlink" title="1.1.2 代价函数"></a>1.1.2 代价函数</h5><p>当我们把线性回归的代价函数放到逻辑回归上使用时，会发现代价函数$J$由凸函数(convex)变成了有很多局部最大值的非凸函数，导致寻找最小值变得困难，所有我们选择了另一种能使LR变成凸函数的代价函数。</p><p><img src="https://s2.loli.net/2022/03/28/1gdrP678mFIhToq.png" alt="image-20220219104254098" style="zoom:80%;"></p><p>而对数函数log的曲线，能让代价函数变为凸函数的方程吗？</p><p>分析</p><p><img src="https://s2.loli.net/2022/03/28/ICF1KWedXqVLzDT.png" alt="image-20220219110405097" style="zoom:80%;"></p><p>化简</p><p><img src="https://s2.loli.net/2022/03/28/c6oIKGzs1MUZgpT.png" alt="image-20220219112119546" style="zoom:80%;"></p><p>得到如下结果，使用了==极大似然法==（能够在统计学中能为不同模型快速寻找参数），并且结果是凸函数</p><script type="math/tex; mode=display">\begin{align}J(\theta)&  = \frac{1}{m}\sum_{i=1}^mCost(h_{\theta}(x^{(i)},y^{(i)}))\\& = -\frac{1}{m}[\sum_{i=1}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]\end{align}</script><p>参数梯度下降：</p><p><img src="https://s2.loli.net/2022/03/28/m5EHc1V9MaerDTU.png" alt="image-20220219113754588" style="zoom:80%;"></p><p>==可以发现，求导后线性回归和逻辑回归的公式是一样的<strong>，</strong>但是他们的假设函数h(θ)是不同的，所以两个函数梯度下降公式是不同的==</p><p>求导sigmiod得到$\partial_{sigmoid}=sigmoid[1-sigmoid]$</p><h4 id="1-2-高级优化"><a href="#1-2-高级优化" class="headerlink" title="1.2 高级优化"></a>1.2 高级优化</h4><ul><li>共轭梯度法Conjugate Gradient</li><li>拟牛顿法中的对称正定迭代矩阵BFGS</li><li>近似BFGS，L-BFGS相对BFGS能够减少空间的使用</li></ul><h5 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h5><ul><li>无需设定学习率，学习率自动动态调整</li><li>大部分情况下速度比梯度下降法快很多</li></ul><h5 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h5><p>实现比梯度下降法复杂很多，但是基本上都有封装好的库，如python中的<code>scipy.optimize.fmin_bfgs</code></p><h4 id="1-3-逻辑回归的多分类任务"><a href="#1-3-逻辑回归的多分类任务" class="headerlink" title="1.3 逻辑回归的多分类任务"></a>1.3 逻辑回归的多分类任务</h4><p>训练多个逻辑回归分类器，然后将输入放到各分类器中，将输入归类为得分值最大的类别即可</p><h4 id="1-4-过拟合和欠拟合解决"><a href="#1-4-过拟合和欠拟合解决" class="headerlink" title="1.4 过拟合和欠拟合解决"></a>1.4 过拟合和欠拟合解决</h4><h5 id="1-4-1-过拟合"><a href="#1-4-1-过拟合" class="headerlink" title="1.4.1 过拟合"></a>1.4.1 过拟合</h5><ul><li>适当减少多余的参数</li><li>使用正则化，适当减少参数维度(阶/次方)/大小</li><li>增加数据量</li><li>dropout</li><li>清晰数据</li><li>提取终止训练</li></ul><h5 id="1-4-2-欠拟合"><a href="#1-4-2-欠拟合" class="headerlink" title="1.4.2 欠拟合"></a>1.4.2 欠拟合</h5><ul><li>增加特征和数据</li><li>增加高阶多项式项</li><li>减少正则化参数</li></ul><h4 id="1-5-正则化惩罚项"><a href="#1-5-正则化惩罚项" class="headerlink" title="1.5 正则化惩罚项"></a>1.5 正则化惩罚项</h4><p>加入惩罚项后，会降低高维参数的值，让他们趋于0（也就是==简化假设模型，限制模型参数，保持参数尽量小==），这样能让假设h函数变得更加的平滑</p><p><img src="https://s2.loli.net/2022/03/28/vmqSDy73hwxEAfM.png" alt="image-20220219164340840"></p><p>我们不知道哪些参数是高维的，该去降低哪些参数的维度，在代价函数中加入正则化惩罚项，对每个参数进行限制</p><h5 id="1-5-1-惩罚项公式以及作用"><a href="#1-5-1-惩罚项公式以及作用" class="headerlink" title="1.5.1 惩罚项公式以及作用"></a>1.5.1 惩罚项公式以及作用</h5><p>公式：<img src="https://s2.loli.net/2022/03/28/4NlhR2pBwMfGxCH.png" alt="image-20220219170024442"></p><p> <img src="https://s2.loli.net/2022/03/28/w64HTcyeGR7XqQW.png" alt="image-20220219222810433"></p><p>==简化假设模型，限制模型参数，保持参数尽量小==，$\lambda$作用是控制两个不同目标之间的取舍，设置合适的$\lambda$参数防止模型欠拟合或者无明显作用</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性回归、代价函数和梯度下降法</title>
      <link href="/posts/articletemplate3.html"/>
      <url>/posts/articletemplate3.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言，技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言。</div><p>线性回归预测函数：</p><script type="math/tex; mode=display">h_{\theta}(x)=\theta^{T}x</script><p>逻辑回归预测函数：</p><script type="math/tex; mode=display">h_{\theta}^{(x)}=\frac{1}{1+e^{-\theta^{T}x}}</script><p>线性回归损失函数：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h(\theta^{(i)}-y^i))^2+\frac{\lambda}{2m}\sum_{j=1}^m\theta_j^2</script><p>逻辑回归损失函数：</p><p>如果直接使用线性回归的<strong>MSE</strong>会让逻辑回归的代价函数变成<strong>非凸函数</strong>，这样就会导致有非常多的局部最优值，导致梯度下降法失效。所以引入了<strong>交叉熵损失函数</strong>来替代线性回归的MSE(均方误差)</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{2m}\sum_{i=1}^m(y^ilog{h_\theta(x^i)}+(1-y^i)log(1-h_\theta x^{(i)})^2+\frac{\lambda}{2m}\sum_{j=1}^m\theta_j^2</script><p>两者损失函数求导后，<strong>除了假设函数不一样，表示形式是一样的</strong>：</p><script type="math/tex; mode=display">\frac{\part J(\theta)}{\part{\theta_j}}=\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)})x_j^{(i)}</script><p><img src="https://s2.loli.net/2022/03/28/qBdCjLkpsAW7gcu.png" alt="image-20220301171851453" style="zoom:80%;"></p><p>损失函数中参数倍数变化并不会影响最优值的最终结果</p><h3 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1.线性回归"></a>1.线性回归</h3><p>两个变量：$y=wx+b$</p><p>多个变量：<strong>假设函数</strong>(hypothesis)：$h_\theta(x)=\theta^Tx=\theta_0x_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n$</p><p><img src="https://s2.loli.net/2022/03/28/CrHw2VAG8SYzvQB.png" alt="image-20220218112740442"></p><h4 id="1-1-代价函数-cost-function"><a href="#1-1-代价函数-cost-function" class="headerlink" title="1.1 代价函数(cost function)"></a>1.1 代价函数(cost function)</h4><ol><li>求(预测值-真实值)差的平方的和，也就是SSE的最小值$min\displaystyle\sum^{m}<em>{i=1}(\hat{y}_i-y</em>{i})^2$</li><li>均方差MSE：$J<em>{(w,b)}=\frac{1}{2m}\displaystyle\sum^{m}</em>{i=1}(\hat{y}<em>i-y</em>{i})^2$，为了方便计算，系数$\frac{1}{m}$换成$\frac{1}{2m}$</li></ol><p>根据x的不同系数w得损失曲线，<strong>根据最小的loss值得到对应系数w</strong></p><p><img src="https://s2.loli.net/2022/03/28/ScgUapGwnKtms7i.png" alt="image-20220215221913546" style="zoom:80%;"></p><p><img src="https://s2.loli.net/2022/03/28/KaIPbu18OlcHWir.png" alt="image-20220215230923601" style="zoom:80%;"></p><h4 id="1-2-梯度下降-迭代求最优值"><a href="#1-2-梯度下降-迭代求最优值" class="headerlink" title="1.2 梯度下降(迭代求最优值)"></a>1.2 梯度下降(迭代求最优值)</h4><p><strong>步长（学习率$\alpha$）</strong>决定了梯度下降的速度，梯度会下降到直至收敛convergence（也就是到局部最小值才停止），所以太大的步长会导致在坡底(<strong>局部最小值</strong>)震荡</p><p><strong>初始化起点</strong>也能影响梯度下降的速度和得到的局部最小值（<strong>局部最小值可能有很多个，初始化下降起点(也就是w和b)会影响局部最小值</strong>）。一般情况下，设置初始化<code>w, b = 0, 0</code></p><p><img src="https://s2.loli.net/2022/03/28/QuVXNgyfHtjGa5I.png" alt="image-20220215232421193" style="zoom:80%;"></p><p>梯度下降公式：<script type="math/tex">\theta_j=\theta_j-\alpha\frac{\partial{J(\theta_0,\theta_1)}}{\partial_{\theta_j}}\space\space\space(for j = 0 \space and \space j = 1)</script></p><h5 id="1-2-1-参数梯度下降实现步骤-方法"><a href="#1-2-1-参数梯度下降实现步骤-方法" class="headerlink" title="1.2.1 参数梯度下降实现步骤/方法"></a>1.2.1 参数梯度下降实现步骤/方法</h5><p><img src="https://s2.loli.net/2022/03/28/FNAP3ieLgwIaxl5.png" alt="image-20220217215555223"></p><p>正确的梯度更新应该是<strong>多个参数同步更新（先获取下降梯度再更新参数）</strong>，否则会影响在其他参数的更新，最终影响结果</p><p>如果刚好初始化值为局部最小值，则代价函数$J_\theta$的值为0</p><p>梯度下降时，学习率$\alpha$不需要变更，因为在梯度下降的过程中，代价函数的梯度$\partial_{J}$会随着慢慢下降而减小，所以梯度下降的速度也会减缓</p><p><img src="https://s2.loli.net/2022/03/28/YvAKqT3VrGIzsEF.png" alt="image-20220217223410230" style="zoom: 80%;"></p><p>线性回归的代价函数求导后得到(二元梯度下降)：</p><p>其中$\theta_{0}$为常数<img src="https://s2.loli.net/2022/03/28/6IkqDcxJjYpL35w.png" alt="image-20220217225105167"></p><p>MSE梯度下降公式：<img src="https://s2.loli.net/2022/03/28/gBysLWF3rZGExVO.png" alt="image-20220218215115716" style="zoom:80%;"></p><p>多元梯度下降：</p><p><img src="https://s2.loli.net/2022/03/28/T3XF17zNe5KVkZM.png" alt="image-20220218154711231" style="zoom:80%;"></p><h5 id="1-2-2-凸函数-convex-function-与线性回归"><a href="#1-2-2-凸函数-convex-function-与线性回归" class="headerlink" title="1.2.2 凸函数(convex function)与线性回归"></a>1.2.2 凸函数(convex function)与线性回归</h5><p><img src="https://s2.loli.net/2022/03/28/hyjdtB8Xlq4uLNo.png" alt="image-20220217230356998" style="zoom:80%;"></p><p>凸函数没有局部最优，只有一个全局最优，像这种函数，只要使用<strong>线性回归</strong>总是能收敛到全局最优</p><h5 id="1-2-3-批梯度下降法-Batch-Gradient-Descent"><a href="#1-2-3-批梯度下降法-Batch-Gradient-Descent" class="headerlink" title="1.2.3 批梯度下降法(Batch Gradient Descent)"></a>1.2.3 批梯度下降法(Batch Gradient Descent)</h5><p>考虑全局的一种方法，在线性回归中使用的MSE即均方差即是考虑了所有数据的一种BGD</p><h5 id="1-2-4-特征缩放-归一化"><a href="#1-2-4-特征缩放-归一化" class="headerlink" title="1.2.4 特征缩放/归一化"></a>1.2.4 特征缩放/归一化</h5><p>==归一化可以加快梯度下降的速度，也就是更快地收敛==</p><p><img src="https://s2.loli.net/2022/03/28/nb7cEaxNlXvMyR3.png" alt="image-20220218155743004" style="zoom: 67%;"></p><h6 id="1-2-4-1-均值归一化Mean-Normalization"><a href="#1-2-4-1-均值归一化Mean-Normalization" class="headerlink" title="1.2.4.1 均值归一化Mean Normalization"></a>1.2.4.1 均值归一化Mean Normalization</h6><script type="math/tex; mode=display">x=\frac{x-\mu}{x_{max}-x_{min}}</script><h5 id="1-2-5-小技巧"><a href="#1-2-5-小技巧" class="headerlink" title="1.2.5 小技巧"></a>1.2.5 小技巧</h5><blockquote><p>如何能够快速判断梯度下降是否正在有效工作/收敛呢？</p></blockquote><p>正确的学习率：<img src="https://s2.loli.net/2022/03/28/nbvBFYIiG38VEZo.png" alt="image-20220218161645087"></p><p>错误的学习率：<img src="https://s2.loli.net/2022/03/28/ZbzGvoDtc14rey3.png" alt="image-20220218162815860"></p><p>方法1：(推荐)运行过程中，根据<strong>迭代次数</strong>和<strong>代价函数的值/导数</strong>(下降速度)来判断梯度是否有效下降/收敛，也就是上述绘制曲线，通过看曲线的方式</p><p>方法2：设定一个阈值，当代价函数变化值小于该阈值则停止训练。但是该方式的缺点是通常这个阈值不好选择</p><h6 id="1-2-5-1-总结"><a href="#1-2-5-1-总结" class="headerlink" title="1.2.5.1 总结"></a>1.2.5.1 总结</h6><ul><li>$\alpha$学习率太小会导致梯度下降速度很慢</li><li>$\alpha$太大会导致梯度反向增长，震荡，甚至是收敛速度慢等</li></ul><p>设置较小的学习率总能收敛，但是速度会偏慢，通过观察运行时的曲线选择合适的学习率</p><p><img src="https://s2.loli.net/2022/03/28/J9C8WspuhiAtyRc.png" alt="image-20220218163635970"></p><h4 id="1-3-多项式回归和线性回归"><a href="#1-3-多项式回归和线性回归" class="headerlink" title="1.3 多项式回归和线性回归"></a>1.3 多项式回归和线性回归</h4><p>在选择特征时，可能有多个角度：如在房价预测时，你可以通过房子的纵深和宽度来计算影响因子，也可以通过面积来直接计算；根据模型/数据实际的效果来选择最合适的即可。</p><p>多项式拟合：<img src="https://s2.loli.net/2022/03/28/Nlj7FZAKzOBXVS6.png" alt="image-20220218170807251" style="zoom:80%;"></p><p>有时候我们能使用线性拟合的方式来得到多项式拟合的效果，如</p><p><img src="https://s2.loli.net/2022/03/28/gy5XRmeC8ObMBpw.png" alt="image-20220218171023479" style="zoom:80%;"></p><h4 id="1-4-正规方程-直接求解最优值-Norm-Equation"><a href="#1-4-正规方程-直接求解最优值-Norm-Equation" class="headerlink" title="1.4 正规方程(直接求解最优值)Norm Equation"></a>1.4 正规方程(直接求解最优值)Norm Equation</h4><p><img src="https://s2.loli.net/2022/03/28/S6P8On1GuzvUc49.png" alt="image-20220218202947319" style="zoom:80%;"></p><p>$\theta=(X^{T}X)^{-1}X^{T}y$该公式计算结果可以直接求得代价函数最小化的$\theta$，也就是算得其中一个参数系数的最优解</p><p>在使用了Norm Equation正规方程后，数据可以不用归一化处理，直接计算即可</p><h5 id="1-4-1-正规方程在不可逆情况下的解决方法"><a href="#1-4-1-正规方程在不可逆情况下的解决方法" class="headerlink" title="1.4.1 正规方程在不可逆情况下的解决方法"></a>1.4.1 正规方程在不可逆情况下的解决方法</h5><p>在Octave/Matlab中使用pinv(伪逆)/inv可以计算得到矩阵的逆，矩阵在一定条件下是不可逆的(矩阵的值为0，也就是某些特征之间存在线性关系，说明部分特征是多余的；样本太少，特征太多，适当减少特征或者使用正则化)，但是使用pinv仍然可以得到该矩阵的逆</p><p><img src="https://s2.loli.net/2022/03/28/l3tRJxmQ8wc2djN.png" alt="image-20220218212054877" style="zoom:80%;"></p><h4 id="1-5-梯度下降法VS正规方程"><a href="#1-5-梯度下降法VS正规方程" class="headerlink" title="1.5 梯度下降法VS正规方程"></a>1.5 梯度下降法VS正规方程</h4><div class="table-container"><table><thead><tr><th>\</th><th>梯度下降法Gradient Desent</th><th>正规方程Norm Equation</th></tr></thead><tbody><tr><td>缺点</td><td>1.需要设置学习率$\alpha$ 2.需要迭代升级</td><td>当参数比较大时，计算$\theta=(X^{T}X)^{-1}X^{T}y$的时间成本为$O(n^{3})$，但参数个数较小(10k以下?)时，速度会较梯度下降法快；对于一些复杂的学习算法，我们不得不使用梯度下降法来替代正规方程</td></tr><tr><td>优点</td><td>当参数非常大时依然能非常好地工作；在一些复杂算法中仍然适用，而正规方程只使用于特定的一些算法中，如线性回归等</td><td>不需要设置学习率$\alpha$；不需要迭代</td></tr></tbody></table></div><p><strong>自适应优化算法（如momentum-SGD、NAG、Adagrad、Adadelta、RMSprop）训练出来的结果通常都不如SGD，尽管这些自适应优化算法在训练时表现的看起来更好。 使用者应当慎重使用自适应优化算法。</strong></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最小二乘法</title>
      <link href="/posts/articletemplate2.html"/>
      <url>/posts/articletemplate2.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言，技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言。</div><p><a href="https://www.bilibili.com/read/cv14977249?spm_id_from=333.999.0.0">“损失函数”是如何设计出来的？直观理解“最小二乘法”和“极大似然估计法” - 哔哩哔哩 (bilibili.com)</a></p><h4 id="1-最小二乘法"><a href="#1-最小二乘法" class="headerlink" title="1.最小二乘法"></a>1.最小二乘法</h4><p>求模型的结果与真实值的差距（或者说是损失大小）</p><p>$\displaystyle\sum<em>{i=1}^n|\hat y_i-y_i|$，<strong>为了方便求导</strong>（梯度下降），我们可以将该算法设计成$min\displaystyle\sum</em>{i=1}^n\frac{1}{2}(\hat y_i-y_i)^2$</p><h4 id="2-极大似然函数"><a href="#2-极大似然函数" class="headerlink" title="2.极大似然函数"></a>2.极大似然函数</h4><p>$L(\theta)$似然函数，$H(\theta)$熵</p><p>求解极大似然函数需要知道数据的<strong>分布</strong>，然后根据概率求参数，最后求解</p><h4 id="3-最小二乘和极大似然"><a href="#3-最小二乘和极大似然" class="headerlink" title="3.最小二乘和极大似然"></a>3.最小二乘和极大似然</h4><p>在正态分布上两者的损失函数是相同的</p><h4 id="4-极大似然和交叉熵"><a href="#4-极大似然和交叉熵" class="headerlink" title="4.极大似然和交叉熵"></a>4.极大似然和交叉熵</h4><p>除去量纲等，两者计算得到的效果是相同的</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Focal Loss和Balanced CE(样本比例不均衡问题)</title>
      <link href="/posts/articletemplate.html"/>
      <url>/posts/articletemplate.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言，技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言。</div><h4 id="1-信息量"><a href="#1-信息量" class="headerlink" title="1.信息量"></a>1.信息量</h4><p>当越不可能的事件或者相关程度越高的事件（今天中午总统吃什么，与我们相关程度低，信息量小；但是对于想应聘总统厨师的人来说，这件事的信息量就很大）发生了，我们获取到的信息量就越大，反之信息量越小。</p><p>样本不足会导致模型信息量不足，从而不能很好的拟合数据</p><h4 id="2-熵"><a href="#2-熵" class="headerlink" title="2.熵"></a>2.熵</h4><p>熵用来表示一个系统中所有信息量的期望，也可以用来表示一个系统的混乱程度</p><h4 id="3-相对熵"><a href="#3-相对熵" class="headerlink" title="3.相对熵"></a>3.相对熵</h4><p>用来衡量两个分布的差异，或者说是一个分布变换到另一个分布需要的信息增量</p><p>在机器学习中，预测分布Q在训练的过程中信息量不足，虽然可以大致描述，但是描述得有偏差，需要额外的一些信息增量才能达到和样本真实分布一样的描述。经过反复训练后，信息量足够描述后就不需要额外的信息增量了。</p><p>$相对熵=p的熵-pq交叉熵$：<img src="https://s2.loli.net/2022/03/28/wsnqF3CvR5AdTuk.png" alt="image-20220313143521524" style="zoom:80%;"></p><h4 id="4-交叉熵"><a href="#4-交叉熵" class="headerlink" title="4.交叉熵"></a>4.交叉熵</h4><p>评估标签和预测值之间的差距，而相对熵中p的熵是不变的，所以只需关注交叉熵即可</p><p>在机器学习中常用交叉熵作loss</p><h4 id="5-交叉熵应用"><a href="#5-交叉熵应用" class="headerlink" title="5.交叉熵应用"></a>5.交叉熵应用</h4><h5 id="5-1-单标签多分类任务"><a href="#5-1-单标签多分类任务" class="headerlink" title="5.1 单标签多分类任务"></a>5.1 单标签多分类任务</h5><p>一张图片只被归为一个标签，对应的一个batch的loss就是：</p><p><img src="https://s2.loli.net/2022/03/28/LnsDHy9JFiK7Gv5.png" alt="image-20220313144748836"></p><h5 id="5-2-多标签多分类任务"><a href="#5-2-多标签多分类任务" class="headerlink" title="5.2 多标签多分类任务"></a>5.2 多标签多分类任务</h5><p>一张图片可能会被归为多个标签，每个Label都是独立分布的，可以用交叉熵对每个独立的类别进行计算，每个类别只有是或不是两种可能，服从弄二项分布，每个类别对应的交叉熵为：</p><p><img src="https://s2.loli.net/2022/03/28/vPKTVdGZ4MWmpIX.png" alt="image-20220313145145469"></p><p>如果一张图片中同时存在青蛙和老鼠，且预测结果如下：</p><div class="table-container"><table><thead><tr><th style="text-align:left">*</th><th style="text-align:left">猫</th><th style="text-align:left">青蛙</th><th style="text-align:left">老鼠</th></tr></thead><tbody><tr><td style="text-align:left">Label</td><td style="text-align:left">0</td><td style="text-align:left">1</td><td style="text-align:left">1</td></tr><tr><td style="text-align:left">Predicted</td><td style="text-align:left">0.1</td><td style="text-align:left">0.7</td><td style="text-align:left">0.8</td></tr></tbody></table></div><p>则$loss=loss<em>猫+loss</em>蛙+loss_鼠$，而损失方式计算如下：</p><p><img src="https://s2.loli.net/2022/03/28/sZQKVfOXG24TtBU.png" alt="image-20220313145344159" style="zoom:80%;"></p><blockquote><p>总结</p></blockquote><p>对于多分类任务(包括二分类)的交叉熵损失为：</p><p><img src="https://s2.loli.net/2022/03/28/ecYqxWb9SkAUpM8.png" alt="image-20220313150602363" style="zoom:80%;"></p><p>其中$y_i$表示真实概率，$p_i$表示预测概率</p><h4 id="6-focal-loss"><a href="#6-focal-loss" class="headerlink" title="6. focal loss"></a>6. focal loss</h4><p>focal loss最初用于图像领域解决是数据不平衡造成的模型性能问题。</p><h5 id="6-1-样本不均衡问题"><a href="#6-1-样本不均衡问题" class="headerlink" title="6.1 样本不均衡问题"></a>6.1 样本不均衡问题</h5><p>例如，在欺诈识别的案例中，好坏样本的比例为10000 : 1，这样模型很容易学习到一个把所有样本都预测为好的模型，也就是模型没有拟合到极大似然，而是只学习到了<strong>先验</strong>(样本分布)，导致模型欠拟合。</p><blockquote><p>影响</p></blockquote><p>样本不均衡带来的根本影响是：模型会学习到样本比例这个先验信息，类别不均衡下的分类边界会侵占少数样本类的区域，也就是影响模型学习的更本质的特征，影响模型的鲁棒性。</p><p><img src="https://s2.loli.net/2022/03/28/rQwop4XLc5umFDe.png" alt="img" style="zoom: 33%;"></p><blockquote><p>问题分析</p></blockquote><p><strong>减少模型学习样本分布(先验信息)</strong>，<strong>让模型学习数据的本质特征</strong>，这样就能解决样本不均衡问题。</p><blockquote><p>必要性</p></blockquote><p>从分类效果出发，不均衡对于分类结果的影响不一定是不好的（除了在预测精度要求比较高等环境下），什么时候需要解决样本不均衡(抑制先验影响)呢？</p><ul><li>判断任务是否复杂：任务的复杂度越高，对样本不均衡越敏感（特征量、噪音等都和任务的复杂度相关）</li><li>训练样本分布与真实样本分布不一致</li><li>不均衡样本中占少数的那个类别数量是不是实在太少，导致模型学习不到好的特征。<ul><li>解决方法<ul><li>解决样本不均衡的情况</li><li>使用一些数据增强的方法</li><li>尝试像异常检测这种的<strong>单分类模型</strong></li></ul></li></ul></li></ul><blockquote><p>样本不均衡问题解决</p></blockquote><p>在学习任务有些难度的情况下，我们可以通过一些方法使得不同类别的样本对模型学习时的loss贡献权重均衡，从而<strong>消除模型对不同类别的偏向性</strong>，学到更为本质的特征。</p><p>我们现在就开始探讨这些解决方法：</p><h6 id="6-1-1-样本层面"><a href="#6-1-1-样本层面" class="headerlink" title="6.1.1 样本层面"></a>6.1.1 样本层面</h6><ul><li><p>欠采样和过采样</p><ul><li>欠采样：减少多数类的数量（如随机欠采样、NearMiss、ENN等）</li><li>过采样：尽量多地增加少数类的样本数量（如随机过采样、数据增强等），使得类别间项目均衡</li><li>混合采样：如smote+ENN</li></ul></li><li><p>数据增强</p><ul><li><p>单样本增强(主要用于图像)：几何操作(翻转缩放)、颜色变换、随机擦除(裁剪)、添加噪声等方法，<code>imgaug</code>库</p></li><li><p>多样本增强：通过组合及转换多个样本，主要有Smote、SamplePairing、Mixup等方法，在特征空间内构造已知样本的邻域值样本</p><p><img src="https://s2.loli.net/2022/03/28/MDu2JCfZFB4ctGi.png" alt="image-20220313175211682" style="zoom:50%;"></p></li><li><p>基于深度学习的数据增强</p><ul><li><p>生成模型如<strong>变分自编码网络</strong>(VAE)和<strong>生成对抗网络</strong>(GAN)</p><p><img src="https://s2.loli.net/2022/03/28/Q2lLNr1bMYDixdC.png" alt="image-20220313175505963" style="zoom:50%;"></p></li></ul></li></ul></li></ul><blockquote><p>缺点及解决方案</p></blockquote><ul><li>随机欠采样可能会导致丢弃含有重要信息的样本，在计算性能足够的情况下，可以考虑根据数据分布的采样方法(通常是基于距离的邻域关系)，如ENN、NearMiss等</li><li>随机过采样或数据增强样本也有可能是引入片面噪声，导致过拟合；也可能是引入信息量不大的样本。此时需要考虑调整采样方法，或者通过半监督算法(可借鉴Pu-Learning思路)选择增强数据的较优子集，以提高模型的泛化能力</li></ul><h6 id="6-1-2-损失函数层面"><a href="#6-1-2-损失函数层面" class="headerlink" title="6.1.2 损失函数层面"></a>6.1.2 损失函数层面</h6><p>损失函数层面主流的方法就是代价敏感学习(cost-sensitive)，即为不同分类损失给予不同的惩罚力度(权重)，在调节类别平衡的同时，也不会增加计算复杂度。</p><ul><li><p>常用方法</p><ul><li><p>class weight：<code>scikit</code>库中内置的方法，可以为不同类别的样本提供不同的权重(少数类的权重更高)，从而平衡各类别的学习。如<code>clf2=LogisticRegression(class_weight=&#123;0:1,1:10&#125;)  # 代价敏感学习</code>为少数类分配更高的权重，以避免决策偏重多数类的现象（类别权重除了设定balanced，还可以作为一个超参搜索）</p><p><img src="https://s2.loli.net/2022/03/28/4YrXUtIhokPuGVW.png" alt="image-20220313202920598" style="zoom: 50%;"></p></li><li><p>OHEM和Focal Loss：类别的不平衡可以归结为难易样本的不平衡，而难易样本的不平衡可以归结为梯度的不平衡，OHEM和Focal Loss都做了两件事：难样本（错分类(或者说是高损失)的样本）挖掘和类别的平衡。（另外还有GHM、PISA等方法）</p><ul><li><p>OHEM(Online Hard Example Mining)算法的核心是选择一些hard examples（多样性和高损失的样本）作为训练样本，针对性地改善模型学习效果。对于数据类别不平衡问题，OHEM的针对性更强。</p></li><li><p>Focal loss的核心思想是在交叉熵损失函数的基础上，<strong>增加了类别的不同权重以及困难（高损失）样本的权重</strong>，以改善模型学习效果。<strong>$p_t$表示与真实值接近程度</strong>，越大越接近，即分类越准确（<strong>也可以说$p_t$反映了分类的难易程度</strong>。$p_t$越大，说明分类置信度越高，样本越易分；反之越难分）。系数$(1-p_t)^\gamma$是<strong>调节因子</strong>，相比交叉熵损失，==Focal Loss对于分类不准确的样本损失没有变化，对于分类准确的样本损失会变小==。整体而言，<strong>相当于增加了分类不准确样本的权重</strong>。</p><p><img src="https://s2.loli.net/2022/03/28/liZtSjE6mXNIU5K.png" alt="image-20220313203941650" style="zoom: 67%;"></p></li><li><p>平衡交叉熵函数：合理分配权重，平衡损失函数分布，即在损失函数中增加惩罚项$\alpha$</p><p><img src="https://s2.loli.net/2022/03/28/T4cBRsOdSVH3DkJ.png" alt="image-20220313205241348" style="zoom:67%;">，其中$\frac{\alpha}{1-\alpha}=\frac{n}{m}$，权重根据样本分布设置</p></li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最小二乘法</title>
      <link href="/posts/e.html"/>
      <url>/posts/e.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言，技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言。</div><p><a href="https://www.bilibili.com/read/cv14977249?spm_id_from=333.999.0.0">“损失函数”是如何设计出来的？直观理解“最小二乘法”和“极大似然估计法” - 哔哩哔哩 (bilibili.com)</a></p><h4 id="1-最小二乘法"><a href="#1-最小二乘法" class="headerlink" title="1.最小二乘法"></a>1.最小二乘法</h4><p>求模型的结果与真实值的差距（或者说是损失大小）</p><p>$\displaystyle\sum<em>{i=1}^n|\hat y_i-y_i|$，<strong>为了方便求导</strong>（梯度下降），我们可以将该算法设计成$min\displaystyle\sum</em>{i=1}^n\frac{1}{2}(\hat y_i-y_i)^2$</p><h4 id="2-极大似然函数"><a href="#2-极大似然函数" class="headerlink" title="2.极大似然函数"></a>2.极大似然函数</h4><p>$L(\theta)$似然函数，$H(\theta)$熵</p><p>求解极大似然函数需要知道数据的<strong>分布</strong>，然后根据概率求参数，最后求解</p><h4 id="3-最小二乘和极大似然"><a href="#3-最小二乘和极大似然" class="headerlink" title="3.最小二乘和极大似然"></a>3.最小二乘和极大似然</h4><p>在正态分布上两者的损失函数是相同的</p><h4 id="4-极大似然和交叉熵"><a href="#4-极大似然和交叉熵" class="headerlink" title="4.极大似然和交叉熵"></a>4.极大似然和交叉熵</h4><p>除去量纲等，两者计算得到的效果是相同的</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Focal Loss和Balanced CE(样本比例不均衡问题)</title>
      <link href="/posts/f.html"/>
      <url>/posts/f.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言，技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言。</div>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>这是随笔文章模板</title>
      <link href="/posts/essay-demo.html"/>
      <url>/posts/essay-demo.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言，技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言。</div><h4 id="这是博主的随笔板块，后续会更新文章，目前弄好了模板格式"><a href="#这是博主的随笔板块，后续会更新文章，目前弄好了模板格式" class="headerlink" title="这是博主的随笔板块，后续会更新文章，目前弄好了模板格式~"></a>这是博主的随笔板块，后续会更新文章，目前弄好了模板格式~</h4><div class="ipage">    <div class="ititle">节选自《你若安好便是晴天》- 前言</div>  <div class="izhengwen">  <p>几场梅雨，几卷荷风，江南已是烟水迷离。小院里湿润的青苔在雨中纯净生长。这个季节，许多人都在打听关于莲荷的消息，以及茉莉在黄昏浮动的神秘幽香。不知多少人会记得有个女子，曾经走过人间四月天，又与莲开的夏季有过相濡以沫的约定。</p>  <p>一个人，一本书，一杯茶，一帘梦。有时候，寂寞是这样叫人心动，也只有此刻，世事才会如此波澜不惊。凉风吹起书页，这烟雨让尘封在书卷里的词章和故事弥漫着潮湿的气息。独倚幽窗，看转角处的青石小巷，一柄久违的油纸伞，遮住了低过屋檐的光阴。</p>  <p>时光微凉，那一场远去的往事被春水浸泡，秋风吹拂，早已洗去铅华，清绝明净。以为历经人生匆匆聚散，尝过尘世种种烟火，应该承担岁月带给我们的沧桑。可流年分明安然无恙，而山石草木是这样毫发无伤。只是曾经许过地老天荒的城，在细雨中越发地清瘦单薄。</p>  <p>青梅煎好的茶水，还是当年的味道；而我们等候的人，不会再来。后来才知道，那一袭素色白衣的女子已化身为燕，去寻觅水乡旧巢。她走过的地方，有一树一树的花开，她呢喃的梁间，还留着余温犹存的梦。有人说，她是个冰洁的女子，所以无论人世如何变迁，她都有着美丽的容颜。有人说，她是个理智的女子，不管面临怎样的诱惑，最后都可以全身而退。</p>  <p>她叫林徽因，出生于杭州，是许多人梦中期待的白莲。她在雨雾之都伦敦，发生过一场空前绝后的康桥之恋。她爱过三个男子，爱得清醒，也爱得平静。徐志摩为她徜徉在康桥，深情地等待一场旧梦可以归来。梁思成与她携手走过千山万水，为完成使命而相约白头。金岳霖为她终身不娶，痴心不改地守候一世。可她懂得人生飘忽不定，要学会随遇而安。</p>  </div></div>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
