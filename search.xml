<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>拉格朗日乘子法和KKT条件</title>
      <link href="/posts/articletemplate6.html"/>
      <url>/posts/articletemplate6.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">拉格朗日乘子法和KKT条件</div><div class="note info">前言，技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言。</div><p>如果两个模型的概率分布是不一样的，所以在衡量模型的差异的时候，不能直接定性衡量两个模型之间的差异，而是需要<strong>定量</strong>的衡量两个模型的差异（<strong>比如极大似然估计、最小二乘法和交叉熵</strong>）</p><h4 id="1-信息量"><a href="#1-信息量" class="headerlink" title="1.信息量"></a>1.信息量</h4><p><strong>信息量</strong>可以理解成一个事件从不确定变成确定的<strong>难度程度(概率)</strong>（或者说事件对某个人带来的价值大小）。难度越大，信息量越大。比如，阿根廷进入8强到赢得决赛的难度为$\frac{1}{2^3}$，则信息量为3比特，再比如中国队从8强赢得决赛的难度为$\frac{1}{2^{10}}$，则信息量为10比特。</p><p>公式为：$I(i)=-log_2p_i$，log以2为底因为最后要以<strong>比特</strong>表示信息量</p><p><img src="https://s2.loli.net/2022/03/28/WVw98ENXlZQIdnL.png" alt="image-20220306204926864" style="zoom:80%;"></p><h4 id="2-熵"><a href="#2-熵" class="headerlink" title="2.熵"></a>2.熵</h4><p><strong>熵</strong>衡量一个系统中的所有事件从不确定到确定的难度大小。对整体的概率模型进行一个衡量，衡量结果能反映出这个概率模型的不确定程度/混乱程度，熵是信息量的<strong>期望</strong>。</p><p>事件的不确定性越高，则信息量越高，即信息量函数$f$与概率$P$成<strong>负相关</strong>，即$f(P)=log\frac{1}{P}=-log(P)$</p><p>两个<strong>独立事件</strong>所产生的信息量等于各自信息量之和，即$f(P_1,P_2)=f(P_1)+f(P_2)$</p><p><strong>信息熵表示的是信息量的期望</strong>，即$H(P)=E[-log(P)]=\displaystyle\sum_XP(X)f(X)=-\displaystyle\sum_XP(X)logP(X)$</p><p>可以证明：$0 \le H(P) \le log\abs{X}$，所以当且仅当$X$的分布为均匀分布时有$H(P)=log\abs{X}$，即$P(X)=\frac{1}{\abs{X}}$时熵最大</p><h5 id="2-1-最大熵原理"><a href="#2-1-最大熵原理" class="headerlink" title="2.1 最大熵原理"></a>2.1 最大熵原理</h5><ol><li>最大熵<code>Max Entropy</code>原理：学习概率模型时，在所有可能的概率分布(模型)中，熵最大的分布(模型)是最好的模型（概率最均匀）<ul><li>通常会有<strong>其他已知条件</strong>来确定概率模型的<strong>集合</strong>，因此最大熵的原理是：在满足已知条件的情况下，选取熵最大的模型</li><li>在满足已知条件的前提下，如果没有其他信息，则不确定部分都是<strong>等可能</strong>的（均匀分布时，熵最大）。而这种等可能性就是由熵最大化得到的</li></ul></li><li>最大熵原理选取熵最大的模型，而决策树的目的是得到熵最小的划分。原因在于：<ul><li>最大熵原理认为在满足已知条件之后，选择不确定性最大(即不确定部分都是等可能的)的模型。即不再施加已知条件之外的约束。这是<strong>求最大不确定性的过程</strong></li><li>决策树的划分目标是为了通过不断划分从而降低实例所属的类的不确定性，最终给实例一个合适的分类。这是一个<strong>不确定性不断减小的过程</strong></li></ul></li></ol><h5 id="2-2-在期望约束下的最大熵模型"><a href="#2-2-在期望约束下的最大熵模型" class="headerlink" title="2.2 在期望约束下的最大熵模型"></a>2.2 在期望约束下的最大熵模型</h5><p>期望的约束表示为：$E[f(X)]=\displaystyle\sum_XP(X)f(X)=\tau$，其中$f(X)$是约束条件，$E(f(X))$是一个常数。</p><p>假设有k个约束条件：<img src="https://s2.loli.net/2022/03/28/NsBQ51TngdDSoYf.png" alt="image-20220316163251704" style="zoom: 67%;"></p><p>即求解约束最优化问题：<img src="https://s2.loli.net/2022/03/28/mNzuywELrvO1sUa.png" alt="image-20220316163435147" style="zoom:67%;"></p><p>利用拉格朗日乘子法求解：<img src="https://s2.loli.net/2022/03/28/PwvdbYIi4WkLtZ7.png" alt="image-20220316163546810" style="zoom:67%;"></p><p>解得：<img src="https://s2.loli.net/2022/03/28/eVHUlKsRfZpyMaE.png" alt="image-20220316163627242" style="zoom:67%;"></p><h5 id="2-3-香农熵Shannon-entropy"><a href="#2-3-香农熵Shannon-entropy" class="headerlink" title="2.3 香农熵Shannon entropy"></a>2.3 香农熵Shannon entropy</h5><script type="math/tex; mode=display">H(p)=-Ep[logp]=\left\{\begin{matrix} H(p)=-\int_xP(x)logP(x)dx \\ H(p)=-\displaystyle\sum_xP(x)logP(x)\end{matrix}\right.</script><p>香农熵就是在连续分布和离散分布中，对信息量的期望</p><p>公式为：熵=事件的概率x事件信息量的和，即各事件对系统贡献的信息量$H(x)=-\displaystyle \sum_{i=1}^nP(x_i)log{P(x_i)}$</p><p>比如，对于二分类(或者是二项分布)的任务，$H(x)=-P(x)logP(x)-(1-P(x))log(P(x))$</p><p><img src="https://s2.loli.net/2022/03/28/zMEqduinWc2Zg6P.png" alt="image-20220306205005670" style="zoom: 80%;"></p><h4 id="3-相对熵-KL散度"><a href="#3-相对熵-KL散度" class="headerlink" title="3.相对熵(KL散度)"></a>3.相对熵(KL散度)</h4><blockquote><p>通信/编码角度的理解</p></blockquote><ul><li>$H(P)$为服从$P(X)$分布的信源$X$的<strong>信息熵</strong>，<strong>也表示对信源$X$编码所需的平均比特数</strong></li><li>$H(P,Q)$称为交叉熵，<strong>表示对服从$P(X)$分布的信源$X$，按照分布$Q(X)$来进行编码所需的平均比特数</strong>，也表示<strong>利用分布$Q(X)$来表示服从$P(X)$分布的$X$的困难程度</strong></li><li>$D(P||Q)$为相对熵，表示用$Q(X)$来对信源$X$编码平均所需要的<strong>额外比特数</strong></li></ul><p>对于同一随机变量x，有两个独立的概率分布，我们可以用KL散度来衡量两个分布的差异。机器学习中，当我们不知道一个模型时，没办法直接求熵，需要依靠相对熵来计算两个模型的差异，也就是loss值（P往往表示样本的真实分布，Q表示预测模型的分布）</p><p><img src="https://s2.loli.net/2022/03/28/9PAGoezN4SJ6Rnv.png" alt="image-20220308160033264" style="zoom:80%;"></p><p>令<img src="https://s2.loli.net/2022/03/28/dpjvkG6lqgT4tPU.png" alt="image-20220308160141012" style="zoom:80%;"></p><p>则相对熵可以写成以下形式：</p><p><img src="https://s2.loli.net/2022/03/28/WzNpQDZHKVraJgt.png" alt="image-20220308160220771" style="zoom:80%;"></p><p>根据吉布斯不等式，$D_{KL}$中$H(P,Q)-H(P)≥0$</p><p>仅当两个模型完全相等时$D(P||Q)=0$，<strong>有差异时$D(P||Q)&gt;0$</strong></p><p>交叉熵越小，表示两个模型越相近，或者说转码不会有过多冗余比特</p><p><strong>因为P的熵是确定的，所以求KL散度(相对熵)变成了求交叉熵的问</strong></p><p>上述讨论的相对熵是在事件数(样本量)一样的情况下，当模型事件数量(样本量)不一样的时候取事件数多的那个</p><p><img src="https://s2.loli.net/2022/03/28/IkcOuM6T5C92LJN.png" alt="image-20220308162326928" style="zoom:80%;"></p><h4 id="4-交叉熵"><a href="#4-交叉熵" class="headerlink" title="4.交叉熵"></a>4.交叉熵</h4><p>在分类问题中，<strong>通常使用交叉熵损失度量标签的真实分布和由分类器预测的分布之间的差异</strong>。要找两个模型最优值，就是要找交叉熵最小的情况。</p><p><img src="https://s2.loli.net/2022/03/28/3MP8G2h9wj7kZOV.png" alt="image-20220308161727614" style="zoom:80%;"></p><p>一般来说<strong>以P为样本数据分布，Q为待优化的预测分布</strong></p><p>在机器学习当中，我们对模型的训练实际上就是一个参数估计的过程。我们<strong>对模型的参数进行调整的过程就是调整模型$Q(X)$来逼近真实数据$P(X)$的优化过程</strong></p><h5 id="4-1-交叉熵与极大似然估计"><a href="#4-1-交叉熵与极大似然估计" class="headerlink" title="4.1 交叉熵与极大似然估计"></a>4.1 交叉熵与极大似然估计</h5><p>极大似然估计<img src="https://s2.loli.net/2022/03/28/P49qVElO2CpgUmc.png" alt="image-20220307111247148" style="zoom:80%;">等价于最小化负对数似然<img src="/posts/articletemplate6.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\熵、信息量、KL散度、交叉熵.assets\image-20220307111346317.png" alt="image-20220307111346317" style="zoom:80%;"></p><p>这与逻辑回归中，用极大似然估计推出的损失函数在形式上是一样的，但是实际意义上是不一样的</p><ul><li><p><strong>极大似然估计中的log是为了将连乘计算量简化为连加</strong></p><p>极大似然估计：<img src="https://s2.loli.net/2022/03/28/r3hnMjH4RobY2KA.png" alt="image-20220307115957278" style="zoom:80%;"></p><p>极大对数似然估计：<img src="https://s2.loli.net/2022/03/28/pzJtFPw1xgvWbT2.png" alt="image-20220307120100501" style="zoom:80%;"></p><p>$log(xyz)=log(x)+log(y)+log(z)$；熵中则是为了计算概率对应的信息量引入-log</p></li><li><p>而且<strong>一个是有量纲，一个是没有量纲的</strong>（交叉熵中的信息量是有量纲(比特)的，但是极大似然估计中是没有的）</p></li><li><p>而且极大似然估计中求的是极大值，LR中强行加了一个负号，使其变成了最小值；熵中是为了计算困难程度对应的概率引入-log（如夺冠的概率为$\frac{1}{8}$，最后夺冠了，则信息量为$-log_2(\frac{1}{8})=3比特$）</p></li></ul><p>我的博客即将同步至腾讯云+社区，邀请大家一同入驻：<a href="https://cloud.tencent.com/developer/support-plan?invite_code=18e4kybkdple">https://cloud.tencent.com/developer/support-plan?invite_code=18e4kybkdple</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(now!)NLP发展到Transformer相关及改进模型</title>
      <link href="/posts/articletemplate21.html"/>
      <url>/posts/articletemplate21.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">(now!)NLP发展到Transformer相关及改进模型</div><h3 id="0-NLP"><a href="#0-NLP" class="headerlink" title="0. NLP"></a>0. NLP</h3><h4 id="0-1-发展"><a href="#0-1-发展" class="headerlink" title="0.1 发展"></a>0.1 发展</h4><p><a href="https://www.jianshu.com/p/d35a2fd593eb">https://www.jianshu.com/p/d35a2fd593eb</a></p><h4 id="0-2-训练流程"><a href="#0-2-训练流程" class="headerlink" title="0.2 训练流程"></a>0.2 训练流程</h4><h3 id="1-RNN和LSTM"><a href="#1-RNN和LSTM" class="headerlink" title="1. RNN和LSTM"></a>1. RNN和LSTM</h3><p><img src="https://s2.loli.net/2022/03/28/gyZSjrW5xHMX2Fh.png" alt="image-20220317203059808" style="zoom: 33%;"></p><p>单词的先后顺序会影响句子的意思，RNN擅长捕捉序列关系，不过对于翻译来说，句子间的单词数量不是一一对应的。</p><h4 id="1-1-缺点"><a href="#1-1-缺点" class="headerlink" title="1.1 缺点"></a>1.1 缺点</h4><ul><li>串行结构，计算速度慢</li><li>梯度爆炸和梯度消失</li></ul><h4 id="1-2-梯度爆炸和梯度消失问题"><a href="#1-2-梯度爆炸和梯度消失问题" class="headerlink" title="1.2 梯度爆炸和梯度消失问题"></a>1.2 梯度爆炸和梯度消失问题</h4><p>见[面试.md]</p><h4 id="1-3-LSTM"><a href="#1-3-LSTM" class="headerlink" title="1.3 LSTM"></a>1.3 LSTM</h4><p>长短期记忆网络相对于RNN，多了一个遗忘门，会适当删减(筛选)前置特</p><h4 id="1-4-存在问题"><a href="#1-4-存在问题" class="headerlink" title="1.4 存在问题"></a>1.4 存在问题</h4><p>受限于结构，RNN只能处理<code>N to N</code>，<code>1 to N</code>和<code>N to 1</code>问题，但是无法处理<code>N to M</code>的问题？</p><hr><h3 id="2-Word2Vec和Seq2Seq"><a href="#2-Word2Vec和Seq2Seq" class="headerlink" title="2. Word2Vec和Seq2Seq"></a>2. Word2Vec和Seq2Seq</h3><p><img src="https://s2.loli.net/2022/03/28/L5eAP6CJUgc9pxf.png" alt="image-20220317203445862" style="zoom: 33%;"></p><p><code>Seq2Seq</code>是一个拥有编码器<code>Encoder</code>和解码器<code>Decoder</code>的模型，<code>Encoder</code>和<code>Decoder</code>依然是RNN网络，不过在<code>Seq2Seq</code>模型中先由<code>Encoder</code>提取原始句子的意义，再由<code>Decoder</code>将意义转化成对应的语言，依靠意义这一中介，<code>Seq2Seq</code>解决了两端单词数不对等的情况。</p><p>意义单元能够存储的信息是有限的，如果一个句子太长，翻译精度就会下降？</p><hr><h3 id="3-Attention"><a href="#3-Attention" class="headerlink" title="3. Attention"></a>3. Attention</h3><h4 id="3-1-描述"><a href="#3-1-描述" class="headerlink" title="3.1 描述"></a>3.1 描述</h4><p>每个阶段我们关注到的内容都有所不同</p><p>机器翻译中，<img src="https://s2.loli.net/2022/03/28/YfS6s2z9PIy7VnD.png" alt="image-20220317205638828" style="zoom: 25%;">写出每个英文单词时，我们会格外注意其中的一部分汉字</p><p>而<code>Attention</code>可以从纷繁复杂的输入信息中，找出对当前输出最重要的部分<img src="https://s2.loli.net/2022/03/28/GFd3DfI4PbsH68c.png" alt="image-20220317210035081" style="zoom: 25%;"></p><h4 id="3-2-工作原理"><a href="#3-2-工作原理" class="headerlink" title="3.2 工作原理"></a>3.2 工作原理</h4><p><code>Q</code>是<code>QUERY</code>，表示输入信息；<code>Key</code>和<code>Value</code>成对出现，通常原始文本（输入）的信息。通过<strong>计算<code>Q</code>与<code>K</code>之间的相关性</strong>，得出不同的<code>K</code>对输出的重要程度，再与对应的<code>V</code><strong>相乘求和</strong>，就得到了<code>Q</code>的输出。</p><p><img src="https://s2.loli.net/2022/03/28/oQ6g9LGiRf3nMPv.png" alt="image-20220317210904542" style="zoom: 50%;"></p><h5 id="3-2-1-例子"><a href="#3-2-1-例子" class="headerlink" title="3.2.1 例子"></a>3.2.1 例子</h5><p>以阅读理解为例，<code>Q</code>是问题，<code>K</code>和<code>V</code>是原始文本，计算<code>Q</code>与<code>K</code>的相关性，让我们找到文本中最需要注意的部分，利用<code>V</code>得到答案。</p><p><img src="https://s2.loli.net/2022/03/28/9WxZ5Pi7erRVLop.png" alt="image-20220317211208013" style="zoom: 50%;"></p><p>在机器翻译中，<code>Q</code>是翻译结果，<code>K</code>是输入文本，<code>V</code>是对应语义</p><p><img src="/posts/articletemplate21.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\transformer.assets\image-20220317204916755.png" alt="image-20220317204916755" style="zoom:50%;"></p><p>在<code>Seq2Seq</code>的基础结构上，在<code>Decoder</code>生成每个单词时，都有意识的从原始句子中提取生成该单词最重要的信息，摆脱了输入序列的长度限制。</p><p>但是在该模型中，RNN需要逐个计算相关性，计算速度太慢了？</p><hr><h3 id="4-Self-Attention"><a href="#4-Self-Attention" class="headerlink" title="4. Self-Attention"></a>4. Self-Attention</h3><p>只关注输入序列元素之间的关系，通过将输入序列直接转化为<code>Q、K、V</code>，然后在内部进行<code>Attention</code>计算，就能很好捕捉文本的内在联系，对其作出再表示</p><hr><h3 id="5-Multi-Head-Attention"><a href="#5-Multi-Head-Attention" class="headerlink" title="5. Multi-Head Attention"></a>5. Multi-Head Attention</h3><p>使用多种变换生成不同的<code>Q、K、V</code>进行运算，再将他们对相关语句的意义的结论综合起来，进一步增强<code>Self-Attention</code>的效果</p><hr><h3 id="6-Transformer-Bert-GPT"><a href="#6-Transformer-Bert-GPT" class="headerlink" title="6. Transformer(Bert+GPT)"></a>6. Transformer(Bert+GPT)</h3><h4 id="6-0-简介"><a href="#6-0-简介" class="headerlink" title="6.0 简介"></a>6.0 简介</h4><h4 id="6-1-Transformer和CNN对比-图像领域"><a href="#6-1-Transformer和CNN对比-图像领域" class="headerlink" title="6.1 Transformer和CNN对比(图像领域)"></a>6.1 Transformer和CNN对比(图像领域)</h4><p><img src="/posts/articletemplate21.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\transformer.assets\image-20220318122215704.png" alt="image-20220318122215704"></p><ul><li>在<strong><code>CNN</code></strong>应用到海量数据后，就会发现其<strong>对数据的适配能力不够强</strong>；而<code>Transformer</code>对大数据的适配能力很强，可以明显看到随着数据的增加，表现/性能<code>performance</code>不断增加。</li><li><code>Transformer</code>的<strong>每个参数是动态变化的</strong>；而<code>CNN</code>学习参数一旦学习完后就固定了<code>fixed</code>。<code>Transformer</code>对每张图的参数都是不一样的、随时变化的，就可以有无限的参数空间来做一件事。像<code>CNN</code>中新引入的<code>Dynamic Network</code>、<code>Dynamic convolution</code>、<code>Dynamic Relu</code>等概念都是为了把这个参数动态化</li></ul><h4 id="6-2-Interest-Point和Attention"><a href="#6-2-Interest-Point和Attention" class="headerlink" title="6.2 Interest Point和Attention"></a>6.2 Interest Point和Attention</h4><p><code>Attention</code>关注<code>feature</code>彼此之间的<strong>相互关系</strong>，<strong><code>Interest Point</code></strong>(如sift/hog中等)则是<strong>可以学到哪些点更突出、更有表达力</strong>。</p><blockquote><p>思考：这Attention和Interest Point是否可以在学习过程中彼此互补？</p></blockquote><h4 id="6-3-总结"><a href="#6-3-总结" class="headerlink" title="6.3 总结"></a>6.3 总结</h4><ul><li><code>dynamic</code>比<code>static</code>好</li><li><code>close loop</code>比<code>open loop</code>好</li></ul><hr><h3 id="7-Bert"><a href="#7-Bert" class="headerlink" title="7. Bert"></a>7. Bert</h3><blockquote><p>机器是如何理解语言的？</p></blockquote><p>就像图像由像素组成，而像素是由<code>RGB</code>数值表示，我们没有办法让机器直接理解语言，需要将它们转换成机器能明白的东西，比如数字组成的向量。</p><blockquote><p>为什么使用向量表示语言？</p></blockquote><p>词语的意义之间是有关联的，<strong>距离</strong>可以表示词与词直接的<strong>关系</strong>。</p><p><img src="/posts/articletemplate21.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\transformer.assets\image-20220318101308833.png" alt="image-20220318101308833" style="zoom:33%;"></p><blockquote><p>如何得到向量？</p></blockquote><p>机器学习的出现让我们不必为一个单词设计向量，而是将收集好的句子、文章等数据交给模型，由它为单词们找到最合适的位置。</p><h4 id="7-1-Transformer-gt-Bert"><a href="#7-1-Transformer-gt-Bert" class="headerlink" title="7.1 Transformer-&gt;Bert"></a>7.1 Transformer-&gt;Bert</h4><p><img src="/posts/articletemplate21.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\transformer.assets\image-20220318102104340.png" alt="image-20220318102104340" style="zoom:50%;"></p><p><code>Bert</code>就是帮助我们找到词之间位置关系的模型之一，<code>Bert</code>源于<code>Transformer</code>。</p><p>既然<code>Encoder</code>可以将语义很好地抽离出来，直接将这部分独立(<code>Bert</code>)，也能很好地对语言作出表示。</p><h4 id="7-2-Bert的训练"><a href="#7-2-Bert的训练" class="headerlink" title="7.2 Bert的训练"></a>7.2 Bert的训练</h4><p>除了结构，人们还为<code>Bert</code>设计了独特的训练方式：</p><ul><li>遮挡<code>masked</code>训练，对语料<strong>随机覆盖</strong><code>15%</code>的词汇，让<code>Bert</code>去猜这些词汇是什么（<strong>完形填空</strong>）</li><li>利用Next Sentence Prediction任务学习句子级别的信息(或者可以理解为上下文是否匹配)<img src="/posts/articletemplate21.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\Transformer相关及改进模型.assets\image-20220318222210585.png" alt="image-20220318222210585" style="zoom:50%;"></li></ul><p><strong>总结</strong>：前者可以让<code>Bert</code>更好地依据语境进行预测（完形填空），后者让<code>Bert</code>对上下文关系有更好的理解（阅读理解）</p><h4 id="7-3-Bert的应用"><a href="#7-3-Bert的应用" class="headerlink" title="7.3 Bert的应用"></a>7.3 Bert的应用</h4><p>在完成不同的<code>NLP</code>任务时，需要将已经训练好的<code>Bert</code>依据任务目标，增加不同功能的输出层联合训练。</p><ul><li>文本分类增加了分类器<code>Linear classifier</code>（输入句子，输出类别<code>如欺诈</code>）</li><li>阅读理解增加了一个全连接层<code>softmax</code>（输入问题和文章，输出答案的位置）</li></ul><p>面对不同任务，<code>Bert</code>只需对任务作出微调即可</p><p><img src="/posts/articletemplate21.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\transformer.assets\image-20220318103412291.png" alt="image-20220318103412291" style="zoom:50%;"></p><hr><h3 id="8-GPT-生成式预训练Transformer"><a href="#8-GPT-生成式预训练Transformer" class="headerlink" title="8. GPT(生成式预训练Transformer)"></a>8. GPT(生成式预训练Transformer)</h3><p><img src="/posts/articletemplate21.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\transformer.assets\image-20220318095857731.png" alt="image-20220318095857731" style="zoom:50%;"></p><p><code>Transformer</code>的<code>Encoder</code>变成了<code>Bert</code>，<code>Decoder</code>变成了<code>GPT</code>。</p><p><code>GPT</code>也是一个预训练语言模型，同样需要面对不同任务进行微调<code>fine tuning</code></p><h4 id="8-1-应用-生成"><a href="#8-1-应用-生成" class="headerlink" title="8.1 应用(生成)"></a>8.1 应用(生成)</h4><p><code>next-token prediction</code></p><ul><li>机器翻译、文本摘要等基于原始文本信息的任务</li><li>新闻写作等无中生有的生成类任务</li><li>学习过大量的音乐数据后，给出一个乐句，<code>GPT-2</code>就能生成一段音乐</li><li>使用文本和对应图像的数据进行训练，<code>GPT-3</code>就能依据描述输出图像，哪怕真实世界并不存在的也能生成</li></ul><h4 id="8-2-GPT-3-暂未开源"><a href="#8-2-GPT-3-暂未开源" class="headerlink" title="8.2 GPT-3(暂未开源)"></a>8.2 GPT-3(暂未开源)</h4><p><code>GPT-3</code>是一个拥有<code>1750</code>亿参数的预训练模型，不需要面对不同的任务再训练，也不需要微调就能直接使用，这就是所谓的<strong>零样本学习<code>Zero-Shot Learning</code></strong>。</p><p>与语言模型相比，<code>GPT-3</code>更接近一个<strong>包含知识、语境理解和语言组织能力</strong>的<strong>数据库</strong>。</p><h5 id="8-2-1-强大功能"><a href="#8-2-1-强大功能" class="headerlink" title="8.2.1 强大功能"></a>8.2.1 强大功能</h5><ul><li>结构决定了功能，<code>Decoder</code>的输出都基于上一步输出的内容(也就是<code>Bert</code>输出的语义)，这时的<strong>生成</strong>成了<code>GPT</code>最强大的能力，给它一个开头，它就能依据这些这些文字的风格和内容不断续写，甚至可以创作歌词和小说。</li><li>进一步训练后，<code>GPT</code>还能跨越语言的壁垒，如音乐生成和根据描述生成图像等</li></ul><hr><h3 id="9-ViT-Vision-Transformer"><a href="#9-ViT-Vision-Transformer" class="headerlink" title="9. ViT(Vision Transformer)"></a>9. ViT(Vision Transformer)</h3><p>通过将图片切块划分转化为序列作为输入</p><p>ViT使用的<code>Transformer</code>没有考虑<code>Transformer</code>对于词在不同位置语义的变化（也就是<code>Transformer</code>更关注图像特征之间的），所以只能用来做分类任务，而图像出现在哪个位置、具体哪些像素属于这个类别，也就是目标检测和分隔仍无法实现，而<code>Swin Transformer</code>解决了这一问题。</p><hr><h3 id="10-迁移学习-Transfer-Learning"><a href="#10-迁移学习-Transfer-Learning" class="headerlink" title="10. 迁移学习(Transfer Learning)"></a>10. 迁移学习(Transfer Learning)</h3><ul><li><p>CNN中前几层都是在提取特征，直到全连接层才进行分类。提取特征的过程很相似，我们可以拿出来直接用，再用苹果与梨的数据训练新的全连接层完成分类。</p></li><li><p>像这样利用数据、任务和模型间的<strong>相似性</strong>，将训练好的内容<strong>应用</strong>到新的任务上，被称为迁移学习。</p></li><li><p>由于这一过程发生在两个领域间，已有的知识和数据，也就是被迁移的对象被称为源域；被赋予经验的领域被称为目标域。</p></li><li><p>不是一种模型，更像是一种<strong>解题思路</strong></p></li></ul><h4 id="10-1-优点"><a href="#10-1-优点" class="headerlink" title="10.1 优点"></a>10.1 优点</h4><ul><li>目标领域的数据太少，需要数据更多的源域的帮助</li><li>为了节约训练时间</li><li>实现个性化应用<code>individualized</code></li></ul><h4 id="10-2-应用及与预训练模型关系"><a href="#10-2-应用及与预训练模型关系" class="headerlink" title="10.2 应用及与预训练模型关系"></a>10.2 应用及与预训练模型关系</h4><h5 id="10-2-1-应用"><a href="#10-2-1-应用" class="headerlink" title="10.2.1 应用"></a>10.2.1 应用</h5><ul><li>语料匮乏的小语种之间的翻译</li><li>缺乏标注的医疗影像数据识别</li><li>面向不同领域快速部署对话系统</li></ul><h5 id="10-2-2-预训练模型与迁移学习的关系"><a href="#10-2-2-预训练模型与迁移学习的关系" class="headerlink" title="10.2.2 预训练模型与迁移学习的关系"></a>10.2.2 预训练模型与迁移学习的关系</h5><p>预训练模型是迁移学习的一种，就像预先学习了一个指数/技术，然后再把这个知识/技术代入到具体任务中。</p><p>像<code>Transformer</code>、<code>Bert</code>、<code>GPT</code>这些<strong>预训练语言模型</strong>，微调后可以完成不同的任务。</p><hr><h3 id="11-提升篇"><a href="#11-提升篇" class="headerlink" title="11. 提升篇"></a>11. 提升篇</h3><h4 id="11-1-模型"><a href="#11-1-模型" class="headerlink" title="11.1 模型"></a>11.1 模型</h4><ul><li>SGNS/cBow、FastText、ELMo等（词向量）</li><li>DSSM、DecAtt、ESIM等（问答和匹配）</li><li>HAN、DPCNN等（分类）</li><li>BiDAF、DrQA、QANet等（MRC）</li><li>CoVe、InferSent等（迁移）</li><li>MM、N-shortest等（分词）</li><li>Bi-LSTM-CRF等（NER）</li><li>LDA等主题模型（文本表示）</li></ul><h4 id="11-2-训练"><a href="#11-2-训练" class="headerlink" title="11.2 训练"></a>11.2 训练</h4><ul><li>point-wise、pair-wise和list-wise（匹配、ranking模型）</li><li>负采样：从非当前label中选择几个作为负样本加入，作为出现负样本的概率加入到损失函数中<ul><li>优点<ul><li>提高训练速度</li><li>改进效果(模拟真实环境下噪声情况，让模型鲁棒性更强)</li></ul></li></ul></li><li>NCE(噪声对比估计)：通过学习数据样本分布和噪声样本分布的区别，从而发现样本的特性。<ul><li>作用<ul><li>解决归一化参数密度估计问题</li></ul></li></ul></li><li>层级softmax方法(fastText模型)，哈夫曼树的构建</li><li>不均衡问题的处理</li><li>KL散度与交叉熵loss函数</li></ul><h4 id="11-3-评估指标"><a href="#11-3-评估指标" class="headerlink" title="11.3 评估指标"></a>11.3 评估指标</h4><ul><li>F1-score</li><li>PPL</li><li>MRR</li><li>MAP</li></ul>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于itsNeko开源博客食用方法</title>
      <link href="/posts/itsneko-opensource-blog.html"/>
      <url>/posts/itsneko-opensource-blog.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言：本文是关于itsNeko开源博客食用方法详解，感谢使用本开源博客。时间过的好快，还好我都记录下来了。</div><h3 id="itsNeko开源介绍"><a href="#itsNeko开源介绍" class="headerlink" title="itsNeko开源介绍"></a>itsNeko开源介绍</h3><ul><li>基于<br>itsNeko开源博客是基于Hexo博客Matery主题魔改，感谢各位对本博客的喜爱与支持。</li><li>作者<br>Hello，itsNeko，我是本博客的作者，itsNeko博主博客: <a href="https://nekodeng.gitee.io/" target="_blank">itsNeko博客~</a></li><li>赞赏<br>如果你觉得本开源博客还可以，欢迎大家的赞赏，赞赏二维码见页面：<a href="https://nekodeng.gitee.io/donate/" target="_blank">赞赏itsNeko开源博主~</a></li><li>提示<br>本博客为纯静态，无数据库，文章使用markdown格式，图片存在json里面，整体打包上传至服务器即可。</li><li>建议<br>最好懂一点编程知识，建议使用VsCode，善于使用<code>“ ctrl+F ”</code>快捷键定位然后修改自定义内容。</li></ul><h3 id="源码下载及命令"><a href="#源码下载及命令" class="headerlink" title="源码下载及命令"></a>源码下载及命令</h3><ul><li>本开源博客源码已公开在在博主的GitHub仓库，国内也可使用Gitee，希望得到各位的<code>小星星，Star</code>。</li><li>GitHub仓库：<a href target="_blank">itsNeko开源博客源码GitHub地址</a></li><li><p>Gitee仓库：<a href target="_blank">itsNeko开源博客源码Gitee地址</a></p></li><li><p>首先，新建一个文件夹名为“ <code>nekoblog</code> ”，在该文件夹下打开git bash，执行命令行</p></li></ul><pre><code>git clone 仓库地址</code></pre><ul><li>将下载的整个文件夹在VsCode中打开，在终端中依次执行以下命令行</li></ul><pre><code>npm installhexo inithexo ghexo s</code></pre><ul><li>然后在浏览器中，打开“ <a href="http://localhost:4000/">http://localhost:4000/</a> ”，即可实时预览网站，再依次修改网站内容。</li></ul><p>注意：依次执行完上述4条命令后，若遇到报错极大概率是npm install的问题，也可能是网络问题，建议删除整个“ node_modules ”文件夹，然后重新执行上述命令行。</p><h3 id="各页面配置详解"><a href="#各页面配置详解" class="headerlink" title="各页面配置详解"></a>各页面配置详解</h3><h4 id="首页"><a href="#首页" class="headerlink" title="首页"></a>首页</h4><h5 id="1、网站标题描述等"><a href="#1、网站标题描述等" class="headerlink" title="1、网站标题描述等"></a>1、网站标题描述等</h5><p>在<code>/nekoblog/_config.yml</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ Site ”</code>，然后自行修改内容。</p><h5 id="2、logo图片与logo字体"><a href="#2、logo图片与logo字体" class="headerlink" title="2、logo图片与logo字体"></a>2、logo图片与logo字体</h5><ul><li>logo图片:<br>在<code>/themes/_config.yml</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ 配置网站favicon和网站LOGO ”</code>，然后自行修改图片并注意图片格式（此处建议图片大小为180*116最佳）。</li><li>logo字体:<br>在<code>/themes/layout/_partial/header.ejs</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ itsNeko-openSource-Blog ”</code>，然后自行修改大屏幕和小屏幕下logo字体（此处建议小屏下字体数目不宜过多）。 </li></ul><h5 id="3、banner图上打字效果字体"><a href="#3、banner图上打字效果字体" class="headerlink" title="3、banner图上打字效果字体"></a>3、banner图上打字效果字体</h5><p>在<code>/themes/_config.yml</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ sub1 ”</code>，然后自行修改。</p><h5 id="4、《质数的孤独》内容部分"><a href="#4、《质数的孤独》内容部分" class="headerlink" title="4、《质数的孤独》内容部分"></a>4、《质数的孤独》内容部分</h5><p>在<code>/themes/layout/_widget/dream.ejs</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ 《质数的孤独》 ”</code>，然后自行修改标题和内容。</p><h5 id="5、公告栏作者等内容部分"><a href="#5、公告栏作者等内容部分" class="headerlink" title="5、公告栏作者等内容部分"></a>5、公告栏作者等内容部分</h5><p>在<code>/themes/layout/index.ejs</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ 公告文字栏开始 ”</code>，然后自行修改内容。</p><h5 id="6、footer内容部分"><a href="#6、footer内容部分" class="headerlink" title="6、footer内容部分"></a>6、footer内容部分</h5><p>在<code>/themes/layout/_partial/footer.ejs</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ 版权信息 ”</code>，然后自行修改各类内容；此处还要在<code>/themes/_config.yml</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ socialLink ”</code>，然后自行修改。</p><h4 id="关于"><a href="#关于" class="headerlink" title="关于"></a>关于</h4><h5 id="1、配置个人信息"><a href="#1、配置个人信息" class="headerlink" title="1、配置个人信息"></a>1、配置个人信息</h5><p>在<code>/themes/_config.yml</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ profile ”</code>，然后自行修改头像、职业和个人介绍。</p><h5 id="2、个人介绍内容"><a href="#2、个人介绍内容" class="headerlink" title="2、个人介绍内容"></a>2、个人介绍内容</h5><p>在<code>/themes/layout/about.ejs</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ 关于我哦 ”</code>，然后自行个人介绍介绍。</p><h4 id="相册"><a href="#相册" class="headerlink" title="相册"></a>相册</h4><h5 id="1、页面介绍"><a href="#1、页面介绍" class="headerlink" title="1、页面介绍"></a>1、页面介绍</h5><p>在<code>/themes/layout/galley.ejs</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ itsNeko开源博客相册 ”</code>，然后自行修改此内容。</p><h5 id="2、相册数据"><a href="#2、相册数据" class="headerlink" title="2、相册数据"></a>2、相册数据</h5><p>相册是存在json里面的，在<code>/nekoblog/source/_data/galley.json</code>文件中。一个相册就是在一个{}对象内的json数据。</p><h5 id="3、新建一个相册流程"><a href="#3、新建一个相册流程" class="headerlink" title="3、新建一个相册流程"></a>3、新建一个相册流程</h5><ul><li>首先，找到<code>/nekoblog/source/galley</code>文件夹。</li><li>复制已存在的“ itsNeko博主的绘画作品 ”文件夹，并取名。</li><li>打开新取名文件夹下的index.md文件，然后修改title字段必须与新取名文件夹名称相同，否则运行报错。</li><li>然后，在<code>/nekoblog/source/_data/galley.json</code>文件中，复制已存在的{}对象内的json数据，与第一段并以逗号区隔，然后修改新相册的各类内容。并注意：<code>name，url_name，album内的title这三个字段内容必须与新取名文件夹名称相同，否则运行报错。</code></li></ul><h4 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h4><h5 id="增加友情链接"><a href="#增加友情链接" class="headerlink" title="增加友情链接"></a>增加友情链接</h5><ul><li>友链信息是存储在<code>/nekoblog/source/_data/friends.json</code>文件中，一个友链就是在一个{}对象内的json数据，自行增添。</li></ul><h5 id="增加网址收藏"><a href="#增加网址收藏" class="headerlink" title="增加网址收藏"></a>增加网址收藏</h5><ul><li>网址收藏信息是存储在<code>/nekoblog/source/_data/collection.json</code>文件中，一个网址信息就是在一个{}对象内的json数据，自行增添。</li></ul><h4 id="其余"><a href="#其余" class="headerlink" title="其余"></a>其余</h4><p>其余页面自定义修改的内容不多，或者没必要修改，这里给出其余页面的对应文件，可自行根据需要定位到对应页面文件中进行修改。</p><ul><li><code>书单 -&gt; /themes/layout/books.ejs</code></li><li><code>留言板 -&gt; /themes/layout/contact.ejs</code></li><li><code>实战项目 -&gt; /themes/layout/project.ejs</code></li><li><code>博客打赏记录 -&gt; /themes/layout/donate.ejs</code></li><li><code>ticktack -&gt; /themes/layout/ticktack.ejs</code></li><li><code>实战项目 -&gt; /themes/layout/project.ejs</code></li><li><code>vlog -&gt; /themes/layout/videos.ejs</code><br>使用“ ctrl+F ”快捷键定位到<code>“ src=”//player.bilibili ”</code>，然后自行修改两个视频的src，建议使用B站的外链。</li><li><code>music -&gt; /themes/layout/musics.ejs</code><br>使用“ ctrl+F ”快捷键定位到<code>“ id=” ”</code>，然后自行修改两个歌单的id，建议使用网易云和QQ音乐的外链。</li><li><code>urls.txt -&gt; /nekoblog/urls.txt</code>将内容替换成自己网站的url地址即可。</li><li><code>网站2个_config.yml文件 -&gt; 主要是网站的总体配置，自行打开两个文件然后只修改里面的包含个人信息的部分，其余不动。</code></li></ul><h3 id="撰写第一篇文章"><a href="#撰写第一篇文章" class="headerlink" title="撰写第一篇文章"></a>撰写第一篇文章</h3><ul><li>修改完个人信息后，你便可以撰写属于你的第一篇文章啦。</li><li>文章采用markdown语法，所有文章存储在<code>/nekoblog/source/_posts</code>文件夹中。</li><li>这里我已给出常用的两类文章模板，可直接复制粘贴然后（修改文章名，链接后缀建议用英文，标签建议只用一个，分类，时间，以及文章banner图片）形成新的文章。</li></ul><h3 id="怎样开通评论"><a href="#怎样开通评论" class="headerlink" title="怎样开通评论"></a>怎样开通评论</h3><ul><li>本开源博客使用valine评论，环境以及搭好，只需填写valine配置数据即可。</li><li>自行根据网上教程得到配置数据，关键词 “ Hexo 博客添加 Valine 评论系统 ”，教程之一：<a href="https://www.zhyong.cn/posts/95cb/" target="_blank">Hexo 博客添加 Valine 评论系统</a></li><li>注意：本开源博客中已经搭好了环境，<code>比如valine.min.js，valine.ejs等文件已经存在，不用再根据教程添加这些文件</code>，只需根据教程步骤配置其余，然后获取到 APP ID 和 APP KEY。</li><li>然后，将获取到的 APP ID 和 APP KEY，在<code>/themes/_config.yml</code>文件中，使用“ ctrl+F ”快捷键定位到<code>“ valine ”</code>，然后自行修改appId，appKey值即可。</li><li>多说一句，若最后评论功能总是报错，不知道怎么弄，那么建议一切根据外链教程（这个不行，换个完整版教程）操作，可替换本博客已经存在的环境文件，<code>学会自己独立使用浏览器解决问题</code>。</li></ul><h3 id="网站怎样上线"><a href="#网站怎样上线" class="headerlink" title="网站怎样上线"></a>网站怎样上线</h3><ul><li><p><code>免费版 使用gitee免费托管</code><br>关键词，“ Hexo部署到Gitee ”，随机教程：<a href="https://blog.csdn.net/qq_38157825/article/details/112783631" target="_blank">Hexo 部署到 Gitee</a></p></li><li><p><code>收费版 自行购买域名服务器</code><br>关键词，“ Hexo部署到自己服务器 ”</p></li></ul><h3 id="七零八碎补充"><a href="#七零八碎补充" class="headerlink" title="七零八碎补充"></a>七零八碎补充</h3><p>可自行根据需要定位到对应文件中进行修改。</p><ul><li><p><code>赞赏二维码图片 -&gt; /themes/source/medias/reward</code></p></li><li><p><code>鼠标左键点击文字 -&gt; /themes/source/js/click_show_text.js</code></p></li><li><p><code>网站标题栏和footer栏背景颜色 -&gt; /themes/source/css/matery.css</code><br>在该文件中使用“ ctrl+F ”快捷键定位到<code>“ 网站标题栏和footer栏背景颜色 ”</code>，然后自行修改颜色。</p></li><li><p><code>各大页面banner图 -&gt; 若你不想使用默认图片，将以下代码复制到对应页面ejs文件中并修改图片链接即可。</code></p></li></ul><pre><code>&lt;style&gt;  /* banner背景图 */  .bg-cover &#123;      background-image: url(&quot;图片链接地址&quot;)!important;  &#125;&lt;/style&gt;</code></pre><h3 id="怎样联系itsNeko"><a href="#怎样联系itsNeko" class="headerlink" title="怎样联系itsNeko"></a>怎样联系itsNeko</h3><ul><li>留言：itsNeko博客地址:<a href="https://nekodeng.gitee.io/" target="_blank">itsNeko博客~</a></li><li>邮箱：nekodeng@qq.com</li><li>QQ: 2018854221</li></ul>]]></content>
      
      
      <categories>
          
          <category> itsNeko博客食用方法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 关于itsNeko开源博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(writing...)从MLE到EM再到GMM、SMO</title>
      <link href="/posts/articletemplate45.html"/>
      <url>/posts/articletemplate45.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">(writing...)从MLE到EM再到GMM、SMO</div><h4 id="极大似然"><a href="#极大似然" class="headerlink" title=".极大似然"></a>.极大似然</h4><p>已知数据及分布，求模型和参数</p><h5 id="1-1-例子"><a href="#1-1-例子" class="headerlink" title="1.1 例子"></a>1.1 例子</h5><blockquote><p>在伯克利分布(二项分布中)</p></blockquote><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\EM期望最大和GMM高斯混合模型.assets\image-20220308182040443.png" alt="image-20220308182040443" style="zoom:80%;"></p><p>令$\part=0$，得到$\theta$的值</p><p>求极大似然估计值的步骤</p><ol><li>写出似然函数</li><li>对数似然</li><li>求导，并令导数为0</li><li>得到参数</li></ol><blockquote><p>在均匀分布中</p></blockquote><p>X~U(0, a)，$L(a)=\frac{1}{a^n}$，此时如果用上述方法来求a是行不通的，-nln(a)导数为$\frac{a}$，令其=0无法得到结果</p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\EM期望最大和GMM高斯混合模型.assets\image-20220308193432622.png" alt="image-20220308193432622" style="zoom:80%;"></p><p>假设我们有样本{$x_1,x_2,…,x_n$}，要使得$L(a)$最小，可以缩小a的范围，但是最小只能到已有样本的位置</p><p>正态分布中得到的极大似然函数(即代价函数)<img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\EM和GMM高斯混合模型.assets\image-20220309101309140.png" alt="image-20220309101309140" style="zoom:80%;"></p><h4 id="2-EM"><a href="#2-EM" class="headerlink" title="2.EM"></a>2.EM</h4><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309215525608.png" alt="image-20220309215525608" style="zoom:80%;"></p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309223352130.png" alt="image-20220309223352130" style="zoom:80%;"></p><ol><li>初始化分布模型的参数</li><li>E步：计算在当前模型中，各batch的样本数据进行分类（当前样本判为各隐变量的概率，用概率密度计算）</li><li>M步：根据batch样本计算出新的参数</li><li>循环直至收敛</li></ol><p>EM求解的是概率模型中既含有<strong>观测变量</strong>，又含有<strong>隐变量</strong>或潜在变量的问题。EM算法就是含有隐变量的概率模型参数的极大似然估计法，或极大后验概率估计法。</p><p>EM和极大似然估计的前提一样，都需要<strong>假设数据的分布</strong></p><h5 id="2-1-例子"><a href="#2-1-例子" class="headerlink" title="2.1 例子"></a>2.1 例子</h5><blockquote><p>一个观测变量，一个隐变量的情况</p></blockquote><p>假设我们想知道人群中吸大麻者的比例，敏感问题很难得到真实答案，当我们在其中增加一个观测变量（即已知概率分布，如手机尾号是否为偶数），将两者答案混在一起，则在样本足够多的情况下，我们可以根据答案的概率推算出吸大麻者的概率。</p><blockquote><p>两个隐变量的情况</p></blockquote><p>当我们对两个变量都不知道概率分布时（如人群中吸烟者和吸大麻者的比例），可以通过将两个问题随机发放给n个用户，只记录答案，不记录问题，这样就得到了很多不知归属的成组答案。</p><p>然后为吸烟者比例和吸大麻者比例<strong>随机初始化</strong></p><p>用估计的概率去推测这些答案属于两个问题的可能性，这是在估算未知变量的期望，因此被称为E-step</p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309171314038.png" alt="image-20220309171314038" style="zoom:80%;"> </p><p>然后用这个期望去估计吸烟者和吸大麻者的概率，由于这个概率的可能性是最大的，所以被称为M-step</p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309171419318.png" alt="image-20220309171419318" style="zoom:80%;"></p><p>得到新的概率后重新代入估计两个问题的可能性，循环往复得到收敛结果</p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309114857591.png" alt="image-20220309114857591" style="zoom:80%;"></p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309114835333.png" alt="image-20220309114835333" style="zoom:80%;"></p><blockquote><p>三硬币模型</p></blockquote><p>假设有3枚硬币A,B,C，硬币出现正面概率分别为$\pi,p和q$。进行如下抛硬币试验：先掷硬币A，根据结果再选择掷硬币B或硬币C（出现正面掷B，出现反面掷C）。（记正面为1，反面为0）</p><p>该模型的概率分布为：<img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309121449469.png" alt="image-20220309121449469" style="zoom:80%;"></p><p>则对数似然为：<img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309141621145.png" alt="image-20220309141621145"></p><p>由于隐状态概率未知，所以该问题无法直接求解，只能通过迭代的方法求近似解。</p><p>EM先初始化三个参数概率，记作$\theta_0=(\pi^{(0)},p^{(0)},q^{(0)})$，然后通过下面步骤迭代计算参数的估计值，直至收敛。</p><p>记第$i$次迭代的参数估计值为$\theta_i=(\pi^{(i)},p^{(i)},q^{(i)})$</p><p>第$i$次迭代如下：</p><p>Expectation-step(期望)：计算在参数估计值$\pi^{(i)},p^{(i)},q^{(i)}$下观测数据$y_j$来自掷硬币B的概率（在该模型中掷C的概率就是$1-P(B)$）</p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309142629342.png" alt="image-20220309142629342">，即$\mu_j^{(i+1)}=\frac{P^{(i)}_j(B)}{P^{(i)}_j(A)+P^{(i)}_j(B)}$</p><p>Maximization-step(极大似然估计)：计算模型参数的新估计值</p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309143441135.png" alt="image-20220309143441135">，<img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309143638455.png" alt="image-20220309143638455">，<img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309143704604.png" alt="image-20220309143704604"></p><blockquote><p>两硬币模型</p></blockquote><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309173610880.png" alt="image-20220309173610880" style="zoom:80%;"></p><p>假设有两枚硬币(A, B)，随机选择一枚掷n次（H表示正面，T表示反面）</p><ol><li><p>已知选择的情况，估计两个硬币出现正面的概率？（极大似然法）<img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309174129095.png" alt="image-20220309174129095" style="zoom:80%;"></p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309173315754.png" alt="image-20220309173315754" style="zoom:80%;"></p></li><li><p>假设不知道选择情况，估计两枚硬币出现正面的概率？（EM算法）</p><p>E步：$A的概率=\frac{硬币A和样本y_1的联合概率}{各隐参数和样本y_1的联合概率之和}$</p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309174249568.png" alt="image-20220309174249568" style="zoom:80%;"></p><p>如：<img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309175542507.png" alt="image-20220309175542507"></p><p>计算得到A和B的出现概率后，该硬币的<strong>正反概率=次数 * 概率</strong>（如发生5正5反，硬币A出现正面的概率为0.45*5=2.25）</p><p>M步：由上述求得的某个硬币对应的正反概率求对应的和<img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309201705124.png" alt="image-20220309201705124" style="zoom:80%;"></p><p>重新估计参数$\hat\theta$，$\hat\theta=\frac{P(H|A)}{P(A)}$</p></li></ol><blockquote><p>假设已知有200人的身高，但不知道男女比例，求男女比例</p></blockquote><p>学生性别属于隐参数，身高分布属于模型参数</p><ol><li>假设男生的身高分布为N($\mu=172,\sigma^2=5^2$)，女生的身高分布为N($\mu=162,\sigma^2=5^2$)</li><li>根据样本数据中的身高进行期望概率估计（如身高180，则极大可能为男生）&lt;—E步</li><li>按E步得到的概率将样本数据分为男生和女生，然后根据数据求解参数的极大似然值&lt;—M步</li><li>将M步得到的参数代入E步迭代，循环往复，直到参数收敛（实现效果有点类似于梯度下降）</li></ol><h5 id="2-2-总结"><a href="#2-2-总结" class="headerlink" title="2.2 总结"></a>2.2 总结</h5><p>最直观了解EM算法的思路是K-Means算法。在K-Means聚类时，每个聚类簇的质心是隐含参数，假设初始化质心为K个，即E步；然后计算得到每个样本最近的质心，并把样本聚类到最近的这个质心，即M步，重复EM，直到质心不再变化，这就完成了K-Means聚类。</p><h4 id="3-EM算法推导"><a href="#3-EM算法推导" class="headerlink" title="3.EM算法推导"></a>3.EM算法推导</h4><h5 id="3-1-凸函数-convex"><a href="#3-1-凸函数-convex" class="headerlink" title="3.1 凸函数(convex)"></a>3.1 凸函数(convex)</h5><ul><li>在实数域上的函数，如果对于任意实数都有$f’’≥0$则该函数为凸函数</li><li>如果不是单个实数，而是实数组成的向量，如果函数的Hesse矩阵是半定的，即$H’’≥0$，则该函数为凸函数</li></ul><p>特别的，如果函数$f’’&gt;0$或者$H’’&gt;0$则该函数被称为严格凸函数</p><h5 id="3-2-Jensen不等式"><a href="#3-2-Jensen不等式" class="headerlink" title="3.2 Jensen不等式"></a>3.2 Jensen不等式</h5><p>如果函数$f$是凸函数，X服从二项分布，x的期望就是a和b的中值，则有$E[f(x)]≥f(E(x))$，其中$E[f(x)]=0.5f(a)+0.5f(b)$，$f(E(x))=f(0.5a+0.5b)$</p><p>图像为：<img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309165129970.png" alt="image-20220309165129970" style="zoom:80%;"></p><p>得到<code>Jensen</code>不等式：</p><ul><li><p>对于凸函数，有：</p><p>$\phi(E[f(x)]) \le E[\phi(f(x))]$，即$f(\displaystyle\sum<em>{i=1}^M\lambda_ix_i) \le \displaystyle\sum</em>{i=1}^M\lambda_if(x_i)$</p></li><li><p>对于凹函数(concave)，有相反的结论：</p><p>$\phi(E[f(x)]) \ge E[\phi(f(x))]$</p></li></ul><p>可以由<code>Jensen</code>不等式的结论推出：信息论中的KL散度恒大于0的吉布斯不等式</p><p>即：<img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309164419295.png" alt="image-20220309164419295"></p><p>当代码是基于真实概率p分布时，平均消息长度被最小化</p><h5 id="3-3-期望"><a href="#3-3-期望" class="headerlink" title="3.3 期望"></a>3.3 期望</h5><p>对于离散型样本X的概率分布$p_i=p{X=x_i}$，数学期望$E(X)$为$E(X)=\displaystyle\sum_ix_ip_i$</p><p>对于连续型样本X的概率密度为$f(x)$，则期望为：$E(X)=\int xf(x)dx$</p><h5 id="3-4-EM推导"><a href="#3-4-EM推导" class="headerlink" title="3.4 EM推导"></a>3.4 EM推导</h5><p><a href="https://zhuanlan.zhihu.com/p/36331115">EM算法-知乎</a></p><h5 id="3-5-坐标下降法到EM算法和SMO算法"><a href="#3-5-坐标下降法到EM算法和SMO算法" class="headerlink" title="3.5 坐标下降法到EM算法和SMO算法"></a>3.5 坐标下降法到EM算法和SMO算法</h5><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309213256015.png" alt="image-20220309213256015" style="zoom:67%;"></p><ul><li>坐标下降法(上升法)：每次只选择一个维度，将原始问题变成一元函数，然后对这个一元函数求极值，如此反复<strong>轮换不同维度进行迭代</strong>。主要应用包括<strong>EM算法</strong>和<strong>SMO算法</strong></li><li>EM：是两个参数迭代的坐标上升问题，特点是两个维度轮换迭代（上图为EM的梯度图）</li><li>SMO：是<strong>m-1维</strong>(m表示样本量)的<strong>坐标下降问题</strong>，难点在于<strong>选择哪个坐标作为下一个循环的迭代</strong>，同时还需要<strong>对求导得到的变量值进行约束条件限制</strong></li></ul><blockquote><p>EM为什么能近似实现对观测数据的极大似然估计呢？</p></blockquote><p>EM算法是通过不断求解<strong>下界的极大化</strong>逼近求解<strong>对数似然函数极大化</strong>的算法</p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309203813238.png" alt="image-20220309203813238" style="zoom:80%;">（9.14）</p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309203742620.png" alt="image-20220309203742620" style="zoom:80%;"></p><h5 id="3-6-EM的收敛性"><a href="#3-6-EM的收敛性" class="headerlink" title="3.6 EM的收敛性"></a>3.6 EM的收敛性</h5><p>概率分布为：</p><script type="math/tex; mode=display">P(X|\theta)=\frac{P(X,Z|\theta)}{P(Z|X,\theta)}</script><p>其中Z为隐变量，取对数后有：</p><script type="math/tex; mode=display">logP(X|\theta)=logP(X,Z|\theta)-logP(Z|X,\theta)</script><p>令<img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从MLE到EM再到GMM、SMO.assets\image-20220315215838600.png" alt="image-20220315215838600" style="zoom:80%;"><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从MLE到EM再到GMM、SMO.assets\image-20220315215908514.png" alt="image-20220315215908514" style="zoom:80%;"></p><p>即：<img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从MLE到EM再到GMM、SMO.assets\image-20220315220050102.png" alt="image-20220315220050102" style="zoom:80%;"></p><p>证明EM的收敛性，即证明$\theta^{j+1}-\theta^j \ge 0$，即证明：$P(X|\theta^{j+1})-P(X|\theta^j)=L(\theta^{j+1},\theta^{j})-L(\theta^{j},\theta^{j})+H(\theta^{j+1},\theta^{j})-H(\theta^{j},\theta^{j})$</p><p>由于极大似然，$\theta^{j+1}$使$L(\theta,\theta^{j})$达到极大，所以$L(\theta^{j+1},\theta^{j})-L(\theta^{j},\theta^{j})\ge0$</p><p>所以只需分析熵函数H在$\theta^{j+1}$和$\theta^j$的情况即可：</p><p>根据<code>jensen</code>不等式<img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从MLE到EM再到GMM、SMO.assets\image-20220316122443640.png" alt="image-20220316122443640" style="zoom:80%;"></p><p>所以<img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从MLE到EM再到GMM、SMO.assets\image-20220316122524455.png" alt="image-20220316122524455" style="zoom:80%;">是可以收敛的，即EM算法可以收敛，但对于<strong>非凸函数，只能收敛到局部最优</strong>。</p><h4 id="4-EM和SVM中的SMO-序列最小优化"><a href="#4-EM和SVM中的SMO-序列最小优化" class="headerlink" title="4.EM和SVM中的SMO(序列最小优化)"></a>4.EM和SVM中的SMO(序列最小优化)</h4><p>更新中</p><h4 id="5-EM和GMM"><a href="#5-EM和GMM" class="headerlink" title="5.EM和GMM"></a>5.EM和GMM</h4><p>GMM的初始化需要设置均值和标准差，最朴素的方法是把它们设置为数据集本身的平均值，聚类结果是圆形；更优的方式是使用由<strong>k-means生成的聚类</strong>来初始化高斯分布，聚类结果是椭圆形且可旋转；最好不要使用随机选择均值和方差</p><p>计算GMM参数的方式和EM的基本一样</p><ol><li>初始化$\mu$和$\sigma$</li><li>E步：计算聚簇分类概率</li><li>M步：根据样本和分类概率计算新的参数</li><li>评估对数似然<img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从MLE到EM再到GMM、SMO.assets\image-20220310095208856.png" alt="image-20220310095208856" style="zoom: 67%;"></li></ol><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309215525608.png" alt="image-20220309215525608" style="zoom:80%;"></p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从极大似然到EM以及GMM高斯混合模型.assets\image-20220309223412714.png" alt="image-20220309223412714" style="zoom:80%;"></p><pre class=" language-lang-python"><code class="language-lang-python">form sklearn import datasets, mixtureX = datasets.load_iris().data[:10]gmm = mixture.GaussianMixture(n_components=3)gmm.fit(X)clustering = gmm.predict(X)</code></pre><h5 id="5-1-GMM的优缺点"><a href="#5-1-GMM的优缺点" class="headerlink" title="5.1 GMM的优缺点"></a>5.1 GMM的优缺点</h5><h6 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h6><ul><li>软聚类算法，多个聚类隶属度</li><li>聚类形状灵活</li></ul><h6 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h6><ul><li>对初始化值敏感</li><li>可能收敛到局部最优</li><li>收敛速度慢</li></ul><h5 id="5-2-GMM的应用"><a href="#5-2-GMM的应用" class="headerlink" title="5.2 GMM的应用"></a>5.2 GMM的应用</h5><ul><li>分离前后背景（人像去除，背景去除）</li><li>分离混合的数据（星座、成绩）</li><li>分析卫星图得到出行高峰和交通方式（将已有数据用PCA等算法选取有用特征，然后通过聚类转化为知识和信息）<img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从MLE到EM再到GMM、SMO.assets\image-20220310103735445.png" alt="image-20220310103735445" style="zoom:67%;"></li><li>识别指纹和其他类别的生物识别</li></ul><h5 id="5-3-聚类评价指数-categories-of-cluster-validation-indices"><a href="#5-3-聚类评价指数-categories-of-cluster-validation-indices" class="headerlink" title="5.3 聚类评价指数(categories of cluster validation indices)"></a>5.3 聚类评价指数(categories of cluster validation indices)</h5><ul><li><p>外部(external)指标：处理有标签数据时使用的评分指标</p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从MLE到EM再到GMM、SMO.assets\image-20220310105814990.png" alt="image-20220310105814990" style="zoom:80%;"></p></li><li><p>内部(internal)指标：无监督学习时，用数据衡量数据和结构之间的吻合度</p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从MLE到EM再到GMM、SMO.assets\image-20220310112410908.png" alt="image-20220310112410908" style="zoom:80%;"></p></li><li><p>相对(relative)指标：两个聚类结构哪一个在某种意义上更好（所有的外部指标都可以作为相对指标）</p></li></ul><p>大多数评价指标是通过紧凑性(compactness)和可分性(separability)来定义的，紧凑性用来衡量一个聚类中元素彼此之间的距离，可分性表示不同距离之间的距离</p><h5 id="5-4-外部指标-兰德系数-Adjusted-Rand-Score"><a href="#5-4-外部指标-兰德系数-Adjusted-Rand-Score" class="headerlink" title="5.4 外部指标-兰德系数(Adjusted Rand Score)"></a>5.4 外部指标-兰德系数(Adjusted Rand Score)</h5><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从MLE到EM再到GMM、SMO.assets\image-20220310110455030.png" alt="image-20220310110455030" style="zoom:80%;"></p><p>n表示样本数，a表示正确聚类的簇数(k中两簇都是正确聚类)，b表示在正确聚类的样本点到其他簇中的样本点的对数(k中有四个点是在正确聚类中且聚类正确的，两粉两青)</p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从MLE到EM再到GMM、SMO.assets\image-20220310112259951.png" alt="image-20220310112259951" style="zoom:80%;"></p><h5 id="5-5-内部指标-轮廓系数-Silhouette-index"><a href="#5-5-内部指标-轮廓系数-Silhouette-index" class="headerlink" title="5.5 内部指标-轮廓系数(Silhouette index)"></a>5.5 内部指标-轮廓系数(Silhouette index)<img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从MLE到EM再到GMM、SMO.assets\image-20220310113109972.png" alt="image-20220310113109972" style="zoom:80%;"></h5><p>a是<strong>簇内不相似度</strong>，也就是同一个聚类中样本i到其他样本的平均距离(蓝色实线)，簇内所有样本的$a_i$均值称为簇的簇内不相似度，样本i的到簇内样本的均值$a_i$称为样本i的簇内不相似度；b是<strong>簇间不相似度</strong>，也就是样本i到不同聚类到样本的最近平均距离(即黄色实线)，$b_i$越大说明样本i越不属于其他簇。</p><p>在不同聚类方法中的表现</p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从MLE到EM再到GMM、SMO.assets\image-20220310114255667.png" alt="image-20220310114255667" style="zoom:80%;"></p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从MLE到EM再到GMM、SMO.assets\image-20220310120705336.png" alt="image-20220310120705336" style="zoom:80%;"></p><p><img src="/posts/articletemplate45.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\从MLE到EM再到GMM、SMO.assets\image-20220310121245467.png" alt="image-20220310121245467" style="zoom:80%;"></p><p>对于<strong>簇结构为凸</strong>的数据轮廓系数值高，而对于<strong>簇结构非凸</strong>需要使用DBSCAN进行聚类的数据，轮廓系数值低，因此，<strong>轮廓系数不应该用来评估不同聚类算法之间的优劣</strong>，比如K-means聚类结果与DBSCAN聚类结果之间的比较，通常情况下，我们<strong>不会使用轮廓系数来评估DBSCAN</strong>，据说DBCV用来评估DBSCAN效果很好。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(writing...)概率图模型之HMM、MRF和CRF</title>
      <link href="/posts/articletemplate66.html"/>
      <url>/posts/articletemplate66.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">(writing...)概率图模型之HMM、MRF和CRF</div><h3 id="0-预备知识"><a href="#0-预备知识" class="headerlink" title="0.预备知识"></a>0.预备知识</h3><p>先验就是根据<strong>经验</strong>或历史数据<strong>统计</strong>分析得到或者<strong>理论值</strong>（即在采样前就可以得到的数据）又叫边缘概率P(A)/P(B)&lt;–频率学派</p><p>后验就是根据实际<strong>样本</strong>分析概率分布，即条件概率P(B|A)（A发生的条件下B发生的概率，即AB的联合概率分布）</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220317154553723.png" alt="image-20220317154553723" style="zoom:80%;"></p><h3 id="1-马尔可夫模型"><a href="#1-马尔可夫模型" class="headerlink" title="1.马尔可夫模型"></a>1.马尔可夫模型</h3><blockquote><p>随机过程特征</p></blockquote><ol><li>事物的状态都是随机的</li><li>事物的状态与前面所有的状态都有或多或少的关系(越近权重越大)&lt;—条件概率</li></ol><p><strong>马尔可夫模型将条件概率简化为当前节点只依赖前置节点</strong>这种对问题的近似解（尽管马尔可夫链是无记忆性的，但是异常发生是小概率事件，可以忽略不计），相对考虑前置所有节点，马尔可夫模型简化了计算，使得很多问题得以解决。</p><h4 id="1-1-马尔可夫假设"><a href="#1-1-马尔可夫假设" class="headerlink" title="1.1 马尔可夫假设"></a>1.1 马尔可夫假设</h4><p>符合上述假设(当前节点只与前置节点有关)的假设被称为马尔可夫假设</p><h4 id="1-2-马尔可夫链-马尔可夫过程"><a href="#1-2-马尔可夫链-马尔可夫过程" class="headerlink" title="1.2 马尔可夫链(马尔可夫过程)"></a>1.2 马尔可夫链(马尔可夫过程)</h4><p>符合马尔可夫假设的随机过程被称为马尔可夫过程，也被称为马尔可夫链</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\马尔可夫链和隐马尔可夫模型.assets\image-20220303103246049.png" alt="image-20220303103246049"></p><p>在这个离散马尔可夫链中，四个圈表示四个状态，每条边表示一个可能的状态转换，边上的权值是转移概率。例如，状态$m_3$到$m_4$有两种可能，$m_3$自旋的概率为0.7，转移到$m_4$的概率为0.3，总概率为1。</p><p>可以把马尔可夫链想象成一台机器，它可以随机选择一个状态作为初始状态，随后按照上述规则随机选择后续状态</p><h3 id="2-隐马尔可夫模型"><a href="#2-隐马尔可夫模型" class="headerlink" title="2.隐马尔可夫模型"></a>2.隐马尔可夫模型</h3><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\马尔可夫链和隐马尔可夫模型.assets\image-20220303121801217.png" alt="image-20220303121801217" style="zoom: 80%;"></p><p><strong>隐马尔科夫模型=马尔可夫链+一般随机过程</strong></p><h4 id="2-1-HMM实例"><a href="#2-1-HMM实例" class="headerlink" title="2.1 HMM实例"></a>2.1 HMM实例</h4><blockquote><p>例子1(学习模型)</p></blockquote><p>已知结果(心情)去推导原因(天气)</p><p>由状态序列得到状态转移概率：<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM(训练、解码算法、EM).assets\image-20220311165842458.png" alt="image-20220311165842458" style="zoom:67%;"></p><p>由观测序列得到观测概率：<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM(训练、解码算法、EM).assets\image-20220311170103696.png" alt="image-20220311170103696" style="zoom: 67%;"></p><p>当我们知道观测序列去推导状态转移概率和观测概率，这就是模型学习的过程，最终得到的HMM模型为：<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM(训练、解码算法、EM).assets\image-20220311171822305.png" alt="image-20220311171822305" style="zoom:50%;"></p><blockquote><p>例子2</p></blockquote><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\马尔可夫模型、HMM(训练、解码算法、EM).assets\image-20220305094220178.png" alt="image-20220305094220178" style="zoom:67%;"></p><p>==隐马尔科夫模型=马尔可夫链+一般随机过程==</p><p><strong>其中马尔可夫链对应隐性状态（上图中的股市状态），隐性状态到观测状态的过程就是一般随机过程</strong></p><p>我们已知股市的状态(牛市、熊市和横盘)，但是牛市不是说一定是涨的，只能说上涨的概率相对不变和下跌更大，我们可以根据观测到的数据表现，从而推断出当前股市状态。并且，股市状态不是一直保持不变的，牛市也有可能因为各种因素而转变成熊市和横盘，这种状态变化都是有概率的。在模型中，我们需要设定一个<strong>初始状态</strong>$\pi$以供模型开始运行。</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\马尔可夫模型、HMM(训练、解码算法、EM).assets\image-20220305101335960.png" alt="image-20220305101335960" style="zoom:67%;"></p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\马尔可夫模型、HMM(训练、解码算法、EM).assets\image-20220305113000007.png" alt="image-20220305113000007" style="zoom:50%;"></p><p>Q：隐状态集合(N个)|V：可观测态集合(M个)|I：状态序列(长度为T)|O：观测序列(一个状态对应一个输出)</p><p>Q, V可以理解为骰子种类和种类对应点数，I指使用的骰子种类序列，O指状态序列对应观测到的骰子点数</p><h4 id="2-2-HMM的三个基本问题"><a href="#2-2-HMM的三个基本问题" class="headerlink" title="2.2 HMM的三个基本问题"></a>2.2 HMM的三个基本问题</h4><ol><li>估计(概率计算)问题：已知模型$\lambda$（状态初值$\pi$、状态转移概率分布$A$和观测状态生成概率$B$），<strong>计算观测序列$O$出现的概率</strong>？==已知模型求某个结果概率==</li><li>学习问题（模型训练）：给定一个假设模型$\lambda$和观测序列$O$，如何<strong>找到最可能产生这个输出的状态序列的参数</strong>？==已知数据和预测模型求参数==（根据数据和预估模型训练最合适的状态转移概率分布和观测概率分布）</li><li>预测问题（解码问题）：给定观测数据和模型，求<strong>最有可能的对应的状态序列</strong>？==已知模型和结果求状态/条件==</li></ol><h4 id="2-3-解决"><a href="#2-3-解决" class="headerlink" title="2.3 解决"></a>2.3 解决</h4><ol><li><p>概率计算问题</p><ul><li><p>直接计算</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\隐马尔可夫模型HMM(训练、解码算法、EM).assets\image-20220305163800850.png" alt="image-20220305163800850" style="zoom: 67%;"></p><p>当前观测序列发生概率=Σ每个观测结果对应的状态转移概率x对应的观测概率，考虑所有可能的状态序列和输出序列导致时间复杂度达到$O(TN^T)$</p></li><li><p>前向后向算法</p><ul><li><strong>前向算法</strong>：从初始节点到终点，并且后续节点在前置节点计算的基础上进行下一次计算，避免了重复的计算，时间复杂度变为$O(TN^2)$，大大降低了时间复杂度。</li></ul></li></ul><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\隐马尔可夫模型HMM(训练、解码算法、EM).assets\image-20220305170916035.png" alt="image-20220305170916035" style="zoom: 80%;"></p><p>​    <strong>实例</strong></p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\马尔可夫模型、HMM(训练、解码算法、EM" alt="image-20220305131750463">.assets\image-20220305131750463.png)</p><p>解释：在前一次观测序列对应概率的基础上，计算从前一状态到指定状态的概率及生成当前观测输出的概率（如在上述例子中，$\alpha<em>{1}(1)$表示第一个观测输出(也就是红球)是从第一个盒子中取出的概率，$\alpha</em>{2}(1)$表示第二个观测输出(此时是白球)是从第一个盒子中取出的概率(前置状态转移到盒子1的概率$a<em>{i1}$和前置状态的观测输出的概率$\alpha_1(i)$)）即$\alpha_t(k)=[\sum</em>{i=1}^N\alpha<em>{t-1}a</em>{ik}]b_k(o_t)$其中t表示递归次数（即观测序列中第t个），k表示隐状态全部转移到第k个状态，$b_k(o_t)$表示第t个观测序列($o_t$)的观测概率。</p><p>将最后一次的结果求和就可以计算得到当前观测序列发生的概率了</p><p><strong>后向传播</strong>：从后往前，设最终的概率为1，观测序列也是从后往前取(逆序)，前置某个节点=（节点状态转移到该状态的概率x当前状态表现对应概率(如盒子1红球概率为0.3)x后置节点的概率）之和</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\隐马尔可夫模型HMM(训练、解码算法、EM" alt="image-20220305204446926">.assets\image-20220305204446926.png)</p><p>最终概率为：</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\隐马尔可夫模型HMM(训练、解码算法、EM).assets\image-20220305202914483.png" alt="image-20220305202914483" style="zoom:80%;"></p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\隐马尔可夫模型HMM(训练、解码算法、EM).assets\image-20220305202414308.png" alt="image-20220305202414308" style="zoom: 80%;"></p><p>前向和后向本质上一样，所以可以统一写成：</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\隐马尔可夫模型HMM(训练、解码算法、EM" alt="image-20220305204542496">.assets\image-20220305204542496.png)</p></li><li><p>学习问题（模型训练）。在利用隐马尔科夫模型解决实际问题中，需要事先知道从前置状态进入当前状态的概率$P(s<em>t|s</em>{t-1})$，也就是<strong>转移概率</strong>，和每个状态产生相应输出$o_t$的概率$P(o_t|s_t)$，也称为<strong>生成概率</strong>(隐性状态的表现概率)。这些概率被称为隐马尔科夫模型的参数，而计算或者估计这些参数的过程称为模型的训练。</p><ul><li>对于<strong>生成概率</strong><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\马尔可夫链和隐马尔可夫模型.assets\image-20220303142518743.png" alt="image-20220303142518743" style="zoom:80%;">，如果有足够多人工标注的数据，通过统计(状态和输出)频率直接算出(估计出)模型的参数。因为数据是人工标注的，所以该方法是<strong>有监督</strong>的训练方法。对于<strong>转移概率</strong><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\马尔可夫链和隐马尔可夫模型.assets\image-20220303142855814.png" alt="image-20220303142855814" style="zoom:80%;">可以参照<strong>统计</strong>语言模型的训练方法直接得到。</li><li>对于声学模型这种无法标注的数据和机器翻译模型这种标注成本很高的数据，训练隐马尔科夫模型更实用的方式是仅仅通过大量观测到的数据$o_t$推算出模型的转移概率和生成概率，这种方法被称为<strong>无监督训练</strong>。其中主要使用的是<strong>鲍姆-韦尔奇算法</strong>。</li></ul></li><li><p>解码问题：可以用<strong>维特比算法</strong>(Viterbi algorithm)解决：即计算当前观测数据对应的状态的最大可能性</p><p>例如，假设有三种骰子(四面骰子D4(1-4)、六面骰子D6(1-6)、八面骰子D8(1-8)。</p><p>假设已知投掷结果为(1,6,3)，根据概率计算D4掷到1的概率最大($\frac{1}{4}$)，而D6和D8的概率为($\frac{1}{6}$、$\frac{1}{8}$)；第二次掷到6，D6的概率最大(D4没有6，D8的概率小于D6的)，以此类推，故最大概率的序列为(D4-&gt;D6-&gt;D4)</p><p><strong>总结</strong>：从头开始计算当前节点最大概率的选择，一步步<strong>递归</strong>向下，找到各节点对应的最优路径（最大概率），直到最后一个节点，再回头比较各最优路径，得到最短路径（即最有可能得到该表现序列的状态序列）。相比暴力地遍历所有节点，viterbi算法到达平行节点只选取各方案的最短路径，大大降低了时间复杂度。</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\马尔可夫模型、HMM(训练、解码算法、EM" alt="image-20220304113601044">.assets\image-20220304113601044.png)</p></li></ol><h3 id="3-HMM小结"><a href="#3-HMM小结" class="headerlink" title="3.HMM小结"></a>3.HMM小结</h3><p>隐马尔可夫模型最初应用于通信领域，继而推广到语音和语言处理中，称为连接自然语言处理和通信的桥梁。和其他机器学习的模型工具一样，需要一个<strong>训练算法（鲍姆-韦尔奇算法）</strong>和使用时的<strong>解码算法（维特比算法）</strong>就可以使用隐马尔科夫模型这个工具了。</p><h3 id="4-概率图"><a href="#4-概率图" class="headerlink" title="4.概率图"></a>4.概率图</h3><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM(训练、解码算法、EM).assets\image-20220310200037260.png" alt="image-20220310200037260" style="zoom:80%;"></p><p>概率图模型中，数据(样本)由公式$G=(V,E)$表示，其中V表示节点（即样本，可以是label标签或者token字符），用Y表示批样本数据$Y=(y_1,…,y_n)$（一条sequence，包含很多token），$P(Y)$表示批样本的分布，E表示边（即概率依赖关系）</p><h4 id="4-1-有向图和无向图"><a href="#4-1-有向图和无向图" class="headerlink" title="4.1 有向图和无向图"></a>4.1 有向图和无向图</h4><p>贝叶斯网络属于概率有向图，马尔科夫网络属于概率无向图。贝叶斯网络<strong>适合为有单向依赖的数据建模</strong>，马尔科夫网络<strong>适合实体之间相互依赖的建模</strong>。他们的差异表现在如何求$P=(Y)$，即如何表示Y这个联合概率</p><h4 id="4-2-有向无环图-贝叶斯网络"><a href="#4-2-有向无环图-贝叶斯网络" class="headerlink" title="4.2 有向无环图(贝叶斯网络)"></a>4.2 有向无环图(贝叶斯网络)</h4><p>又叫<strong>信念(belief)网络</strong></p><p>对于有向图，联合概率为$P(Y)=\displaystyle\prod_{i=0}P(x_i|\pi(x_i))$</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM(训练、解码算法、EM).assets\image-20220310204044444.png" alt="image-20220310204044444" style="zoom:50%;"></p><p>上图表示$x_1$决定$x_2$的概率，$x_2$又决定了$x_3$和$x_4$的概率，$x_3$和$x_4$共同决定了$x_5$的概率，而后续节点都可以说是间接依赖于前序节点（即前序节点或多或少都对后序节点有影响）</p><p>联合概率分布为：$P(x_1,x_2,x_3,x_4,x_5)=P(x_1)P(x_2|x_1)P(x_3|x_2)P(x_4|x_2)P(x_5|x_3,x_4)$，其中无父节点使用先验概率，有父节点使用条件概率</p><p>一维空间中描述了点的状态(时间)，一维<strong>马尔科夫随机过程描述了随机过程中某点的状态只与该点之前的一个点的状态有关</strong>（HMM）</p><h5 id="4-2-1-贝叶斯网络结构"><a href="#4-2-1-贝叶斯网络结构" class="headerlink" title="4.2.1 贝叶斯网络结构"></a>4.2.1 贝叶斯网络结构</h5><ol><li><p><code>head-to-head</code><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220317160312884.png" alt="image-20220317160312884" style="zoom: 50%;"><code>c</code>未知，<code>a,b</code>独立(被阻断<code>blocked</code>)</p></li><li><p><code>tail-to-tail</code><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220317160525292.png" alt="image-20220317160525292" style="zoom: 50%;"></p><ul><li><code>c</code>未知，有：<code>P(a,b,c)=P(c)P(a|c)P(b|c)</code>，此时，没法得出<code>P(a,b) = P(a)P(b)</code>，即<code>a,b</code>不独立</li><li><code>c</code>已知，有：<code>P(a,b|c)=P(a,b,c)/P(c)</code>，然后将<code>P(a,b,c)=P(c)P(a|c)P(b|c)</code>带入式子中，得到：<code>P(a,b|c)=P(a,b,c)/P(c) = P(c)P(a|c)P(b|c) / P(c) = P(a|c)*P(b|c)</code>，即<code>a,b</code>独立</li></ul></li><li><p><code>head-to-tail</code><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220317160842814.png" alt="image-20220317160842814" style="zoom: 50%;"></p><ul><li><code>c</code>未知，有：<code>P(a,b,c)=P(a)*P(c|a)*P(b|c)</code>，但无法推出<code>P(a,b) = P(a)P(b)</code>，即<code>a,b</code>不独立</li><li><code>c</code>已知，有：<code>P(a,b|c)=P(a,b,c)/P(c)</code>，且根据<code>P(a,c) = P(a)P(c|a) = P(c)P(a|c)</code>，得到<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220317161341301.png" alt="image-20220317161341301" style="zoom:67%;">，即<code>a,b</code>独立</li></ul><p>所以，在<code>c</code>给定的条件下，<code>a,b</code>被阻断，是独立的，称之为<code>head-to-tail</code>条件独立</p><p><code>head-to-tail</code>就是链式网络<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220317161523403.png" alt="image-20220317161523403" style="zoom:50%;"></p><p>在中间变量$x<em>i$给定的条件下，$x</em>{i+1}$的分布和$x<em>1,…,x</em>{i-1}$条件独立，也就是当前状态只与上一状态有关，这个过程叫马尔科夫链</p></li></ol><h5 id="4-2-2-因子图"><a href="#4-2-2-因子图" class="headerlink" title="4.2.2 因子图"></a>4.2.2 因子图</h5><p><strong>定义</strong>：一个全局函数通过因式分解能够分解为多个局部函数(势函数)的乘积，分解得到的<strong>双向图</strong>叫做因子图。(由于是双向的，所以<strong>变成了无向图</strong>)，这些<strong>局部函数和对应的变量关系</strong>就体现在因子图上。</p><p>通俗来说，因子团表示的是某个变量的局部关系（<strong>只考虑该变量对应的局部关系</strong>，不考虑与全局其他变量的联系）</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220317165644926.png" alt="image-20220317165644926" style="zoom:50%;"></p><blockquote><p>例子</p></blockquote><p>假设有全局变量<code>g</code>，因式分解方程为：</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220317165440334.png" alt="image-20220317165440334" style="zoom:80%;"></p><p>其中$f$表示变量之间的关系，对应的概率图为：</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220317165644926.png" alt="image-20220317165644926" style="zoom:50%;"></p><p>上述因子图等价于：<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220317192144357.png" alt="image-20220317192144357" style="zoom: 50%;"></p><blockquote><p>求解</p></blockquote><p>在概率图中，<strong>求某个变量的边缘分布</strong>是常见的问题。这个问题有很多求解方法，其中之一就是<strong>把贝叶斯网络或马尔科夫随机场转换成因子图</strong>，<strong>然后用<code>sum-product</code>算法求解</strong>。</p><p>在贝叶斯网络或马尔科夫随机场中：</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220317191412262.png" alt="image-20220317191412262" style="zoom: 67%;"></p><p>对应的变量关系为：<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220317191924254.png" alt="image-20220317191924254" style="zoom:80%;"></p><p>对应的因子图为：<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220317191955425.png" alt="image-20220317191955425" style="zoom:67%;"></p><p>如果因子图是无环的，则可以准确求得任意一个变量的边缘分布，如果是有环的，则无法用<code>sum-product</code>算法准确求得边缘分布。</p><blockquote><p>贝叶斯网络与无向环</p></blockquote><p>若贝叶斯网络中存在环(无向)，则构造的因子图会得到环，而有向图的前置事件概率会对后续事件概率产生影响，环的出现不利于概率计算</p><p>解决：</p><ol><li>使用最大权生成树算法删除网络中若干条边，使其不含无向环<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220317193317560.png" alt="image-20220317193317560" style="zoom:33%;"></li><li>重构没有环的贝叶斯网络</li><li>选择<code>loopy belief propagation算法</code>(可以简单理解为<code>sum-product</code>算法的递归版本)</li></ol><h4 id="4-3-无向图-马尔科夫网络-马尔科夫随机场"><a href="#4-3-无向图-马尔科夫网络-马尔科夫随机场" class="headerlink" title="4.3 无向图(马尔科夫网络/马尔科夫随机场)"></a>4.3 无向图(马尔科夫网络/马尔科夫随机场)</h4><p>二维空间中的图像(空间)，可以看作是一个二维随机场。二维马尔科夫随机场将时间上的马尔科夫性转换到空间上，考虑空间的关系，二维MRF的平面网格结构可以表现图像中像素之间的空间相关性</p><p>根据<code>Hammersley Clifford</code>定理，一个无向图模型的概率可以表示为所有最大连通图的势函数的乘积</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM(训练、解码算法、EM).assets\image-20220310204206148.png" alt="image-20220310204206148" style="zoom:50%;"></p><p>$P=(Y)$可以分解为一个图的若干最大连通子图的联合概率累积</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM(训练、解码算法、EM).assets\image-20220310204909701.png" alt="image-20220310204909701" style="zoom: 80%;"></p><p>其中<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM(训练、解码算法、EM).assets\image-20220310205051879.png" alt="image-20220310205051879" style="zoom:80%;">，归一化的目的是让结果变成概率，其中$\psi_cY_c$是一个最大连通图上样本的联合概率，一般取指数函数，这个最大连通图的联合概率的指数函数又叫做<strong>势函数</strong>$\psi_c(Y_c)=e^{-E(Y_c)}=e^{\sum_k\lambda_kf_k(c,y|c,x)}$</p><p>势函数因子分解后表示为：<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM(训练、解码算法、EM).assets\image-20220310210007399.png" alt="image-20220310210007399" style="zoom:80%;"></p><h4 id="4-4-随机场-RF"><a href="#4-4-随机场-RF" class="headerlink" title="4.4 随机场(RF)"></a>4.4 随机场(RF)</h4><p>当给每一个<strong>位置</strong>中按照某种分布随机赋予<strong>相空间</strong>的一个值之后，其全体就叫做随机场。</p><p>其中，位置可以理解成一块地，相空间可以理解成不同种类的庄稼。随机场就可以理解为在不同的地种上不同的庄稼。<strong>随机场也就是在哪块地里种什么庄稼的事情</strong></p><h4 id="4-5-MRF-马尔科夫随机场"><a href="#4-5-MRF-马尔科夫随机场" class="headerlink" title="4.5 MRF(马尔科夫随机场)"></a>4.5 MRF(马尔科夫随机场)</h4><p><strong>随机场中，利用邻域系统可以分析空间上的马尔可夫性，如一个像素点的特性，更可能受它周围像素的影响，与它距离越远的像素，对它的特性影响越小</strong>。根据这个距离可以建立一种分阶邻域系统，常用的有欧式距离、市区距离、棋盘距离等距离函数（如下图中的市区距离）</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM(训练、解码算法、EM).assets\image-20220310213733078.png" alt="image-20220310213733078" style="zoom:33%;"></p><p>在随机场上加上马尔科夫性（也就是今天只与昨天有关），以种庄稼为例，我们可以把MRF理解为<strong>在一块地里种的庄稼的种类只与它邻近的地里种的庄稼有关，与其他地方的庄稼的种类无关</strong>，那么<strong>这些地里种的庄稼集合就是一个马尔科夫随机场</strong>。</p><h4 id="4-6-马尔科夫随机场与图像的关系"><a href="#4-6-马尔科夫随机场与图像的关系" class="headerlink" title="4.6 马尔科夫随机场与图像的关系"></a>4.6 马尔科夫随机场与图像的关系</h4><p>MRF将图像模拟成一个随机变量组成的网格，变量只对邻近元素有依赖性，该模型考虑每个像元关于它的邻近像元的条件分布，<strong>有效描述图像的局部统计特性</strong>。</p><p>图像分隔问题求解的是满足最大后验概率的每个像素的分类标号，我们称为标号场，记为X</p><p>在图像中，<strong>格点集</strong>S表示像素的位置(坐标)，X称为<strong>标号场</strong>(数值)（像素值的集合或经小波变换后的小波系数集合），L表示将图像分割为不同区域的数目，即<strong>标签集合</strong>(分类数)</p><h4 id="4-7-吉布斯-Gibbs-分布"><a href="#4-7-吉布斯-Gibbs-分布" class="headerlink" title="4.7 吉布斯(Gibbs)分布"></a>4.7 吉布斯(Gibbs)分布</h4><p>$\delta$是在S上的邻域系统，当且仅当随机场X的<strong>联合概率分布</strong>具有如下形式：</p><p>$P(X=x)=(\frac{1}{Z})e^{-U(x)}$，则称X为<strong>吉布斯随机场</strong>，其中$U(x)=-\displaystyle\sum V_c(x)$称为<strong>能量函数</strong>，$V_c(x)$是仅与子团c内各像元值有关的子团<strong>势函数</strong>，$Z=\displaystyle\sum e^{-U(x)}$称为<strong>配分函数</strong>，是一个归一化常数</p><h4 id="4-8-Hammersley-Clifford"><a href="#4-8-Hammersley-Clifford" class="headerlink" title="4.8 Hammersley-Clifford"></a>4.8 Hammersley-Clifford</h4><p>MRF是用来描述图像的<strong>局部特性</strong>，而吉布斯随机场由<strong>随机场的全局性</strong>来刻画，那么一定存在可以将两个随机场联系起来的关系，Hammersley-Clifford定理证明了Gibbs分布和MRF的关系</p><p>Hammersley-Clifford定理：领域系统M在集合S(地)中，若S上随机场X(庄稼)符合Gibbs随机场，那么X也是一个MRF。</p><p>从而可以用Gibbs分布求解MRF中的概率分布，响应的MRF模型的结构信息就可以由Gibbs分布表达式描述</p><h4 id="4-8’-吉布斯分布和马尔科夫随机场"><a href="#4-8’-吉布斯分布和马尔科夫随机场" class="headerlink" title="4.8’ 吉布斯分布和马尔科夫随机场"></a>4.8’ 吉布斯分布和马尔科夫随机场</h4><p>Gibbs分布与MRF等价条件：</p><p>在随机场是关于领域系统的Gibbs分布时，这个随机场是关于领域系统的MRF，表示为：<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM(训练、解码算法、EM).assets\image-20220311105040016.png" alt="image-20220311105040016" style="zoom: 33%;"></p><p>通过<strong>能量函数</strong>确定MRF的条件概率，从而使其在<strong>全局</strong>上有一致性。通过单个像素及其领域的简单局部交互，MRF模型可以获得复杂的全局行为。（即计算局部的Gibbs分布得到全局的统计结果）则MRF中概率分布的问题就转化成对势函数$V_c(x)$的研究</p><h4 id="4-9-贝叶斯公式"><a href="#4-9-贝叶斯公式" class="headerlink" title="4.9 贝叶斯公式"></a>4.9 贝叶斯公式</h4><p>$P(AB)=P(A)<em>P(B|A)=P(B)</em>P(A|B)$</p><p>$P(A)$是A的先验概率或边缘概率，称为先验是因为不用考虑B的作用，也称为<strong>标准化常量</strong></p><p>$P(A|B)$是已知B发生后A发生的条件概率，也被称为A的后验概率</p><h4 id="4-10-基于MRF的图像分割模型"><a href="#4-10-基于MRF的图像分割模型" class="headerlink" title="4.10 基于MRF的图像分割模型"></a>4.10 基于MRF的图像分割模型</h4><p>我们可以把图像分割问题转化为<strong>图像的标记问题</strong>。<strong>标记场</strong>是用来对待测目标的<strong>像素</strong>进行<strong>跟踪标记</strong>，<strong>特征场</strong>是拟合原始的<strong>观测数据</strong>，尽可能准确地反映每一个<strong>像素位置</strong>的<strong>特征信息</strong>，使图像分割的结果中能够保留更多的<strong>细节信息</strong>。</p><p>分割问题可以描述为：<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM(训练、解码算法、EM).assets\image-20220311123805192.png" alt="image-20220311123805192" style="zoom:33%;"></p><p>根据贝叶斯公式，最优分割准则为：$\hat{X}=argmax\frac{P(Y|X)P(X)}{P(Y)}$，MAP求解的就是$P(Y|X)P(X)$(似然*先验)</p><h4 id="4-11-标记场求解"><a href="#4-11-标记场求解" class="headerlink" title="4.11 标记场求解"></a>4.11 标记场求解</h4><blockquote><p>P(X)求解</p></blockquote><p>P(Y|X)服从高斯分布，而$P(X)$通过MRF转换为Gibbs分布得到，最后更新标号场使得成绩最大，得到最佳分割</p><p>对于M*N的图像Y，其中任意一个像素$y_i$分割后对应的标记为$x_i$，定义两个随机场：$X={x_i,i∈S}$表示图像分割后的类别标号场(图像的值)，$x_i=1,2,…,L$表示分割成L个区域(类别)，但类别状态不能直接观察到(没有打标)，$Y={y_i,i∈S}$是可观测的随机场(灰度值)</p><p>根据贝叶斯准则，最优分割准则为<code>最大的先验*似然</code>。由于随机场X是MRF，具有正概率性和马尔科夫性，由MRF和Gibbs分布等价可知<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM(训练、解码算法、EM).assets\image-20220311115549101.png" alt="image-20220311115549101" style="zoom: 50%;">，其中$V_c(x_i)$是包含$x_i$的基团c的势函数，C是所有基团(最大连通子图)的集合</p><h5 id="4-11-1-势函数"><a href="#4-11-1-势函数" class="headerlink" title="4.11.1 势函数"></a>4.11.1 势函数</h5><p>选择MLL（多级逻辑模型）作为势函数的参考模型。</p><p>对于单点子团(也就是最大连通图为一个节点的连通图)，函数依赖于已经分配的标签值$V_c(x)=V_c(x_s)=\alpha_l$，对于成对的子团，势函数为$V_c(x_i)=$<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM(训练、解码算法、EM).assets\image-20220311120639666.png" alt="image-20220311120639666" style="zoom:33%;">，$J$为像素点$i$<strong>领域内的点</strong>，$\beta$为<strong>耦合系数</strong>，表示<strong>相邻像素的惩罚程度</strong>。根据势函数，我们就可以求出$P(X)$</p><h4 id="4-12-特征场模型建立"><a href="#4-12-特征场模型建立" class="headerlink" title="4.12  特征场模型建立"></a>4.12  特征场模型建立</h4><blockquote><p>P(Y|X)求解</p></blockquote><p>$P(Y|X)$表示给定标记场(图片<code>rgb</code>数值)$X=x_i$的条件下，特征场$Y=y_i$的联合分布</p><p>拟合原始的观测数据，尽可能地反映出每一个分类的特征信息，使分割结果保留更多的细节信息。假设图像的观测数据可以视为多个高斯函数的加权来表现其特征，即$P(Y|X)$服从高斯分布。</p><p>公式为：$P(Y|X)=\displaystyle \sum_{k=1}^n\pi_mf(y|x=m)$</p><p>其中，y表示该点的灰度值，m表示观测区域的类别标记状态，$\pi_m$表示图像中标记为k类像素的个数占图像像素个数的比重（即当前类别的比重），即$0&lt;\pi_m&lt;1$</p><p>$f(Y|X=m)$表示对应类别的高斯概率密度函数<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM(训练、解码算法、EM).assets\image-20220311150133164.png" alt="image-20220311150133164" style="zoom:50%;"></p><p>则根据上述的P(X)和$P(Y|X)$采用最大后验估计来得到图像标号的分布，即求得最优分割结果，即完成图像分割的过程</p><h3 id="4‘-MRF图像操作总结"><a href="#4‘-MRF图像操作总结" class="headerlink" title="4‘.MRF图像操作总结"></a>4‘.MRF图像操作总结</h3><p>根据像素点的值判断归属类别，也就是输入图像求分类信息</p><h4 id="4-0’-概念理解"><a href="#4-0’-概念理解" class="headerlink" title="4.0’ 概念理解"></a>4.0’ 概念理解</h4><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220316193250808.png" alt="image-20220316193250808" style="zoom: 80%;"></p><p>团：<strong>相互连通</strong>的连通图，(1,2),(2,3)等</p><p>极大团：<strong>相互连通</strong>的最大连通图，(1,2,3),(1,3,4),(4,5)</p><p>势函数：一个极大团Q中变量之间的联合概率分布（因为极大团肯定包括小团），记为$\psi_Q(X_Q)$，表示从随机变量集合到实数域的一个映射</p><p><code>Hammersley Clifford</code>定理：设所有团构成集合C，极大团Q∈C对应的变量集记为$X_Q$，则极大团之间的联合概率公式为$P(X)=\frac{1}{Z}\prod\psi_Q(X_Q)$。</p><p>其中$Z=\displaystyle\sum<em>X\prod\psi</em>{Q}(X_Q)$，Z是规范化因子(也叫配分函数)，保证P(X)构成一个概率分布（即归一化）</p><p>在上图中表示为：$P(X)=\frac{1}{Z}\psi<em>{1,2,3}(1,2,3)\psi</em>{1,3,4}(1,3,4)\psi_{4,5}(4,5)$</p><blockquote><p>例子</p></blockquote><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220316215034673.png" alt="image-20220316215034673" style="zoom:50%;"></p><p>极大团的概率分布情况：</p><div class="table-container"><table><thead><tr><th>$\psi_1(A,B)$</th><th>$\psi_2(B,C)$</th><th>$\psi_3(C,D)$</th><th>$\psi_4(D,A)$</th></tr></thead><tbody><tr><td>$a^0 \ b^0 \ 30$</td><td>$b^0 \ c^0 \ 100$</td><td>$c^0 \ d^0 \ 1$</td><td>$d^0 \ a^0 \ 100$</td></tr><tr><td>$a^0 \ b^1 \ 5$</td><td>$b^0 \ c^1 \ 1$</td><td>$c^0 \ d^1 \ 100$</td><td>$d^0 \ a^1 \ 1$</td></tr><tr><td>$a^1 \ b^0 \ 1$</td><td>$b^1 \ c^0 \ 1$</td><td>$c^1 \ d^0 \ 100$</td><td>$d^1 \ a^0 \ 1$</td></tr><tr><td>$a^1 \ b^1 \ 10$</td><td>$b^1 \ c^1 \ 100$</td><td>$c^1 \ d^1 \ 1$</td><td>$d^1 \ a^1 \ 100$</td></tr></tbody></table></div><p>联合概率分布和归一化（某种状态序列对应的累积和除以因子乘积的和）后对应的概率分布：</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220316215405086.png" alt="image-20220316215405086" style="zoom: 50%;"></p><blockquote><p>势函数和边缘概率的关系</p></blockquote><p>以团(A,B)为例，势函数为<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220316221411621.png" alt="image-20220316221411621" style="zoom:80%;">，边缘概率<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220316221500777.png" alt="image-20220316221500777" style="zoom:50%;"></p><p>势函数和边缘概率是部分和整体的关系，势函数只考虑当前团的概率分布，而边缘概率则考虑了在当前势函数的状态组合中，剩余因子的状态概率和，如$P(A,B)=\displaystyle\sum_{C,D}P(A,B,C,D)$</p><h4 id="4-1’-MRF原理和应用实例-图像分割"><a href="#4-1’-MRF原理和应用实例-图像分割" class="headerlink" title="4.1’ MRF原理和应用实例-图像分割"></a>4.1’ MRF原理和应用实例-图像分割</h4><p>$P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}$，要求的P(Y|X)就是<strong>根据输入图像求分类信息</strong>；而似然函数P(X|Y)表示已知分类信息求分类信息对应的像素点的概率，判断分好类的各像素点和真实像素点分布是否匹配的关系；先验概率P(Y)表示分类标记(标记场)；P(X)就是输入图像的分布(已知且为固定，可忽略)</p><blockquote><p>求解分类标记P(Y)</p></blockquote><p>初始化的分类个数是假设的，我们并不知道这个图像分割为几类了，难道我们要无中生有吗？</p><p>这时候就需要联系<strong>吉布斯分布</strong>(Gibbs sampling)：</p><p>$P=(Y)$可以分解为一个图的若干最大连通子图(因子团)的联合概率累积，用<strong>吉布斯分布</strong>表示为：</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM(训练、解码算法、EM).assets\image-20220310204909701.png" alt="image-20220310204909701" style="zoom: 67%;"></p><p>其中<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM.assets\image-20220310205051879.png" alt="image-20220310205051879" style="zoom: 67%;">称为配分函数，是一个归一化常数，<strong>目的是让结果变成概率</strong>。</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM.assets\image-20220312120513065.png" alt="image-20220312120513065" style="zoom: 50%;"></p><p>如在上图中，该无向图的分类标记为：<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM.assets\image-20220312120550477.png" alt="image-20220312120550477" style="zoom:67%;"></p><p>其中$\psi_c(Y_c)$是最大连通子图(因子团)集合中随机变量的<strong>联合概率</strong>，我们要计算的是这些因子团的势，所以<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM.assets\image-20220312121320861.png" alt="image-20220312121320861">，也叫做势函数，记作$V_c(x_i)$。</p><p>则<strong>无相连通图的联合概率分布</strong>可以表示为：$P(X=x)=(\frac{1}{Z})e^{-U(x)}$。（最大熵模型，即在未知条件下概率都是相等的）<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220316171210021.png" alt="image-20220316171210021" style="zoom:67%;"></p><p>其中$U(x)=-\displaystyle\sum V_c(x_i)$称为<strong>能量函数</strong>；</p><p>$V_c(x_i)$称为<strong>势函数</strong>，是<strong>一个极大团上样本的联合概率</strong>，其中C是最大连通图(因子团)的集合：对于单点子团(也就是最大连通图为一个节点的连通图)，函数依赖于已经分配的标签值$V_c(x_i)=V_c(x_s)=\alpha_l$，对于成对的子团，势函数为$V_c(x_i)=$<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM.assets\image-20220311120639666-16470570936702.png" alt="image-20220311120639666" style="zoom:33%;">，$j$为像素点$i$<strong>领域内的点</strong>，$\beta$为<strong>耦合系数</strong>，表示<strong>相邻像素的惩罚程度</strong>；</p><p>$Z=\displaystyle\sum e^\frac{-U(x)}{T}$称为<strong>配分函数</strong>，是一个归一化常数，目的是让结果变成概率(其中T的作用是表示$P(Y)$的分割窗口也就是分类窗口(即分割框)的平滑程度，T越大越平滑)。</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM.assets\image-20220311235201989.png" alt="image-20220311235201989" style="zoom: 67%;"></p><p>根据<code>Hammersley Clifford</code>理论，马尔科夫随机场和Gibbs<strong>分布</strong>是一致的，即P(Y)的分布满足Gibbs分布</p><p><strong>吉布斯采样</strong>(Gibbs Sampling)是利用条件分布进行运算最终近似得到联合分布的一种采样方式，也就是满足吉布斯分布的信息<strong>可以通过计算条件概率来近似求得这些分布信息的联合分布</strong>。(即<strong>利用P(A|B)和P(B|A)近似求P(A,B)</strong>)</p><p>在图像中，我们可以知道某点与周围像素点之间的局部作用关系，或者说我们可以知道像素点及周围的分类标记信息，我们可以通过周围分布的分类标记信息，从而确定该点是否正确/需要更新。而实际计算中，我们一般只<strong>计算该像素点周围标记信息的次数来判断该像素点的分类信息</strong>。</p><p>由吉布斯采样结论得：$P(X|Y)<em>P(Y)=P(X,Y)$，而求$P(Y|X)$就是求$P(X|Y)</em>P(Y)$，即求解MRF就转化成了求$P(X,Y)$即图像和分割信息的联合概率分布，其中图像像素值的分布P(X)也就是样本的分布是已知的，所以我们只需要求P(Y,Y)分布即可。求解吉布斯分布即可得到随机场的分布P(Y)：</p><p>吉布斯采样把随机场问题转化为了求解势能的问题，通过能量函数确定MRF的条件概率，从而使其在全局上具有一致性。也就是通过单个像素及邻域的简单局部交互，<strong>MRF</strong>就获得了复杂的<strong>全局行为</strong>，即利用局部的Gibbs分布得到全局的统计结果。（<strong>中心点影响邻域，而邻域中的每个像素点又可以作为中心点影响他的邻域，从而得到复杂的全局行为</strong>）</p><blockquote><p>求解分割信息分布P(X|Y)</p></blockquote><p>得到$P(Y)$标记信息后，我们只需要根据样本分布$P(X)$再求得$P(X|Y)$即可求得图像分割信息$P(Y|X)$，即<strong>利用标记信息去估计像素点的值</strong>，假设标记分类中的像素点分布P(Y)服从高斯分布，我们就可以<strong>根据像素点的值判断其归属类别</strong></p><p>假设图像为：<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM.assets\image-20220312172720812.png" alt="image-20220312172720812" style="zoom:50%;"></p><p>概率分布为：$P(X|y=1,2,3,4)=$<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM.assets\image-20220312172951799.png" alt="image-20220312172951799" style="zoom: 80%;"></p><p>其中<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM.assets\image-20220312173215386.png" alt="image-20220312173215386" style="zoom:80%;"></p><p>这样，通过$P(X|y_1),P(X|y_2),…$就可以估计每个像素点的分割分类，最后我们得到后验概率和似然函数的乘积，迭代更新P(Y)得到最佳分割。（其中y表示各个类别的分布）</p><p>输入：<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM.assets\image-20220312173706247.png" alt="image-20220312173706247" style="zoom:33%;"></p><p>输出：<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM.assets\image-20220312173745497.png" alt="image-20220312173745497" style="zoom:33%;"></p><h4 id="4-2‘-图像去噪"><a href="#4-2‘-图像去噪" class="headerlink" title="4.2‘ 图像去噪"></a>4.2‘ 图像去噪</h4><p>相邻像素点之间有联系，根据这种联系和像素值进行分类</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM、MRF.assets\image-20220316174635018.png" alt="image-20220316174635018" style="zoom:50%;"></p><h3 id="5-判别式-discriminative-模型和生成式-generative-模型"><a href="#5-判别式-discriminative-模型和生成式-generative-模型" class="headerlink" title="5.判别式(discriminative)模型和生成式(generative)模型"></a>5.判别式(discriminative)模型和生成式(generative)模型</h3><h4 id="5-1-判别模型"><a href="#5-1-判别模型" class="headerlink" title="5.1 判别模型"></a>5.1 判别模型</h4><ul><li>概念：直接学习条件概率分布$P(Y|X)$，不关心数据怎么生成，只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类（公式为：$P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}$）</li><li>基本思想<ul><li>有限样本—&gt;预测函数—&gt;模型—&gt;预测</li><li>判别学习直接学习决策函数或者条件概率分布</li></ul></li><li>算法<ul><li>逻辑回归LR</li><li>决策树DT</li><li>支持向量机SVM</li><li>条件随机场CRF</li><li>最近邻KNN</li><li>神经网络traditional neural network</li><li>线性判别分析LDA</li><li>提升算法Boosting</li><li>线性回归</li></ul></li><li>应用<ul><li>图像和文本分类</li><li>生物序列分析</li><li>时间序列预测</li></ul></li><li>优点<ul><li>直接预测，学习准确率更高<ul><li>分类边界更灵活，比纯概率方法或生成模型得到的更高级</li><li>能清晰分辨出多类或者某一类与其他类之间的差异特征</li></ul></li><li>在聚类、视角变换、部分遮挡、尺度改变等方面效果更好</li><li>适用于较多类别的识别</li><li>由于直接学习P(Y|X)或P(X)，可以获取数据各种程度上的抽象的特征</li><li>性能更简单，比较容易学习</li></ul></li><li>缺点<ul><li>不能反映训练数据本身的特性，只能寻找不同类别之间的最优分类边界，反映的是数据之间的差异</li><li>缺少生成模型的优点，即先验结构的不确定性</li><li>黑盒模型，即变量间的关系不可见</li></ul></li></ul><h4 id="5-2-生成模型"><a href="#5-2-生成模型" class="headerlink" title="5.2 生成模型"></a>5.2 生成模型</h4><ul><li>概念<ul><li>生成模型要学习X和Y的联合概率分布$P(X,Y)$，然后根据贝叶斯公式来求得条件概率P(Y|X)，预测概率最大的Y。也就是对后验概率建模，从统计的角度表示数据的分布，反映了同类数据本身的相似度（公式为：$P(Y|X)=\frac{P(X,Y)}{P(X)}$）</li><li>生成算法尝试去找到底数据是怎么生成的，然后再对一个信号进行分类。也能够根据生成的结果，反推出最大概率的状态序列</li></ul></li><li>基本思想<ul><li>样本—&gt;预测生成模型—&gt;预测状态或者结果</li><li>对P(X)建模</li></ul></li><li>算法<ul><li>隐马尔可夫模型HMM</li><li>朴素贝叶斯Naive Bayesian</li><li>马尔科夫随机场MRF</li><li>贝叶斯网络</li><li>高斯混合模型GMM</li><li>隐狄利克雷分配模型LDA</li></ul></li><li>应用<ul><li>医学诊断</li><li>NLP</li></ul></li><li>优点<ul><li>输出结果携带的信息比判别模型丰富（因为是由状态转移概率等推算得）</li><li>研究单类问题比判别模型灵活性强</li><li>数据不完整时也能训练</li><li>模型可以通过增量学习得到</li><li>存在隐变量时，仍可以使用生成模型训练</li></ul></li><li>缺点<ul><li>容易产生错误分类，尤其是高度视觉相似的对象（如牛和马）</li><li>学习和计算过程比较复杂</li></ul></li></ul><h4 id="5-3-对比"><a href="#5-3-对比" class="headerlink" title="5.3 对比"></a>5.3 对比</h4><ul><li>生成模型可以得到判别模型，而判别模型不可以得到生成模型</li><li>本质区别<ul><li>判别模型估计的是条件概率分布</li><li>生成模型估计的是联合概率分布</li></ul></li><li>优化准则不同<ul><li>生成模型优化联合概率分布</li><li>判别模型优化条件概率分布，判别模型与序列标记问题有较好的对应性</li></ul></li><li>对观测序列的处理不同<ul><li>生成模型中，观测序列作为模型的一部分</li><li>判别模型中，观测序列作为条件，因此可以针对观测序列设计灵活的特征</li></ul></li><li>训练复杂度不同<ul><li>判别模型的训练复杂度较高</li></ul></li><li>是否支持无标签训练<ul><li>生成模型支持无标签训练</li></ul></li></ul><h3 id="6-条件随机场-CRF"><a href="#6-条件随机场-CRF" class="headerlink" title="6.条件随机场(CRF)"></a>6.条件随机场(CRF)</h3><p><strong>应用</strong>：命名实体标注和词性标注</p><p>给定一组输入随机变量的条件下，另一组输出随机变量的<strong>条件概率</strong>分布模型。比如输入一个多维的信号X，然后得到相应的多维输出Y，对应的概率分布模型为P(Y|X)</p><blockquote><p>CRF和MRF的区别</p></blockquote><p>MRF中是一个联合概率分布P(A,B)，当有一个无向图G，有多条路径来计算P(Y|X=x)。例如图像分类问题，通过输入X(图像)来得到Y(分类结果)。这是典型的监督学习，我们通过输入X(dataset)和Y(label)来实现P(Y|X)。</p><p>而随机场在监督学习中又分成两种，一种是属于<strong>生成模型</strong>的MRF，$P(Y|X)=\frac{P(X,Y)}{P(X)}$；而<strong>判别模型</strong>这是直接学习$X$（如K-Means），或者学习$P(Y|X)$（如CRF）。也就是说，判别模型只关心输入数据和输出数据是什么，然后直接进行预测。</p><p><strong>条件随机场</strong>只能对标准的预测问题（<strong>需要给定输入和指定输出</strong>），因为目的明确，所以准确度相对较高；而<strong>马尔科夫随机场</strong>是典型的生成模型，看一看对任何想预测的问题进行建模（如丢失了部分输入x，MRF可以对丢失的数据进行全概率分布的建模，就是<strong>可以通过输出结果y反向求解丢失的数据的概率分布P(X|Y=y)</strong>）</p><h4 id="6-1-总结"><a href="#6-1-总结" class="headerlink" title="6.1 总结"></a>6.1 总结</h4><p>条件随机场就是特殊条件下的马尔科夫随机场</p><h3 id="7-概率图的应用"><a href="#7-概率图的应用" class="headerlink" title="7.概率图的应用"></a>7.概率图的应用</h3><p>贝叶斯网络是有向图模型，而马尔科夫随机场是无向图模型。有向图模型的特点是序列之间有先后顺序，前序结果会对后序结果产生影响，有向图模型(CRF)通常应用于：机器人定位、词性标注、基因测序、语音识别等</p><p><img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM.assets\image-20220311215352574.png" alt="image-20220311215352574" style="zoom:50%;"></p><p>而马尔科夫随机场则不同，MRF经常用于图像方面，因为MRF只与前后节点有关联，在图像中每个点都和周围的点有关系，但是和远处的点或者初始点没有关系，离这个点越近对这个点的影响越大。（某点附近的像素点颜色与该点相近，但是远离该点就无法保证）。邻域的范围和大小，是由我们自己去决定的。</p><p>例如一阶邻域系统：<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM.assets\image-20220311235002013.png" alt="image-20220311235002013" style="zoom:80%;"></p><p>二阶邻域系统：<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM.assets\image-20220311235051673.png" alt="image-20220311235051673" style="zoom:80%;"></p><p>一阶和二阶邻域系统相应的因子团：<img src="/posts/articletemplate66.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\CRF和HMM.assets\image-20220311235201989.png" alt="image-20220311235201989"></p><p>无向图G中团是指无向图中节点的集合，且团节点之间是相通的，最大团是指团中再多加入一个节点所有节点就不是相通的，即无向图中最大团的概念。而概率无向图又可以将极大似然分解为每个节点的概率积，这个过程也叫因子分解，所以这些团叫因子团。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这是技术文章模板</title>
      <link href="/posts/articletemplate8.html"/>
      <url>/posts/articletemplate8.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">熵、信息量、KL散度、交叉熵、最大熵</div><p>如果两个模型的概率分布是不一样的，所以在衡量模型的差异的时候，不能直接定性衡量两个模型之间的差异，而是需要<strong>定量</strong>的衡量两个模型的差异（<strong>比如极大似然估计、最小二乘法和交叉熵</strong>）</p><h4 id="1-信息量"><a href="#1-信息量" class="headerlink" title="1.信息量"></a>1.信息量</h4><p><strong>信息量</strong>可以理解成一个事件从不确定变成确定的<strong>难度程度(概率)</strong>（或者说事件对某个人带来的价值大小）。难度越大，信息量越大。比如，阿根廷进入8强到赢得决赛的难度为$\frac{1}{2^3}$，则信息量为3比特，再比如中国队从8强赢得决赛的难度为$\frac{1}{2^{10}}$，则信息量为10比特。</p><p>公式为：$I(i)=-log_2p_i$，log以2为底因为最后要以<strong>比特</strong>表示信息量</p><p><img src="https://s2.loli.net/2022/03/28/R8r1z7T2qVH4aun.png" alt="image-20220306204926864" style="zoom:80%;"></p><h4 id="2-熵"><a href="#2-熵" class="headerlink" title="2.熵"></a>2.熵</h4><p><strong>熵</strong>衡量一个系统中的所有事件从不确定到确定的难度大小。对整体的概率模型进行一个衡量，衡量结果能反映出这个概率模型的不确定程度/混乱程度，熵是信息量的<strong>期望</strong>。</p><p>事件的不确定性越高，则信息量越高，即信息量函数$f$与概率$P$成<strong>负相关</strong>，即$f(P)=log\frac{1}{P}=-log(P)$</p><p>两个<strong>独立事件</strong>所产生的信息量等于各自信息量之和，即$f(P_1,P_2)=f(P_1)+f(P_2)$</p><p><strong>信息熵表示的是信息量的期望</strong>，即$H(P)=E[-log(P)]=\displaystyle\sum_XP(X)f(X)=-\displaystyle\sum_XP(X)logP(X)$</p><p>可以证明：$0 \le H(P) \le log\abs{X}$，所以当且仅当$X$的分布为均匀分布时有$H(P)=log\abs{X}$，即$P(X)=\frac{1}{\abs{X}}$时熵最大</p><h5 id="2-1-最大熵原理"><a href="#2-1-最大熵原理" class="headerlink" title="2.1 最大熵原理"></a>2.1 最大熵原理</h5><ol><li>最大熵<code>Max Entropy</code>原理：学习概率模型时，在所有可能的概率分布(模型)中，熵最大的分布(模型)是最好的模型（概率最均匀）<ul><li>通常会有<strong>其他已知条件</strong>来确定概率模型的<strong>集合</strong>，因此最大熵的原理是：在满足已知条件的情况下，选取熵最大的模型</li><li>在满足已知条件的前提下，如果没有其他信息，则不确定部分都是<strong>等可能</strong>的（均匀分布时，熵最大）。而这种等可能性就是由熵最大化得到的</li></ul></li><li>最大熵原理选取熵最大的模型，而决策树的目的是得到熵最小的划分。原因在于：<ul><li>最大熵原理认为在满足已知条件之后，选择不确定性最大(即不确定部分都是等可能的)的模型。即不再施加已知条件之外的约束。这是<strong>求最大不确定性的过程</strong></li><li>决策树的划分目标是为了通过不断划分从而降低实例所属的类的不确定性，最终给实例一个合适的分类。这是一个<strong>不确定性不断减小的过程</strong></li></ul></li></ol><h5 id="2-2-在期望约束下的最大熵模型"><a href="#2-2-在期望约束下的最大熵模型" class="headerlink" title="2.2 在期望约束下的最大熵模型"></a>2.2 在期望约束下的最大熵模型</h5><p>期望的约束表示为：$E[f(X)]=\displaystyle\sum_XP(X)f(X)=\tau$，其中$f(X)$是约束条件，$E(f(X))$是一个常数。</p><p>假设有k个约束条件：<img src="https://s2.loli.net/2022/03/28/Hdw6BQu79VSgUET.png" alt="image-20220316163251704" style="zoom: 67%;"></p><p>即求解约束最优化问题：<img src="https://s2.loli.net/2022/03/28/fzVgtTxOMBCKPZi.png" alt="image-20220316163435147" style="zoom:67%;"></p><p>利用拉格朗日乘子法求解：<img src="https://s2.loli.net/2022/03/28/B2Abcv3xPqIHlTF.png" alt="image-20220316163546810" style="zoom:67%;"></p><p>解得：<img src="https://s2.loli.net/2022/03/28/QNmeJUAuZz8sV1c.png" alt="image-20220316163627242" style="zoom:67%;"></p><h5 id="2-3-香农熵Shannon-entropy"><a href="#2-3-香农熵Shannon-entropy" class="headerlink" title="2.3 香农熵Shannon entropy"></a>2.3 香农熵Shannon entropy</h5><script type="math/tex; mode=display">H(p)=-Ep[logp]=\left\{\begin{matrix} H(p)=-\int_xP(x)logP(x)dx \\ H(p)=-\displaystyle\sum_xP(x)logP(x)\end{matrix}\right.</script><p>香农熵就是在连续分布和离散分布中，对信息量的期望</p><p>公式为：熵=事件的概率x事件信息量的和，即各事件对系统贡献的信息量$H(x)=-\displaystyle \sum_{i=1}^nP(x_i)log{P(x_i)}$</p><p>比如，对于二分类(或者是二项分布)的任务，$H(x)=-P(x)logP(x)-(1-P(x))log(P(x))$</p><p><img src="https://s2.loli.net/2022/03/28/WSDdHiU2E3XMQqF.png" alt="image-20220306205005670" style="zoom: 80%;"></p><h4 id="3-相对熵-KL散度"><a href="#3-相对熵-KL散度" class="headerlink" title="3.相对熵(KL散度)"></a>3.相对熵(KL散度)</h4><blockquote><p>通信/编码角度的理解</p></blockquote><ul><li>$H(P)$为服从$P(X)$分布的信源$X$的<strong>信息熵</strong>，<strong>也表示对信源$X$编码所需的平均比特数</strong></li><li>$H(P,Q)$称为交叉熵，<strong>表示对服从$P(X)$分布的信源$X$，按照分布$Q(X)$来进行编码所需的平均比特数</strong>，也表示<strong>利用分布$Q(X)$来表示服从$P(X)$分布的$X$的困难程度</strong></li><li>$D(P||Q)$为相对熵，表示用$Q(X)$来对信源$X$编码平均所需要的<strong>额外比特数</strong></li></ul><p>对于同一随机变量x，有两个独立的概率分布，我们可以用KL散度来衡量两个分布的差异。机器学习中，当我们不知道一个模型时，没办法直接求熵，需要依靠相对熵来计算两个模型的差异，也就是loss值（P往往表示样本的真实分布，Q表示预测模型的分布）</p><p><img src="https://s2.loli.net/2022/03/28/bGNUKB6DotYOle7.png" alt="image-20220308160033264" style="zoom:80%;"></p><p>令<img src="https://s2.loli.net/2022/03/28/yA7wpRqBsMzJgEh.png" alt="image-20220308160141012" style="zoom:80%;"></p><p>则相对熵可以写成以下形式：</p><p><img src="https://s2.loli.net/2022/03/28/XW1Ff9Q4SrTgux5.png" alt="image-20220308160220771" style="zoom:80%;"></p><p>根据吉布斯不等式，$D_{KL}$中$H(P,Q)-H(P)≥0$</p><p>仅当两个模型完全相等时$D(P||Q)=0$，<strong>有差异时$D(P||Q)&gt;0$</strong></p><p>交叉熵越小，表示两个模型越相近，或者说转码不会有过多冗余比特</p><p><strong>因为P的熵是确定的，所以求KL散度(相对熵)变成了求交叉熵的问</strong></p><p>上述讨论的相对熵是在事件数(样本量)一样的情况下，当模型事件数量(样本量)不一样的时候取事件数多的那个</p><p><img src="https://s2.loli.net/2022/03/28/cDFGgbdakYix7w1.png" alt="image-20220308162326928" style="zoom:80%;"></p><h4 id="4-交叉熵"><a href="#4-交叉熵" class="headerlink" title="4.交叉熵"></a>4.交叉熵</h4><p>在分类问题中，<strong>通常使用交叉熵损失度量标签的真实分布和由分类器预测的分布之间的差异</strong>。要找两个模型最优值，就是要找交叉熵最小的情况。</p><p><img src="https://s2.loli.net/2022/03/28/5geLrXpiHWJhbST.png" alt="image-20220308161727614" style="zoom:80%;"></p><p>一般来说<strong>以P为样本数据分布，Q为待优化的预测分布</strong></p><p>在机器学习当中，我们对模型的训练实际上就是一个参数估计的过程。我们<strong>对模型的参数进行调整的过程就是调整模型$Q(X)$来逼近真实数据$P(X)$的优化过程</strong></p><h5 id="4-1-交叉熵与极大似然估计"><a href="#4-1-交叉熵与极大似然估计" class="headerlink" title="4.1 交叉熵与极大似然估计"></a>4.1 交叉熵与极大似然估计</h5><p>极大似然估计<img src="https://s2.loli.net/2022/03/28/yAqoXWKU4nFpg8P.png" alt="image-20220307111247148" style="zoom:80%;">等价于最小化负对数似然<img src="/posts/articletemplate8.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\吴恩达机器学习\熵、信息量、KL散度、交叉熵.assets\image-20220307111346317.png" alt="image-20220307111346317" style="zoom:80%;"></p><p>这与逻辑回归中，用极大似然估计推出的损失函数在形式上是一样的，但是实际意义上是不一样的</p><ul><li><p><strong>极大似然估计中的log是为了将连乘计算量简化为连加</strong></p><p>极大似然估计：<img src="https://s2.loli.net/2022/03/28/SiZp2NkUjzWFvAP.png" alt="image-20220307115957278" style="zoom:80%;"></p><p>极大对数似然估计：<img src="https://s2.loli.net/2022/03/28/efJ5nmPIMhUXVGK.png" alt="image-20220307120100501" style="zoom:80%;"></p><p>$log(xyz)=log(x)+log(y)+log(z)$；熵中则是为了计算概率对应的信息量引入-log</p></li><li><p>而且<strong>一个是有量纲，一个是没有量纲的</strong>（交叉熵中的信息量是有量纲(比特)的，但是极大似然估计中是没有的）</p></li><li><p>而且极大似然估计中求的是极大值，LR中强行加了一个负号，使其变成了最小值；熵中是为了计算困难程度对应的概率引入-log（如夺冠的概率为$\frac{1}{8}$，最后夺冠了，则信息量为$-log_2(\frac{1}{8})=3比特$）</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性回归</title>
      <link href="/posts/articletemplate4.html"/>
      <url>/posts/articletemplate4.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">线性回归</div><p>线性回归预测函数：</p><script type="math/tex; mode=display">h_{\theta}(x)=\theta^{T}x</script><p>逻辑回归预测函数：</p><script type="math/tex; mode=display">h_{\theta}^{(x)}=\frac{1}{1+e^{-\theta^{T}x}}</script><p>线性回归损失函数：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h(\theta^{(i)}-y^i))^2+\frac{\lambda}{2m}\sum_{j=1}^m\theta_j^2</script><p>逻辑回归损失函数：</p><p>MSE直接应用到LR中会导致损失函数变成非凸函数，所以我们加入log让损失函数变成了凸函数</p><p>极大似然（二项分布中）：<img src="https://s2.loli.net/2022/03/28/JVhKAlC8GqdmHuB.png" alt="image-20220307115455459" style="zoom:80%;"></p><p>非二项分布：<img src="https://s2.loli.net/2022/03/28/xtyceCuAWTp3Hfz.png" alt="image-20220308100421863" style="zoom:80%;">（特定采样结果出现的概率累乘）</p><p>由于小数连乘操作可能造成<strong>下溢</strong>，一般会采用<strong>极大对数似然</strong>进行计算</p><p>极大对数似然（二项分布中）：<img src="https://s2.loli.net/2022/03/28/RVpeWD29NGnHkd4.png" alt="image-20220307115510199" style="zoom:80%;"></p><p>非二项分布：<img src="https://s2.loli.net/2022/03/28/Pt3ulvn1SE6fYzW.png" alt="image-20220308102624958" style="zoom:80%;"></p><p>损失函数（经验损失+结构损失）：</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{2m}\sum_{i=1}^m(y^ilog{h_\theta(x^i)}+(1-y^i)log(1-h_\theta x^{(i)})^2+\frac{\lambda}{2m}\sum_{j=1}^m\theta_j^2</script><p>两者损失函数求导后，<strong>除了假设函数不一样，表示形式是一样的</strong>：</p><script type="math/tex; mode=display">\frac{\part J(\theta)}{\part{\theta_j}}=\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)})x_j^{(i)}</script><p><img src="https://s2.loli.net/2022/03/28/qBdCjLkpsAW7gcu.png" alt="image-20220301171851453" style="zoom:80%;"></p><p>损失函数中参数倍数变化并不会影响最优值的最终结果</p><h4 id="1-1-逻辑回归LR-logistic-regression"><a href="#1-1-逻辑回归LR-logistic-regression" class="headerlink" title="1.1 逻辑回归LR(logistic regression)"></a>1.1 逻辑回归LR(logistic regression)</h4><h5 id="1-1-1-LR与sigmiod"><a href="#1-1-1-LR与sigmiod" class="headerlink" title="1.1.1 LR与sigmiod"></a>1.1.1 LR与sigmiod</h5><script type="math/tex; mode=display">h_{\theta}^{(x)}=\frac{1}{1+e^{-\theta^{T}x}}</script><p>其中$\theta$是收敛之后得到的结果</p><p><img src="https://s2.loli.net/2022/03/28/qDIhoYGim7lnbKk.png" alt="image-20220218231835992" style="zoom:80%;"></p><p><img src="https://s2.loli.net/2022/03/28/t1lJq9RNegYmLcH.png" alt="image-20220219101314250" style="zoom:80%;"></p><p>根据sigmoid曲线，$h_{\theta}≥0$时，置为1；否则置为0</p><h6 id="1-1-1-1-决策边界"><a href="#1-1-1-1-决策边界" class="headerlink" title="1.1.1.1 决策边界"></a>1.1.1.1 决策边界</h6><p><img src="https://s2.loli.net/2022/03/28/2BNgWfo6KwAqGzE.png" alt="image-20220219102924814" style="zoom:80%;"></p><p><img src="https://s2.loli.net/2022/03/28/5pSIeCALls2z8nr.png" alt="image-20220219103450449" style="zoom:80%;"></p><h5 id="1-1-2-代价函数"><a href="#1-1-2-代价函数" class="headerlink" title="1.1.2 代价函数"></a>1.1.2 代价函数</h5><p>当我们把线性回归的代价函数放到逻辑回归上使用时，会发现代价函数$J$由凸函数(convex)变成了有很多局部最大值的非凸函数，导致寻找最小值变得困难，所有我们选择了另一种能使LR变成凸函数的代价函数。</p><p><img src="https://s2.loli.net/2022/03/28/1gdrP678mFIhToq.png" alt="image-20220219104254098" style="zoom:80%;"></p><p>而对数函数log的曲线，能让代价函数变为凸函数的方程吗？</p><p>分析</p><p><img src="https://s2.loli.net/2022/03/28/ICF1KWedXqVLzDT.png" alt="image-20220219110405097" style="zoom:80%;"></p><p>化简</p><p><img src="https://s2.loli.net/2022/03/28/c6oIKGzs1MUZgpT.png" alt="image-20220219112119546" style="zoom:80%;"></p><p>得到如下结果，使用了==极大似然法==（能够在统计学中能为不同模型快速寻找参数），并且结果是凸函数</p><script type="math/tex; mode=display">\begin{align}J(\theta)&  = \frac{1}{m}\sum_{i=1}^mCost(h_{\theta}(x^{(i)},y^{(i)}))\\& = -\frac{1}{m}[\sum_{i=1}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]\end{align}</script><p>参数梯度下降：</p><p><img src="https://s2.loli.net/2022/03/28/m5EHc1V9MaerDTU.png" alt="image-20220219113754588" style="zoom:80%;"></p><p>==可以发现，求导后线性回归和逻辑回归的公式是一样的<strong>，</strong>但是他们的假设函数h(θ)是不同的，所以两个函数梯度下降公式是不同的==</p><p>求导sigmiod得到$\partial_{sigmoid}=sigmoid[1-sigmoid]$</p><h4 id="1-2-高级优化"><a href="#1-2-高级优化" class="headerlink" title="1.2 高级优化"></a>1.2 高级优化</h4><ul><li>共轭梯度法Conjugate Gradient</li><li>拟牛顿法中的对称正定迭代矩阵BFGS</li><li>近似BFGS，L-BFGS相对BFGS能够减少空间的使用</li></ul><h5 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h5><ul><li>无需设定学习率，学习率自动动态调整</li><li>大部分情况下速度比梯度下降法快很多</li></ul><h5 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h5><p>实现比梯度下降法复杂很多，但是基本上都有封装好的库，如python中的<code>scipy.optimize.fmin_bfgs</code></p><h4 id="1-3-逻辑回归的多分类任务"><a href="#1-3-逻辑回归的多分类任务" class="headerlink" title="1.3 逻辑回归的多分类任务"></a>1.3 逻辑回归的多分类任务</h4><p>训练多个逻辑回归分类器，然后将输入放到各分类器中，将输入归类为得分值最大的类别即可</p><h4 id="1-4-过拟合和欠拟合解决"><a href="#1-4-过拟合和欠拟合解决" class="headerlink" title="1.4 过拟合和欠拟合解决"></a>1.4 过拟合和欠拟合解决</h4><h5 id="1-4-1-过拟合"><a href="#1-4-1-过拟合" class="headerlink" title="1.4.1 过拟合"></a>1.4.1 过拟合</h5><ul><li>适当减少多余的参数</li><li>使用正则化，适当减少参数维度(阶/次方)/大小</li><li>增加数据量</li><li>dropout</li><li>清晰数据</li><li>提取终止训练</li></ul><h5 id="1-4-2-欠拟合"><a href="#1-4-2-欠拟合" class="headerlink" title="1.4.2 欠拟合"></a>1.4.2 欠拟合</h5><ul><li>增加特征和数据</li><li>增加高阶多项式项</li><li>减少正则化参数</li></ul><h4 id="1-5-正则化惩罚项"><a href="#1-5-正则化惩罚项" class="headerlink" title="1.5 正则化惩罚项"></a>1.5 正则化惩罚项</h4><p>加入惩罚项后，会降低高维参数的值，让他们趋于0（也就是==简化假设模型，限制模型参数，保持参数尽量小==），这样能让假设h函数变得更加的平滑</p><p><img src="https://s2.loli.net/2022/03/28/vmqSDy73hwxEAfM.png" alt="image-20220219164340840"></p><p>我们不知道哪些参数是高维的，该去降低哪些参数的维度，在代价函数中加入正则化惩罚项，对每个参数进行限制</p><h5 id="1-5-1-惩罚项公式以及作用"><a href="#1-5-1-惩罚项公式以及作用" class="headerlink" title="1.5.1 惩罚项公式以及作用"></a>1.5.1 惩罚项公式以及作用</h5><p>公式：<img src="https://s2.loli.net/2022/03/28/4NlhR2pBwMfGxCH.png" alt="image-20220219170024442"></p><p> <img src="https://s2.loli.net/2022/03/28/w64HTcyeGR7XqQW.png" alt="image-20220219222810433"></p><p>==简化假设模型，限制模型参数，保持参数尽量小==，$\lambda$作用是控制两个不同目标之间的取舍，设置合适的$\lambda$参数防止模型欠拟合或者无明显作用</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>复杂链表的复制</title>
      <link href="/posts/articletemplate5656.html"/>
      <url>/posts/articletemplate5656.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">复杂链表的复制</div><p><img src="/posts/articletemplate5656.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\剑指Offer\链表\image-20220217165148234.png" alt="image-20220217165148234"></p><p><img src="/posts/articletemplate5656.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\剑指Offer\链表\2CD1CC917CD1875FF9CD391C2924DF09.png" alt="img" style="zoom:33%;"></p><p><img src="/posts/articletemplate5656.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\剑指Offer\链表\image-20220217165220604.png" alt="image-20220217165220604"></p><p><img src="/posts/articletemplate5656.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\剑指Offer\链表\971325772A17A314D3C44EBCDB6E7209.png" alt="img" style="zoom:33%;"></p><h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><pre><code>输入：&amp;#123;1,2,3,4,5,3,5,#,2,#&amp;#125;返回值：&amp;#123;1,2,3,4,5,3,5,#,2,#&amp;#125;</code></pre><h5 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h5><pre><code>方法1：创建新节点直接存方法2：原节点上操作再分离(1-&gt;1&#39;-&gt;2-&gt;2&#39;)方法2思路：1.在原节点插入副本节点2.复制random指针(很关键的一步是copy-&gt;random=cur-&gt;random-&gt;next)指向当前指针的随机指针中的下一节点3.分离</code></pre><h5 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h5><pre class=" language-lang-c++"><code class="language-lang-c++">/*struct RandomListNode &#123;    int label;    struct RandomListNode *next, *random;    RandomListNode(int x) :            label(x), next(NULL), random(NULL) &#123;    &#125;&#125;;*/class Solution &#123;public:    RandomListNode* Clone(RandomListNode* pHead) &#123;        if (!pHead) &#123;            return NULL;        &#125;        //1.创建原节点副本，放到原节点后        RandomListNode *cur = pHead;        while (cur) &#123;            RandomListNode *cpyNode = new RandomListNode(-1);            //创建新节点            cpyNode->label = cur->label;//传递label            cpyNode->next = cur->next;//传递next指针            //插入新节点            cur->next = cpyNode;//插入节点            cur = cpyNode->next;//指针指向下一节点        &#125;        //2.构造随机指针        RandomListNode *tmp = pHead;//指到头部        RandomListNode *tmp2;        while (tmp) &#123;            tmp2 = tmp->next;            if (tmp->random) &#123;                tmp2->random = tmp->random->next;//很关键，注意是指向random->next            &#125;            tmp = tmp2->next;        &#125;        //3.分成两个链表        RandomListNode *p1 = pHead;//1        RandomListNode *p2;        RandomListNode *clone = pHead->next;//2        while (p1) &#123;            p2 = p1->next;            p1->next = p2->next;            p1 = p1->next;            if (p1) &#123;//判断是否有下一节点                p2->next = p1->next;            &#125;        &#125;        return clone;    &#125;&#125;;</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># -*- coding:utf-8 -*-# class RandomListNode:#     def __init__(self, x):#         self.label = x#         self.next = None#         self.random = Noneclass Solution:    # 返回 RandomListNode    def Clone(self, pHead):        # write code here        if not pHead:            return None        cur = pHead        while cur:#1.复制val和next            cpyNode = RandomListNode(-1)            cpyNode.label = cur.label            cpyNode.next = cur.next            cur.next = cpyNode            cur = cpyNode.next        cur = pHead        while cur:#复制random            if cur.random:                cur.next.random = cur.random.next            cur = cur.next.next        cur = pHead        clone = pHead.next        while cur.next://拆分            tmp = cur.next            cur.next = tmp.next            cur = tmp        return clone</code></pre>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>删除链表的节点</title>
      <link href="/posts/articletemplate5523.html"/>
      <url>/posts/articletemplate5523.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">删除链表的节点</div><h4 id="18-删除链表的节点"><a href="#18-删除链表的节点" class="headerlink" title="18.删除链表的节点"></a>18.删除链表的节点</h4><h5 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h5><pre><code>给定单向链表的头指针和一个要删除的节点的值，定义一个函数删除该节点。返回删除后的链表的头节点。1.此题对比原题有改动2.题目保证链表中节点的值互不相同3.该题只会输出返回的链表和结果做对比，所以若使用 C 或 C++ 语言，你不需要 free 或 delete 被删除的节点数据范围:0&lt;=链表节点值&lt;=100000&lt;=链表长度&lt;=10000</code></pre><p><img src="/posts/articletemplate5523.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\剑指Offer\链表\image-20220217172652133.png" alt="image-20220217172652133"></p><pre><code>思路：指针跳过要删除的节点，考虑特殊节点情况即可</code></pre><pre><code>/** * struct ListNode &amp;#123; *    int val; *    struct ListNode *next; *    ListNode(int x) : val(x), next(nullptr) &amp;#123;&amp;#125; * &amp;#125;; */class Solution &amp;#123;public:    /**     * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可     *     *      * @param head ListNode类      * @param val int整型      * @return ListNode类     */    ListNode* deleteNode(ListNode* head, int val) &amp;#123;        // write code here        if (!head)&amp;#123;            return NULL;        &amp;#125;        if (head-&gt;val == val)&amp;#123;            return head-&gt;next;        &amp;#125;        ListNode *tmp = head;        while (tmp) &amp;#123;            if (tmp-&gt;next-&gt;val == val)&amp;#123;                tmp-&gt;next = tmp-&gt;next-&gt;next;                break;            &amp;#125;            tmp = tmp-&gt;next;        &amp;#125;        return head;    &amp;#125;&amp;#125;;</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># class ListNode:#     def __init__(self, x):#         self.val = x#         self.next = None## 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可## # @param head ListNode类 # @param val int整型 # @return ListNode类#class Solution:    def deleteNode(self , head: ListNode, val: int) -> ListNode:        # write code here        if not head:            return None        if head.val == val:            return head.next        tmp = head        while tmp.next:            if tmp.next.val == val:                tmp.next = tmp.next.next                break            tmp = tmp.next        return head</code></pre>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>链表中倒数最后k个节点</title>
      <link href="/posts/articletemplate445.html"/>
      <url>/posts/articletemplate445.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">链表中倒数最后k个节点</div><h4 id="22-链表中倒数最后k个节点"><a href="#22-链表中倒数最后k个节点" class="headerlink" title="22.链表中倒数最后k个节点"></a>22.链表中倒数最后k个节点</h4><h5 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h5><p><img src="/posts/articletemplate445.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\剑指Offer\链表\image-20220217130507286.png" alt="image-20220217130507286"></p><pre><code>思路：1.快慢指针 2.存入容器vector&lt;*ListNode*&gt;，取倒数k个节点(v.size-k)</code></pre><pre class=" language-lang-c++"><code class="language-lang-c++">/** * struct ListNode &#123; *    int val; *    struct ListNode *next; *    ListNode(int x) : val(x), next(nullptr) &#123;&#125; * &#125;; */class Solution &#123;public:    /**     * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可     *     *      * @param pHead ListNode类      * @param k int整型      * @return ListNode类     */    ListNode* FindKthToTail(ListNode* pHead, int k) &#123;        // write code here        if (!pHead) &#123;            return pHead;        &#125;        ListNode *fast = pHead;        ListNode *slow = pHead;        while (k--) &#123;            if (!fast) &#123;                return NULL;            &#125;            fast = fast->next;        &#125;        while (fast) &#123;            fast = fast->next;            slow = slow->next;        &#125;        return slow;    &#125;&#125;;</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># class ListNode:#     def __init__(self, x):#         self.val = x#         self.next = None## 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可## # @param pHead ListNode类 # @param k int整型 # @return ListNode类#class Solution:    def FindKthToTail(self , pHead: ListNode, k: int) -> ListNode:        # write code here        if not pHead:return pHead        if k <= 0:            return None        fast = pHead        slow = pHead        for i in range(k):            if not fast:return None            fast = fast.next        while fast:            fast = fast.next            slow = slow.next        return slow</code></pre>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最小二乘法</title>
      <link href="/posts/articletemplate2.html"/>
      <url>/posts/articletemplate2.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">最小二乘法</div><p><a href="https://www.bilibili.com/read/cv14977249?spm_id_from=333.999.0.0">“损失函数”是如何设计出来的？直观理解“最小二乘法”和“极大似然估计法” - 哔哩哔哩 (bilibili.com)</a></p><h4 id="1-最小二乘法"><a href="#1-最小二乘法" class="headerlink" title="1.最小二乘法"></a>1.最小二乘法</h4><p>求模型的结果与真实值的差距（或者说是损失大小）</p><p>$\displaystyle\sum<em>{i=1}^n|\hat y_i-y_i|$，<strong>为了方便求导</strong>（梯度下降），我们可以将该算法设计成$min\displaystyle\sum</em>{i=1}^n\frac{1}{2}(\hat y_i-y_i)^2$</p><h4 id="2-极大似然函数"><a href="#2-极大似然函数" class="headerlink" title="2.极大似然函数"></a>2.极大似然函数</h4><p>$L(\theta)$似然函数，$H(\theta)$熵</p><p>求解极大似然函数需要知道数据的<strong>分布</strong>，然后根据概率求参数，最后求解</p><h4 id="3-最小二乘和极大似然"><a href="#3-最小二乘和极大似然" class="headerlink" title="3.最小二乘和极大似然"></a>3.最小二乘和极大似然</h4><p>在正态分布上两者的损失函数是相同的</p><h4 id="4-极大似然和交叉熵"><a href="#4-极大似然和交叉熵" class="headerlink" title="4.极大似然和交叉熵"></a>4.极大似然和交叉熵</h4><p>除去量纲等，两者计算得到的效果是相同的</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性回归、代价函数和梯度下降</title>
      <link href="/posts/articletemplate3.html"/>
      <url>/posts/articletemplate3.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">线性回归、代价函数和梯度下降法</div><p>线性回归预测函数：</p><script type="math/tex; mode=display">h_{\theta}(x)=\theta^{T}x</script><p>逻辑回归预测函数：</p><script type="math/tex; mode=display">h_{\theta}^{(x)}=\frac{1}{1+e^{-\theta^{T}x}}</script><p>线性回归损失函数：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h(\theta^{(i)}-y^i))^2+\frac{\lambda}{2m}\sum_{j=1}^m\theta_j^2</script><p>逻辑回归损失函数：</p><p>如果直接使用线性回归的<strong>MSE</strong>会让逻辑回归的代价函数变成<strong>非凸函数</strong>，这样就会导致有非常多的局部最优值，导致梯度下降法失效。所以引入了<strong>交叉熵损失函数</strong>来替代线性回归的MSE(均方误差)</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{2m}\sum_{i=1}^m(y^ilog{h_\theta(x^i)}+(1-y^i)log(1-h_\theta x^{(i)})^2+\frac{\lambda}{2m}\sum_{j=1}^m\theta_j^2</script><p>两者损失函数求导后，<strong>除了假设函数不一样，表示形式是一样的</strong>：</p><script type="math/tex; mode=display">\frac{\part J(\theta)}{\part{\theta_j}}=\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)})x_j^{(i)}</script><p><img src="https://s2.loli.net/2022/03/28/qBdCjLkpsAW7gcu.png" alt="image-20220301171851453" style="zoom:80%;"></p><p>损失函数中参数倍数变化并不会影响最优值的最终结果</p><h3 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1.线性回归"></a>1.线性回归</h3><p>两个变量：$y=wx+b$</p><p>多个变量：<strong>假设函数</strong>(hypothesis)：$h_\theta(x)=\theta^Tx=\theta_0x_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n$</p><p><img src="https://s2.loli.net/2022/03/28/CrHw2VAG8SYzvQB.png" alt="image-20220218112740442"></p><h4 id="1-1-代价函数-cost-function"><a href="#1-1-代价函数-cost-function" class="headerlink" title="1.1 代价函数(cost function)"></a>1.1 代价函数(cost function)</h4><ol><li>求(预测值-真实值)差的平方的和，也就是SSE的最小值$min\displaystyle\sum^{m}<em>{i=1}(\hat{y}_i-y</em>{i})^2$</li><li>均方差MSE：$J<em>{(w,b)}=\frac{1}{2m}\displaystyle\sum^{m}</em>{i=1}(\hat{y}<em>i-y</em>{i})^2$，为了方便计算，系数$\frac{1}{m}$换成$\frac{1}{2m}$</li></ol><p>根据x的不同系数w得损失曲线，<strong>根据最小的loss值得到对应系数w</strong></p><p><img src="https://s2.loli.net/2022/03/28/ScgUapGwnKtms7i.png" alt="image-20220215221913546" style="zoom:80%;"></p><p><img src="https://s2.loli.net/2022/03/28/KaIPbu18OlcHWir.png" alt="image-20220215230923601" style="zoom:80%;"></p><h4 id="1-2-梯度下降-迭代求最优值"><a href="#1-2-梯度下降-迭代求最优值" class="headerlink" title="1.2 梯度下降(迭代求最优值)"></a>1.2 梯度下降(迭代求最优值)</h4><p><strong>步长（学习率$\alpha$）</strong>决定了梯度下降的速度，梯度会下降到直至收敛convergence（也就是到局部最小值才停止），所以太大的步长会导致在坡底(<strong>局部最小值</strong>)震荡</p><p><strong>初始化起点</strong>也能影响梯度下降的速度和得到的局部最小值（<strong>局部最小值可能有很多个，初始化下降起点(也就是w和b)会影响局部最小值</strong>）。一般情况下，设置初始化<code>w, b = 0, 0</code></p><p><img src="https://s2.loli.net/2022/03/28/QuVXNgyfHtjGa5I.png" alt="image-20220215232421193" style="zoom:80%;"></p><p>梯度下降公式：<script type="math/tex">\theta_j=\theta_j-\alpha\frac{\partial{J(\theta_0,\theta_1)}}{\partial_{\theta_j}}\space\space\space(for j = 0 \space and \space j = 1)</script></p><h5 id="1-2-1-参数梯度下降实现步骤-方法"><a href="#1-2-1-参数梯度下降实现步骤-方法" class="headerlink" title="1.2.1 参数梯度下降实现步骤/方法"></a>1.2.1 参数梯度下降实现步骤/方法</h5><p><img src="https://s2.loli.net/2022/03/28/FNAP3ieLgwIaxl5.png" alt="image-20220217215555223"></p><p>正确的梯度更新应该是<strong>多个参数同步更新（先获取下降梯度再更新参数）</strong>，否则会影响在其他参数的更新，最终影响结果</p><p>如果刚好初始化值为局部最小值，则代价函数$J_\theta$的值为0</p><p>梯度下降时，学习率$\alpha$不需要变更，因为在梯度下降的过程中，代价函数的梯度$\partial_{J}$会随着慢慢下降而减小，所以梯度下降的速度也会减缓</p><p><img src="https://s2.loli.net/2022/03/28/YvAKqT3VrGIzsEF.png" alt="image-20220217223410230" style="zoom: 80%;"></p><p>线性回归的代价函数求导后得到(二元梯度下降)：</p><p>其中$\theta_{0}$为常数<img src="https://s2.loli.net/2022/03/28/6IkqDcxJjYpL35w.png" alt="image-20220217225105167"></p><p>MSE梯度下降公式：<img src="https://s2.loli.net/2022/03/28/gBysLWF3rZGExVO.png" alt="image-20220218215115716" style="zoom:80%;"></p><p>多元梯度下降：</p><p><img src="https://s2.loli.net/2022/03/28/T3XF17zNe5KVkZM.png" alt="image-20220218154711231" style="zoom:80%;"></p><h5 id="1-2-2-凸函数-convex-function-与线性回归"><a href="#1-2-2-凸函数-convex-function-与线性回归" class="headerlink" title="1.2.2 凸函数(convex function)与线性回归"></a>1.2.2 凸函数(convex function)与线性回归</h5><p><img src="https://s2.loli.net/2022/03/28/hyjdtB8Xlq4uLNo.png" alt="image-20220217230356998" style="zoom:80%;"></p><p>凸函数没有局部最优，只有一个全局最优，像这种函数，只要使用<strong>线性回归</strong>总是能收敛到全局最优</p><h5 id="1-2-3-批梯度下降法-Batch-Gradient-Descent"><a href="#1-2-3-批梯度下降法-Batch-Gradient-Descent" class="headerlink" title="1.2.3 批梯度下降法(Batch Gradient Descent)"></a>1.2.3 批梯度下降法(Batch Gradient Descent)</h5><p>考虑全局的一种方法，在线性回归中使用的MSE即均方差即是考虑了所有数据的一种BGD</p><h5 id="1-2-4-特征缩放-归一化"><a href="#1-2-4-特征缩放-归一化" class="headerlink" title="1.2.4 特征缩放/归一化"></a>1.2.4 特征缩放/归一化</h5><p>==归一化可以加快梯度下降的速度，也就是更快地收敛==</p><p><img src="https://s2.loli.net/2022/03/28/nb7cEaxNlXvMyR3.png" alt="image-20220218155743004" style="zoom: 67%;"></p><h6 id="1-2-4-1-均值归一化Mean-Normalization"><a href="#1-2-4-1-均值归一化Mean-Normalization" class="headerlink" title="1.2.4.1 均值归一化Mean Normalization"></a>1.2.4.1 均值归一化Mean Normalization</h6><script type="math/tex; mode=display">x=\frac{x-\mu}{x_{max}-x_{min}}</script><h5 id="1-2-5-小技巧"><a href="#1-2-5-小技巧" class="headerlink" title="1.2.5 小技巧"></a>1.2.5 小技巧</h5><blockquote><p>如何能够快速判断梯度下降是否正在有效工作/收敛呢？</p></blockquote><p>正确的学习率：<img src="https://s2.loli.net/2022/03/28/nbvBFYIiG38VEZo.png" alt="image-20220218161645087"></p><p>错误的学习率：<img src="https://s2.loli.net/2022/03/28/ZbzGvoDtc14rey3.png" alt="image-20220218162815860"></p><p>方法1：(推荐)运行过程中，根据<strong>迭代次数</strong>和<strong>代价函数的值/导数</strong>(下降速度)来判断梯度是否有效下降/收敛，也就是上述绘制曲线，通过看曲线的方式</p><p>方法2：设定一个阈值，当代价函数变化值小于该阈值则停止训练。但是该方式的缺点是通常这个阈值不好选择</p><h6 id="1-2-5-1-总结"><a href="#1-2-5-1-总结" class="headerlink" title="1.2.5.1 总结"></a>1.2.5.1 总结</h6><ul><li>$\alpha$学习率太小会导致梯度下降速度很慢</li><li>$\alpha$太大会导致梯度反向增长，震荡，甚至是收敛速度慢等</li></ul><p>设置较小的学习率总能收敛，但是速度会偏慢，通过观察运行时的曲线选择合适的学习率</p><p><img src="https://s2.loli.net/2022/03/28/J9C8WspuhiAtyRc.png" alt="image-20220218163635970"></p><h4 id="1-3-多项式回归和线性回归"><a href="#1-3-多项式回归和线性回归" class="headerlink" title="1.3 多项式回归和线性回归"></a>1.3 多项式回归和线性回归</h4><p>在选择特征时，可能有多个角度：如在房价预测时，你可以通过房子的纵深和宽度来计算影响因子，也可以通过面积来直接计算；根据模型/数据实际的效果来选择最合适的即可。</p><p>多项式拟合：<img src="https://s2.loli.net/2022/03/28/Nlj7FZAKzOBXVS6.png" alt="image-20220218170807251" style="zoom:80%;"></p><p>有时候我们能使用线性拟合的方式来得到多项式拟合的效果，如</p><p><img src="https://s2.loli.net/2022/03/28/gy5XRmeC8ObMBpw.png" alt="image-20220218171023479" style="zoom:80%;"></p><h4 id="1-4-正规方程-直接求解最优值-Norm-Equation"><a href="#1-4-正规方程-直接求解最优值-Norm-Equation" class="headerlink" title="1.4 正规方程(直接求解最优值)Norm Equation"></a>1.4 正规方程(直接求解最优值)Norm Equation</h4><p><img src="https://s2.loli.net/2022/03/28/S6P8On1GuzvUc49.png" alt="image-20220218202947319" style="zoom:80%;"></p><p>$\theta=(X^{T}X)^{-1}X^{T}y$该公式计算结果可以直接求得代价函数最小化的$\theta$，也就是算得其中一个参数系数的最优解</p><p>在使用了Norm Equation正规方程后，数据可以不用归一化处理，直接计算即可</p><h5 id="1-4-1-正规方程在不可逆情况下的解决方法"><a href="#1-4-1-正规方程在不可逆情况下的解决方法" class="headerlink" title="1.4.1 正规方程在不可逆情况下的解决方法"></a>1.4.1 正规方程在不可逆情况下的解决方法</h5><p>在Octave/Matlab中使用pinv(伪逆)/inv可以计算得到矩阵的逆，矩阵在一定条件下是不可逆的(矩阵的值为0，也就是某些特征之间存在线性关系，说明部分特征是多余的；样本太少，特征太多，适当减少特征或者使用正则化)，但是使用pinv仍然可以得到该矩阵的逆</p><p><img src="https://s2.loli.net/2022/03/28/l3tRJxmQ8wc2djN.png" alt="image-20220218212054877" style="zoom:80%;"></p><h4 id="1-5-梯度下降法VS正规方程"><a href="#1-5-梯度下降法VS正规方程" class="headerlink" title="1.5 梯度下降法VS正规方程"></a>1.5 梯度下降法VS正规方程</h4><div class="table-container"><table><thead><tr><th>\</th><th>梯度下降法Gradient Desent</th><th>正规方程Norm Equation</th></tr></thead><tbody><tr><td>缺点</td><td>1.需要设置学习率$\alpha$ 2.需要迭代升级</td><td>当参数比较大时，计算$\theta=(X^{T}X)^{-1}X^{T}y$的时间成本为$O(n^{3})$，但参数个数较小(10k以下?)时，速度会较梯度下降法快；对于一些复杂的学习算法，我们不得不使用梯度下降法来替代正规方程</td></tr><tr><td>优点</td><td>当参数非常大时依然能非常好地工作；在一些复杂算法中仍然适用，而正规方程只使用于特定的一些算法中，如线性回归等</td><td>不需要设置学习率$\alpha$；不需要迭代</td></tr></tbody></table></div><p><strong>自适应优化算法（如momentum-SGD、NAG、Adagrad、Adadelta、RMSprop）训练出来的结果通常都不如SGD，尽管这些自适应优化算法在训练时表现的看起来更好。 使用者应当慎重使用自适应优化算法。</strong></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Focal Loss和Balanced CE(样本比例不均衡问题)</title>
      <link href="/posts/articletemplate444.html"/>
      <url>/posts/articletemplate444.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">Focal Loss和Balanced CE(样本比例不均衡问题)</div><h4 id="1-信息量"><a href="#1-信息量" class="headerlink" title="1.信息量"></a>1.信息量</h4><p>当越不可能的事件或者相关程度越高的事件（今天中午总统吃什么，与我们相关程度低，信息量小；但是对于想应聘总统厨师的人来说，这件事的信息量就很大）发生了，我们获取到的信息量就越大，反之信息量越小。</p><p>样本不足会导致模型信息量不足，从而不能很好的拟合数据</p><h4 id="2-熵"><a href="#2-熵" class="headerlink" title="2.熵"></a>2.熵</h4><p>熵用来表示一个系统中所有信息量的期望，也可以用来表示一个系统的混乱程度</p><h4 id="3-相对熵"><a href="#3-相对熵" class="headerlink" title="3.相对熵"></a>3.相对熵</h4><p>用来衡量两个分布的差异，或者说是一个分布变换到另一个分布需要的信息增量</p><p>在机器学习中，预测分布Q在训练的过程中信息量不足，虽然可以大致描述，但是描述得有偏差，需要额外的一些信息增量才能达到和样本真实分布一样的描述。经过反复训练后，信息量足够描述后就不需要额外的信息增量了。</p><p>$相对熵=p的熵-pq交叉熵$：<img src="https://s2.loli.net/2022/03/28/wsnqF3CvR5AdTuk.png" alt="image-20220313143521524" style="zoom:80%;"></p><h4 id="4-交叉熵"><a href="#4-交叉熵" class="headerlink" title="4.交叉熵"></a>4.交叉熵</h4><p>评估标签和预测值之间的差距，而相对熵中p的熵是不变的，所以只需关注交叉熵即可</p><p>在机器学习中常用交叉熵作loss</p><h4 id="5-交叉熵应用"><a href="#5-交叉熵应用" class="headerlink" title="5.交叉熵应用"></a>5.交叉熵应用</h4><h5 id="5-1-单标签多分类任务"><a href="#5-1-单标签多分类任务" class="headerlink" title="5.1 单标签多分类任务"></a>5.1 单标签多分类任务</h5><p>一张图片只被归为一个标签，对应的一个batch的loss就是：</p><p><img src="https://s2.loli.net/2022/03/28/LnsDHy9JFiK7Gv5.png" alt="image-20220313144748836"></p><h5 id="5-2-多标签多分类任务"><a href="#5-2-多标签多分类任务" class="headerlink" title="5.2 多标签多分类任务"></a>5.2 多标签多分类任务</h5><p>一张图片可能会被归为多个标签，每个Label都是独立分布的，可以用交叉熵对每个独立的类别进行计算，每个类别只有是或不是两种可能，服从弄二项分布，每个类别对应的交叉熵为：</p><p><img src="https://s2.loli.net/2022/03/28/vPKTVdGZ4MWmpIX.png" alt="image-20220313145145469"></p><p>如果一张图片中同时存在青蛙和老鼠，且预测结果如下：</p><div class="table-container"><table><thead><tr><th style="text-align:left">*</th><th style="text-align:left">猫</th><th style="text-align:left">青蛙</th><th style="text-align:left">老鼠</th></tr></thead><tbody><tr><td style="text-align:left">Label</td><td style="text-align:left">0</td><td style="text-align:left">1</td><td style="text-align:left">1</td></tr><tr><td style="text-align:left">Predicted</td><td style="text-align:left">0.1</td><td style="text-align:left">0.7</td><td style="text-align:left">0.8</td></tr></tbody></table></div><p>则$loss=loss<em>猫+loss</em>蛙+loss_鼠$，而损失方式计算如下：</p><p><img src="https://s2.loli.net/2022/03/28/sZQKVfOXG24TtBU.png" alt="image-20220313145344159" style="zoom:80%;"></p><blockquote><p>总结</p></blockquote><p>对于多分类任务(包括二分类)的交叉熵损失为：</p><p><img src="https://s2.loli.net/2022/03/28/ecYqxWb9SkAUpM8.png" alt="image-20220313150602363" style="zoom:80%;"></p><p>其中$y_i$表示真实概率，$p_i$表示预测概率</p><h4 id="6-focal-loss"><a href="#6-focal-loss" class="headerlink" title="6. focal loss"></a>6. focal loss</h4><p>focal loss最初用于图像领域解决是数据不平衡造成的模型性能问题。</p><h5 id="6-1-样本不均衡问题"><a href="#6-1-样本不均衡问题" class="headerlink" title="6.1 样本不均衡问题"></a>6.1 样本不均衡问题</h5><p>例如，在欺诈识别的案例中，好坏样本的比例为10000 : 1，这样模型很容易学习到一个把所有样本都预测为好的模型，也就是模型没有拟合到极大似然，而是只学习到了<strong>先验</strong>(样本分布)，导致模型欠拟合。</p><blockquote><p>影响</p></blockquote><p>样本不均衡带来的根本影响是：模型会学习到样本比例这个先验信息，类别不均衡下的分类边界会侵占少数样本类的区域，也就是影响模型学习的更本质的特征，影响模型的鲁棒性。</p><p><img src="https://s2.loli.net/2022/03/28/rQwop4XLc5umFDe.png" alt="img" style="zoom: 33%;"></p><blockquote><p>问题分析</p></blockquote><p><strong>减少模型学习样本分布(先验信息)</strong>，<strong>让模型学习数据的本质特征</strong>，这样就能解决样本不均衡问题。</p><blockquote><p>必要性</p></blockquote><p>从分类效果出发，不均衡对于分类结果的影响不一定是不好的（除了在预测精度要求比较高等环境下），什么时候需要解决样本不均衡(抑制先验影响)呢？</p><ul><li>判断任务是否复杂：任务的复杂度越高，对样本不均衡越敏感（特征量、噪音等都和任务的复杂度相关）</li><li>训练样本分布与真实样本分布不一致</li><li>不均衡样本中占少数的那个类别数量是不是实在太少，导致模型学习不到好的特征。<ul><li>解决方法<ul><li>解决样本不均衡的情况</li><li>使用一些数据增强的方法</li><li>尝试像异常检测这种的<strong>单分类模型</strong></li></ul></li></ul></li></ul><blockquote><p>样本不均衡问题解决</p></blockquote><p>在学习任务有些难度的情况下，我们可以通过一些方法使得不同类别的样本对模型学习时的loss贡献权重均衡，从而<strong>消除模型对不同类别的偏向性</strong>，学到更为本质的特征。</p><p>我们现在就开始探讨这些解决方法：</p><h6 id="6-1-1-样本层面"><a href="#6-1-1-样本层面" class="headerlink" title="6.1.1 样本层面"></a>6.1.1 样本层面</h6><ul><li><p>欠采样和过采样</p><ul><li>欠采样：减少多数类的数量（如随机欠采样、NearMiss、ENN等）</li><li>过采样：尽量多地增加少数类的样本数量（如随机过采样、数据增强等），使得类别间项目均衡</li><li>混合采样：如smote+ENN</li></ul></li><li><p>数据增强</p><ul><li><p>单样本增强(主要用于图像)：几何操作(翻转缩放)、颜色变换、随机擦除(裁剪)、添加噪声等方法，<code>imgaug</code>库</p></li><li><p>多样本增强：通过组合及转换多个样本，主要有Smote、SamplePairing、Mixup等方法，在特征空间内构造已知样本的邻域值样本</p><p><img src="https://s2.loli.net/2022/03/28/MDu2JCfZFB4ctGi.png" alt="image-20220313175211682" style="zoom:50%;"></p></li><li><p>基于深度学习的数据增强</p><ul><li><p>生成模型如<strong>变分自编码网络</strong>(VAE)和<strong>生成对抗网络</strong>(GAN)</p><p><img src="https://s2.loli.net/2022/03/28/Q2lLNr1bMYDixdC.png" alt="image-20220313175505963" style="zoom:50%;"></p></li></ul></li></ul></li></ul><blockquote><p>缺点及解决方案</p></blockquote><ul><li>随机欠采样可能会导致丢弃含有重要信息的样本，在计算性能足够的情况下，可以考虑根据数据分布的采样方法(通常是基于距离的邻域关系)，如ENN、NearMiss等</li><li>随机过采样或数据增强样本也有可能是引入片面噪声，导致过拟合；也可能是引入信息量不大的样本。此时需要考虑调整采样方法，或者通过半监督算法(可借鉴Pu-Learning思路)选择增强数据的较优子集，以提高模型的泛化能力</li></ul><h6 id="6-1-2-损失函数层面"><a href="#6-1-2-损失函数层面" class="headerlink" title="6.1.2 损失函数层面"></a>6.1.2 损失函数层面</h6><p>损失函数层面主流的方法就是代价敏感学习(cost-sensitive)，即为不同分类损失给予不同的惩罚力度(权重)，在调节类别平衡的同时，也不会增加计算复杂度。</p><ul><li><p>常用方法</p><ul><li><p>class weight：<code>scikit</code>库中内置的方法，可以为不同类别的样本提供不同的权重(少数类的权重更高)，从而平衡各类别的学习。如<code>clf2=LogisticRegression(class_weight=&#123;0:1,1:10&#125;)  # 代价敏感学习</code>为少数类分配更高的权重，以避免决策偏重多数类的现象（类别权重除了设定balanced，还可以作为一个超参搜索）</p><p><img src="https://s2.loli.net/2022/03/28/4YrXUtIhokPuGVW.png" alt="image-20220313202920598" style="zoom: 50%;"></p></li><li><p>OHEM和Focal Loss：类别的不平衡可以归结为难易样本的不平衡，而难易样本的不平衡可以归结为梯度的不平衡，OHEM和Focal Loss都做了两件事：难样本（错分类(或者说是高损失)的样本）挖掘和类别的平衡。（另外还有GHM、PISA等方法）</p><ul><li><p>OHEM(Online Hard Example Mining)算法的核心是选择一些hard examples（多样性和高损失的样本）作为训练样本，针对性地改善模型学习效果。对于数据类别不平衡问题，OHEM的针对性更强。</p></li><li><p>Focal loss的核心思想是在交叉熵损失函数的基础上，<strong>增加了类别的不同权重以及困难（高损失）样本的权重</strong>，以改善模型学习效果。<strong>$p_t$表示与真实值接近程度</strong>，越大越接近，即分类越准确（<strong>也可以说$p_t$反映了分类的难易程度</strong>。$p_t$越大，说明分类置信度越高，样本越易分；反之越难分）。系数$(1-p_t)^\gamma$是<strong>调节因子</strong>，相比交叉熵损失，==Focal Loss对于分类不准确的样本损失没有变化，对于分类准确的样本损失会变小==。整体而言，<strong>相当于增加了分类不准确样本的权重</strong>。</p><p><img src="https://s2.loli.net/2022/03/28/liZtSjE6mXNIU5K.png" alt="image-20220313203941650" style="zoom: 67%;"></p></li><li><p>平衡交叉熵函数：合理分配权重，平衡损失函数分布，即在损失函数中增加惩罚项$\alpha$</p><p><img src="https://s2.loli.net/2022/03/28/T4cBRsOdSVH3DkJ.png" alt="image-20220313205241348" style="zoom:67%;">，其中$\frac{\alpha}{1-\alpha}=\frac{n}{m}$，权重根据样本分布设置</p></li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>两个链表的第一个公共节点</title>
      <link href="/posts/articletemplate564.html"/>
      <url>/posts/articletemplate564.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">两个链表的第一个公共节点</div><h4 id="52-两个链表的第一个公共节点"><a href="#52-两个链表的第一个公共节点" class="headerlink" title="52.两个链表的第一个公共节点"></a>52.两个链表的第一个公共节点</h4><h5 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h5><pre><code>输入两个无环的单向链表，找出它们的第一个公共结点，如果没有公共节点则返回空。（注意因为传入数据是链表，所以错误测试数据的提示是用其他方式显示的，保证传入数据是正确的）数据范围： n \le 1000n≤1000要求：空间复杂度 O(1)O(1)，时间复杂度 O(n)O(n)例如，输入&amp;#123;1,2,3&amp;#125;,&amp;#123;4,5&amp;#125;,&amp;#123;6,7&amp;#125;时，两个无环的单向链表的结构如下图所示：</code></pre><p><img src="/posts/articletemplate564.htm/Users\86172\Documents\WeChat Files\wxid_yp1bexdqvj8122\FileStorage\File\2022-03\每日学习\每日学习\每日学习\剑指Offer\链表\394BB7AFD5CEA3DC64D610F62E6647A6.png" alt="img" style="zoom:33%;"></p><pre><code>可以看到它们的第一个公共结点的结点值为6，所以返回结点值为6的结点。输入描述：输入分为是3段，第一段是第一个链表的非公共部分，第二段是第二个链表的非公共部分，第三段是第一个链表和二个链表的公共部分。 后台会将这3个参数组装为两个链表，并将这两个链表对应的头节点传入到函数FindFirstCommonNode里面，用户得到的输入只有pHead1和pHead2。返回值描述：返回传入的pHead1和pHead2的第一个公共结点，后台会打印以该节点为头节点的链表。</code></pre><h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><pre><code>输入：&amp;#123;1,2,3&amp;#125;,&amp;#123;4,5&amp;#125;,&amp;#123;6,7&amp;#125;返回值：&amp;#123;6,7&amp;#125;说明：第一个参数&amp;#123;1,2,3&amp;#125;代表是第一个链表非公共部分，第二个参数&amp;#123;4,5&amp;#125;代表是第二个链表非公共部分，最后的&amp;#123;6,7&amp;#125;表示的是2个链表的公共部分这3个参数最后在后台会组装成为2个两个无环的单链表，且是有公共节点的      输入：&amp;#123;1&amp;#125;,&amp;#123;2,3&amp;#125;,&amp;#123;&amp;#125;返回值：&amp;#123;&amp;#125;说明：2个链表没有公共节点 ,返回null，后台打印&amp;#123;&amp;#125;</code></pre><pre><code>方法一：我们可以把两个链表拼接起来，一个pHead1在前pHead2在后，一个pHead2在前pHead1在后这样，生成了两个相同长度的链表，那么我们只要同时遍历这两个表，就一定能找到公共结点时间复杂度O(m+n)，空间复杂度O(m+n)方法二：我们也可以先让把长的链表的头砍掉，让两个链表长度相同，这样，同时遍历也能找到公共结点此时，时间复杂度O(m+n)，空间复杂度为O(MAX(m,n))</code></pre><pre class=" language-lang-c++"><code class="language-lang-c++">/*struct ListNode &#123;    int val;    struct ListNode *next;    ListNode(int x) :            val(x), next(NULL) &#123;    &#125;&#125;;*/class Solution &#123;//下面所述的z为公共节点public://(x+z+y)=(y+z+x)把两个链表弄成一样长，相等即是有第一个公共节点(效果类似于把长链表的长的部分给去掉)    ListNode* FindFirstCommonNode( ListNode* pHead1, ListNode* pHead2) &#123;        ListNode *l1 = pHead1;        ListNode *l2 = pHead2;        while (l1 != l2) &#123;//判断是否相等，不相等就循环，当两个指针都指空也是一种相等，也会跳出循环            l1 = l1 ? l1->next : pHead2;//当前节点是否够用，不够即指向另一个链表            l2 = l2 ? l2->next : pHead1;        &#125;// l1 = l1.next if l1 else l2 <-python代码        return l1;    &#125;&#125;;</code></pre><pre><code># class ListNode:#     def __init__(self, x):#         self.val = x#         self.next = None## # @param pHead1 ListNode类 # @param pHead2 ListNode类 # @return ListNode类#class Solution:    def FindFirstCommonNode(self , pHead1 , pHead2 ):        # write code here        l1,l2 = pHead1,pHead2        while l1 != l2:            l1 = l1.next if l1 else pHead2            l2 = l2.next if l2 else pHead1        return l1</code></pre>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
